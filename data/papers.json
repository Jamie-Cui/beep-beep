{
  "papers": [
    {
      "id": "arxiv_2602.10915v1",
      "arxiv_id": "2602.10915v1",
      "title": "Blind Gods and Broken Screens: Architecting a Secure, Intent-Centric Mobile Agent Operating System",
      "authors": [
        "Zhenhua Zou",
        "Sheng Guo",
        "Qiuyang Zhan",
        "Lepeng Zhao",
        "Shuo Li",
        "Qi Li",
        "Ke Xu",
        "Mingwei Xu",
        "Zhuotao Liu"
      ],
      "abstract": "The evolution of Large Language Models (LLMs) has shifted mobile computing from App-centric interactions to system-level autonomous agents. Current implementations predominantly rely on a \"Screen-as-Interface\" paradigm, which inherits structural vulnerabilities and conflicts with the mobile ecosystem's economic foundations. In this paper, we conduct a systematic security analysis of state-of-the-art mobile agents using Doubao Mobile Assistant as a representative case. We decompose the threat landscape into four dimensions - Agent Identity, External Interface, Internal Reasoning, and Action Execution - revealing critical flaws such as fake App identity, visual spoofing, indirect prompt injection, and unauthorized privilege escalation stemming from a reliance on unstructured visual data.   To address these challenges, we propose Aura, an Agent Universal Runtime Architecture for a clean-slate secure agent OS. Aura replaces brittle GUI scraping with a structured, agent-native interaction model. It adopts a Hub-and-Spoke topology where a privileged System Agent orchestrates intent, sandboxed App Agents execute domain-specific tasks, and the Agent Kernel mediates all communication. The Agent Kernel enforces four defense pillars: (i) cryptographic identity binding via a Global Agent Registry; (ii) semantic input sanitization through a multilayer Semantic Firewall; (iii) cognitive integrity via taint-aware memory and plan-trajectory alignment; and (iv) granular access control with non-deniable auditing. Evaluation on MobileSafetyBench shows that, compared to Doubao, Aura improves low-risk Task Success Rate from roughly 75% to 94.3%, reduces high-risk Attack Success Rate from roughly 40% to 4.4%, and achieves near-order-of-magnitude latency gains. These results demonstrate Aura as a viable, secure alternative to the \"Screen-as-Interface\" paradigm.",
      "published": "2026-02-11",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10915v1",
      "url": "https://arxiv.org/abs/2602.10915v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "prompt",
        "security",
        "injection"
      ],
      "keyword_score": 4,
      "summary": "This paper identifies fundamental security flaws in current mobile LLM agents—exemplified by Doubao Mobile Assistant—that stem from their reliance on fragile, unstructured screen scraping (“Screen-as-Interface”). Through a systematic four-dimensional threat analysis (Agent Identity, External Interface, Internal Reasoning, Action Execution), the authors expose vulnerabilities including visual spoofing, indirect prompt injection, and unauthorized privilege escalation. To address these, they propose *Aura*, a clean-slate, intent-centric mobile agent OS featuring a Hub-and-Spoke architecture with a privileged System Agent, sandboxed App Agents, and a security-enforcing Agent Kernel. The Kernel implements four novel defenses: cryptographic identity binding, semantic input sanitization, cognitive integrity via taint-aware memory and plan alignment, and granular auditable access control. Evaluated on MobileSafetyBench, Aura achieves a 94.3% task success rate (vs. 75% for Doubao), slashes high-risk attack success from ~40% to 4.4%, and delivers near-order-of-magnitude latency improvements.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10869v1",
      "arxiv_id": "2602.10869v1",
      "title": "Agentic Knowledge Distillation: Autonomous Training of Small Language Models for SMS Threat Detection",
      "authors": [
        "Adel ElZemity",
        "Joshua Sylvester",
        "Budi Arief",
        "Rogério De Lemos"
      ],
      "abstract": "SMS-based phishing (smishing) attacks have surged, yet training effective on-device detectors requires labelled threat data that quickly becomes outdated. To deal with this issue, we present Agentic Knowledge Distillation, which consists of a powerful LLM acts as an autonomous teacher that fine-tunes a smaller student SLM, deployable for security tasks without human intervention. The teacher LLM autonomously generates synthetic data and iteratively refines a smaller on-device student model until performance plateaus. We compare four LLMs in this teacher role (Claude Opus 4.5, GPT 5.2 Codex, Gemini 3 Pro, and DeepSeek V3.2) on SMS spam/smishing detection with two student SLMs (Qwen2.5-0.5B and SmolLM2-135M). Our results show that performance varies substantially depending on the teacher LLM, with the best configuration achieving 94.31% accuracy and 96.25% recall. We also compare against a Direct Preference Optimisation (DPO) baseline that uses the same synthetic knowledge and LoRA setup but without iterative feedback or targeted refinement; agentic knowledge distillation substantially outperforms it (e.g. 86-94% vs 50-80% accuracy), showing that closed-loop feedback and targeted refinement are critical. These findings demonstrate that agentic knowledge distillation can rapidly yield effective security classifiers for edge deployment, but outcomes depend strongly on which teacher LLM is used.",
      "published": "2026-02-11",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10869v1",
      "url": "https://arxiv.org/abs/2602.10869v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces *Agentic Knowledge Distillation*, a fully autonomous framework where a large language model (LLM) acts as a self-directed teacher to iteratively generate synthetic SMS threat data and refine a small language model (SLM) for on-device smishing detection—without human involvement. Using four LLM teachers (e.g., Claude Opus, GPT-5.2 Codex) and two student SLMs (Qwen2.5-0.5B, SmolLM2-135M), the method achieves up to 94.31% accuracy and 96.25% recall, significantly outperforming a Direct Preference Optimization (DPO) baseline (50–80% accuracy) that lacks iterative feedback. Key results show performance is highly sensitive to the choice of teacher LLM, underscoring its critical role in the distillation loop. The work demonstrates that closed-loop, agentic refinement enables rapid development of high-performing, deployable security models for edge devices.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10787v1",
      "arxiv_id": "2602.10787v1",
      "title": "VulReaD: Knowledge-Graph-guided Software Vulnerability Reasoning and Detection",
      "authors": [
        "Samal Mukhtar",
        "Yinghua Yao",
        "Zhu Sun",
        "Mustafa Mustafa",
        "Yew Soon Ong",
        "Youcheng Sun"
      ],
      "abstract": "Software vulnerability detection (SVD) is a critical challenge in modern systems. Large language models (LLMs) offer natural-language explanations alongside predictions, but most work focuses on binary evaluation, and explanations often lack semantic consistency with Common Weakness Enumeration (CWE) categories. We propose VulReaD, a knowledge-graph-guided approach for vulnerability reasoning and detection that moves beyond binary classification toward CWE-level reasoning. VulReaD leverages a security knowledge graph (KG) as a semantic backbone and uses a strong teacher LLM to generate CWE-consistent contrastive reasoning supervision, enabling student model training without manual annotations. Students are fine-tuned with Odds Ratio Preference Optimization (ORPO) to encourage taxonomy-aligned reasoning while suppressing unsupported explanations. Across three real-world datasets, VulReaD improves binary F1 by 8-10% and multi-class classification by 30% Macro-F1 and 18% Micro-F1 compared to state-of-the-art baselines. Results show that LLMs outperform deep learning baselines in binary detection and that KG-guided reasoning enhances CWE coverage and interpretability.",
      "published": "2026-02-11",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10787v1",
      "url": "https://arxiv.org/abs/2602.10787v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR",
        "cs.IR"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "VulReaD introduces a knowledge-graph-guided framework for software vulnerability detection that shifts focus from binary classification to fine-grained, CWE-aligned reasoning. It constructs a security knowledge graph (KG) as a semantic backbone and employs a teacher LLM to generate high-quality, CWE-consistent contrastive reasoning data—eliminating the need for manual annotations—and fine-tunes student models using Odds Ratio Preference Optimization (ORPO) to enforce taxonomy-aligned explanations while penalizing unsupported ones. On three real-world datasets, VulReaD achieves substantial gains: +8–10% binary F1 and +30% Macro-F1 / +18% Micro-F1 for multi-class (CWE-level) classification over SOTA baselines. The results demonstrate that KG-guided LLM reasoning not only improves detection accuracy but also significantly enhances interpretability and coverage across CWE categories.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10780v1",
      "arxiv_id": "2602.10780v1",
      "title": "Kill it with FIRE: On Leveraging Latent Space Directions for Runtime Backdoor Mitigation in Deep Neural Networks",
      "authors": [
        "Enrico Ahlers",
        "Daniel Passon",
        "Yannic Noller",
        "Lars Grunske"
      ],
      "abstract": "Machine learning models are increasingly present in our everyday lives; as a result, they become targets of adversarial attackers seeking to manipulate the systems we interact with. A well-known vulnerability is a backdoor introduced into a neural network by poisoned training data or a malicious training process. Backdoors can be used to induce unwanted behavior by including a certain trigger in the input. Existing mitigations filter training data, modify the model, or perform expensive input modifications on samples. If a vulnerable model has already been deployed, however, those strategies are either ineffective or inefficient. To address this gap, we propose our inference-time backdoor mitigation approach called FIRE (Feature-space Inference-time REpair). We hypothesize that a trigger induces structured and repeatable changes in the model's internal representation. We view the trigger as directions in the latent spaces between layers that can be applied in reverse to correct the inference mechanism. Therefore, we turn the backdoored model against itself by manipulating its latent representations and moving a poisoned sample's features along the backdoor directions to neutralize the trigger. Our evaluation shows that FIRE has low computational overhead and outperforms current runtime mitigations on image benchmarks across various attacks, datasets, and network architectures.",
      "published": "2026-02-11",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10780v1",
      "url": "https://arxiv.org/abs/2602.10780v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CV"
      ],
      "published_official": true,
      "keywords": [
        "machine",
        "backdoor",
        "learning",
        "adversarial",
        "neural"
      ],
      "keyword_score": 5,
      "summary": "This paper introduces FIRE (Feature-space Inference-time REpair), a novel *runtime* backdoor mitigation method that operates during inference—without retraining or modifying the deployed model. FIRE identifies and reverses the latent-space direction(s) induced by a backdoor trigger across intermediate layers, hypothesizing that triggers cause structured, directional shifts in feature representations; it then applies corrective adjustments to poisoned inputs’ latent features to neutralize the backdoor effect. Evaluated on image benchmarks (e.g., CIFAR-10, ImageNet) under diverse backdoor attacks (e.g., BadNets, Blend) and architectures (ResNet, VGG), FIRE achieves strong attack suppression (often >90% reduction in attack success rate) while maintaining clean accuracy and incurring minimal computational overhead—outperforming existing inference-time defenses.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10778v1",
      "arxiv_id": "2602.10778v1",
      "title": "GoodVibe: Security-by-Vibe for LLM-Based Code Generation",
      "authors": [
        "Maximilian Thang",
        "Lichao Wu",
        "Sasha Behrouzi",
        "Mohamadreza Rostami",
        "Jona te Lintelo",
        "Stjepan Picek",
        "Ahmad-Reza Sadeghi"
      ],
      "abstract": "Large language models (LLMs) are increasingly used for code generation in fast, informal development workflows, often referred to as vibe coding, where speed and convenience are prioritized, and security requirements are rarely made explicit. In this setting, models frequently produce functionally correct but insecure code, creating a growing security risk. Existing approaches to improving code security rely on full-parameter fine-tuning or parameter-efficient adaptations, which are either costly and prone to catastrophic forgetting or operate at coarse granularity with limited interpretability and control.   We present GoodVibe, a neuron-level framework for improving the security of code language models by default. GoodVibe is based on the key insight that security-relevant reasoning is localized to a small subset of neurons. We identify these neurons using gradient-based attribution from a supervised security task and perform neuron-selective fine-tuning that updates only this security-critical subspace. To further reduce training cost, we introduce activation-driven neuron clustering, enabling structured updates with minimal overhead. We evaluate GoodVibe on six LLMs across security-critical programming languages, including C++, Java, Swift, and Go. GoodVibe substantially improves the security of generated code while preserving general model utility, achieving up to a 2.5x improvement over base models, matching or exceeding full fine-tuning with over 4,700x fewer trainable parameters, and reducing training computation by more than 3.6x compared to the parameter-efficient baseline (LoRA). Our results demonstrate that neuron-level optimization offers an effective and scalable approach to securing code generation without sacrificing efficiency or generality.",
      "published": "2026-02-11",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10778v1",
      "url": "https://arxiv.org/abs/2602.10778v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "GoodVibe introduces a neuron-level intervention framework to enhance the security of LLM-generated code—especially in “vibe coding” scenarios where security is often overlooked. It identifies security-critical neurons via gradient-based attribution on a supervised security task and applies selective fine-tuning only to those neurons, further optimized using activation-driven clustering for structured, low-overhead updates. Evaluated across six LLMs and four programming languages (C++, Java, Swift, Go), GoodVibe achieves up to 2.5× improvement in code security over base models—matching or surpassing full fine-tuning while using over 4,700× fewer trainable parameters and reducing training computation by >3.6× compared to LoRA. The method preserves general model utility and demonstrates that localized, interpretable neuron-level optimization is both highly effective and scalable for securing code generation.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10510v1",
      "arxiv_id": "2602.10510v1",
      "title": "Privacy-Utility Tradeoffs in Quantum Information Processing",
      "authors": [
        "Theshani Nuradha",
        "Sujeet Bhalerao",
        "Felix Leditzky"
      ],
      "abstract": "When sensitive information is encoded in data, it is important to ensure the privacy of information when attempting to learn useful information from the data. There is a natural tradeoff whereby increasing privacy requirements may decrease the utility of a learning protocol. In the quantum setting of differential privacy, such tradeoffs between privacy and utility have so far remained largely unexplored. In this work, we study optimal privacy-utility tradeoffs for both generic and application-specific utility metrics when privacy is quantified by $(\\varepsilon,δ)$-quantum local differential privacy. In the generic setting, we focus on optimizing fidelity and trace distance between the original state and the privatized state. We show that the depolarizing mechanism achieves the optimal utility for given privacy requirements. We then study the specific application of learning the expectation of an observable with respect to an input state when only given access to privatized states. We derive a lower bound on the number of samples of privatized data required to achieve a fixed accuracy guarantee with high probability. To prove this result, we employ existing lower bounds on private quantum hypothesis testing, thus showcasing the first operational use of them. We also devise private mechanisms that achieve optimal sample complexity with respect to the privacy parameters and accuracy parameters, demonstrating that utility can be significantly improved for specific tasks in contrast to the generic setting. In addition, we show that the number of samples required to privately learn observable expectation values scales as $Θ((\\varepsilon β)^{-2})$, where $\\varepsilon \\in (0,1)$ is the privacy parameter and $β$ is the accuracy tolerance. We conclude by initiating the study of private classical shadows, which promise useful applications for private learning tasks.",
      "published": "2026-02-11",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10510v1",
      "url": "https://arxiv.org/abs/2602.10510v1",
      "categories": [
        "quant-ph",
        "cs.CR",
        "cs.IT",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "differential",
        "privacy"
      ],
      "keyword_score": 2,
      "summary": "This paper establishes foundational privacy-utility tradeoffs in quantum information processing under $(\\varepsilon,\\delta)$-quantum local differential privacy. It proves that the depolarizing channel is optimal for generic utility metrics—namely, maximizing fidelity and minimizing trace distance between original and privatized quantum states. For the application-specific task of estimating an observable’s expectation value from privatized states, the authors derive a tight $\\Theta((\\varepsilon \\beta)^{-2})$ sample complexity lower bound (where $\\beta$ is accuracy tolerance), leveraging quantum hypothesis testing lower bounds—the first operational use of such bounds in private quantum learning. They also construct private mechanisms achieving this optimal sample complexity, demonstrating that task-specific designs significantly outperform generic privatization. Finally, the work introduces private classical shadows as a promising framework for scalable private quantum learning.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10498v1",
      "arxiv_id": "2602.10498v1",
      "title": "When Skills Lie: Hidden-Comment Injection in LLM Agents",
      "authors": [
        "Qianli Wang",
        "Boyang Ma",
        "Minghui Xu",
        "Yue Zhang"
      ],
      "abstract": "LLM agents often rely on Skills to describe available tools and recommended procedures. We study a hidden-comment prompt injection risk in this documentation layer: when a Markdown Skill is rendered to HTML, HTML comment blocks can become invisible to human reviewers, yet the raw text may still be supplied verbatim to the model. In experiments, we find that DeepSeek-V3.2 and GLM-4.5-Air can be influenced by malicious instructions embedded in a hidden comment appended to an otherwise legitimate Skill, yielding outputs that contain sensitive tool intentions. A short defensive system prompt that treats Skills as untrusted and forbids sensitive actions prevents these malicious tool calls and instead surfaces the suspicious hidden instructions.",
      "published": "2026-02-11",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10498v1",
      "url": "https://arxiv.org/abs/2602.10498v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "prompt",
        "injection"
      ],
      "keyword_score": 2,
      "summary": "This paper identifies a novel prompt injection vulnerability—“hidden-comment injection”—in LLM agents that use Markdown-formatted Skills to document tools, where HTML comment blocks (e.g., `<!-- ... -->`) become invisible to human reviewers during HTML rendering but remain visible to the LLM in raw text. The authors demonstrate that models like DeepSeek-V3.2 and GLM-4.5-Air can be manipulated by malicious instructions embedded in such hidden comments, causing them to execute sensitive or unintended tool calls. Their key finding is that a simple defensive system prompt—explicitly treating Skills as untrusted input and prohibiting sensitive actions—effectively mitigates the attack by rejecting malicious tool invocations and instead surfacing the hidden instructions for inspection. This highlights a critical trust boundary issue in agent documentation layers and proposes a lightweight, practical defense.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10481v1",
      "arxiv_id": "2602.10481v1",
      "title": "Protecting Context and Prompts: Deterministic Security for Non-Deterministic AI",
      "authors": [
        "Mohan Rajagopalan",
        "Vinay Rao"
      ],
      "abstract": "Large Language Model (LLM) applications are vulnerable to prompt injection and context manipulation attacks that traditional security models cannot prevent. We introduce two novel primitives--authenticated prompts and authenticated context--that provide cryptographically verifiable provenance across LLM workflows. Authenticated prompts enable self-contained lineage verification, while authenticated context uses tamper-evident hash chains to ensure integrity of dynamic inputs. Building on these primitives, we formalize a policy algebra with four proven theorems providing protocol-level Byzantine resistance--even adversarial agents cannot violate organizational policies. Five complementary defenses--from lightweight resource controls to LLM-based semantic validation--deliver layered, preventative security with formal guarantees. Evaluation against representative attacks spanning 6 exhaustive categories achieves 100% detection with zero false positives and nominal overhead. We demonstrate the first approach combining cryptographically enforced prompt lineage, tamper-evident context, and provable policy reasoning--shifting LLM security from reactive detection to preventative guarantees.",
      "published": "2026-02-11",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10481v1",
      "url": "https://arxiv.org/abs/2602.10481v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.MA"
      ],
      "published_official": true,
      "keywords": [
        "prompt",
        "llm",
        "security",
        "injection"
      ],
      "keyword_score": 4,
      "summary": "This paper introduces *authenticated prompts* and *authenticated context*—cryptographic primitives that enable verifiable provenance and tamper-evident integrity for LLM inputs and dynamic context, respectively. It formalizes a policy algebra with four proven theorems guaranteeing Byzantine-resistant enforcement of organizational policies, even against adversarial agents. The approach integrates five layered defenses—including lightweight resource controls and LLM-based semantic validation—to achieve preventative (not just reactive) security. Evaluated across six attack categories, the system achieves 100% detection with zero false positives and minimal overhead, representing the first framework to combine cryptographically enforced prompt lineage, tamper-evident context, and provably sound policy reasoning.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10453v1",
      "arxiv_id": "2602.10453v1",
      "title": "The Landscape of Prompt Injection Threats in LLM Agents: From Taxonomy to Analysis",
      "authors": [
        "Peiran Wang",
        "Xinfeng Li",
        "Chong Xiang",
        "Jinghuai Zhang",
        "Ying Li",
        "Lixia Zhang",
        "Xiaofeng Wang",
        "Yuan Tian"
      ],
      "abstract": "The evolution of Large Language Models (LLMs) has resulted in a paradigm shift towards autonomous agents, necessitating robust security against Prompt Injection (PI) vulnerabilities where untrusted inputs hijack agent behaviors. This SoK presents a comprehensive overview of the PI landscape, covering attacks, defenses, and their evaluation practices. Through a systematic literature review and quantitative analysis, we establish taxonomies that categorize PI attacks by payload generation strategies (heuristic vs. optimization) and defenses by intervention stages (text, model, and execution levels). Our analysis reveals a key limitation shared by many existing defenses and benchmarks: they largely overlook context-dependent tasks, in which agents are authorized to rely on runtime environmental observations to determine actions. To address this gap, we introduce AgentPI, a new benchmark designed to systematically evaluate agent behavior under context-dependent interaction settings. Using AgentPI, we empirically evaluate representative defenses and show that no single approach can simultaneously achieve high trustworthiness, high utility, and low latency. Moreover, we show that many defenses appear effective under existing benchmarks by suppressing contextual inputs, yet fail to generalize to realistic agent settings where context-dependent reasoning is essential. This SoK distills key takeaways and open research problems, offering structured guidance for future research and practical deployment of secure LLM agents.",
      "published": "2026-02-11",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10453v1",
      "url": "https://arxiv.org/abs/2602.10453v1",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "injection",
        "agent",
        "prompt",
        "llm"
      ],
      "keyword_score": 5,
      "summary": "This paper provides a systematic overview (SoK) of Prompt Injection (PI) threats in LLM-based autonomous agents, introducing novel taxonomies for PI attacks (by payload generation: heuristic vs. optimization) and defenses (by intervention stage: text, model, or execution level). Critically, it identifies a major gap in existing research: most defenses and benchmarks ignore *context-dependent tasks*, where agents must reason over dynamic environmental observations—leading to overly optimistic evaluations. To address this, the authors propose **AgentPI**, a new benchmark that explicitly models realistic, context-aware agent interactions. Empirical evaluation on AgentPI reveals that no current defense simultaneously achieves high trustworthiness, utility, and low latency—and many defenses fail in context-rich settings because they suppress or ignore contextual inputs rather than securely integrating them. The work thus reframes PI security around context-aware reasoning and highlights urgent open challenges for robust, deployable LLM agents.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10418v1",
      "arxiv_id": "2602.10418v1",
      "title": "SecCodePRM: A Process Reward Model for Code Security",
      "authors": [
        "Weichen Yu",
        "Ravi Mangal",
        "Yinyi Luo",
        "Kai Hu",
        "Jingxuan He",
        "Corina S. Pasareanu",
        "Matt Fredrikson"
      ],
      "abstract": "Large Language Models are rapidly becoming core components of modern software development workflows, yet ensuring code security remains challenging. Existing vulnerability detection pipelines either rely on static analyzers or use LLM/GNN-based detectors trained with coarse program-level supervision. Both families often require complete context, provide sparse end-of-completion feedback, and can degrade as code length grows, making them ill-suited for real-time, prefix-level assessment during interactive coding and streaming generation. We propose SecCodePRM, a security-oriented process reward model that assigns a context-aware, step-level security score along a code trajectory. To train the model, we derive step-level supervision labels from static analyzers and expert annotations, allowing the model to attend more precisely to fine-grained regions associated with inter-procedural vulnerabilities. SecCodePRM has three applications: full-code vulnerability detection (VD), partial-code VD, and secure code generation (CG). For VD, SecCodePRM uses risk-sensitive aggregation that emphasizes high-risk steps; for CG, SecCodePRM supports inference-time scaling by ranking candidate continuations and favoring higher cumulative reward. This design yields dense, real-time feedback that scales to long-horizon generation. Empirically, SecCodePRM outperforms prior approaches in all three settings, while preserving code functional correctness, suggesting improved security without a safety-utility tradeoff.",
      "published": "2026-02-11",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10418v1",
      "url": "https://arxiv.org/abs/2602.10418v1",
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "published_official": true,
      "keywords": [
        "inference",
        "train",
        "security",
        "llm"
      ],
      "keyword_score": 4,
      "summary": "SecCodePRM introduces a novel process reward model that provides fine-grained, step-level security scoring during code generation—unlike prior coarse, program-level vulnerability detectors. It is trained on step-level supervision derived from static analyzers and expert annotations, enabling precise attention to inter-procedural vulnerability patterns in partial or streaming code contexts. The model supports three key applications: full-code and partial-code vulnerability detection (using risk-sensitive aggregation) and secure code generation (via reward-guided candidate ranking and cumulative reward optimization). Empirically, SecCodePRM outperforms existing methods across all tasks while maintaining functional correctness—demonstrating improved security without sacrificing utility.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10870v1",
      "arxiv_id": "2602.10870v1",
      "title": "FedPS: Federated data Preprocessing via aggregated Statistics",
      "authors": [
        "Xuefeng Xu",
        "Graham Cormode"
      ],
      "abstract": "Federated Learning (FL) enables multiple parties to collaboratively train machine learning models without sharing raw data. However, before training, data must be preprocessed to address missing values, inconsistent formats, and heterogeneous feature scales. This preprocessing stage is critical for model performance but is largely overlooked in FL research. In practical FL systems, privacy constraints prohibit centralizing raw data, while communication efficiency introduces further challenges for distributed preprocessing. We introduce FedPS, a unified framework for federated data preprocessing based on aggregated statistics. FedPS leverages data-sketching techniques to efficiently summarize local datasets while preserving essential statistical information. Building on these summaries, we design federated algorithms for feature scaling, encoding, discretization, and missing-value imputation, and extend preprocessing-related models such as k-Means, k-Nearest Neighbors, and Bayesian Linear Regression to both horizontal and vertical FL settings. FedPS provides flexible, communication-efficient, and consistent preprocessing pipelines for practical FL deployments.",
      "published": "2026-02-11",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10870v1",
      "url": "https://arxiv.org/abs/2602.10870v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "federated",
        "learning"
      ],
      "keyword_score": 2,
      "summary": "FedPS introduces a unified, privacy-preserving framework for federated data preprocessing—addressing a critical but underexplored challenge in FL where raw data cannot be centralized. It uses lightweight data-sketching techniques to compute and aggregate local statistical summaries (e.g., means, quantiles, frequency counts) across clients with minimal communication overhead. Based on these aggregated statistics, FedPS enables consistent, distributed execution of key preprocessing tasks—including feature scaling, categorical encoding, discretization, and missing-value imputation—as well as extensions of preprocessing-dependent models (e.g., k-Means, k-NN, Bayesian Linear Regression) to both horizontal and vertical FL settings. Experiments demonstrate that FedPS achieves preprocessing consistency and model performance comparable to centralized preprocessing, while significantly reducing communication cost and preserving data privacy.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10820v1",
      "arxiv_id": "2602.10820v1",
      "title": "Adaptive Sampling for Private Worst-Case Group Optimization",
      "authors": [
        "Max Cairney-Leeming",
        "Amartya Sanyal",
        "Christoph H. Lampert"
      ],
      "abstract": "Models trained by minimizing the average loss often fail to be accurate on small or hard-to-learn groups of the data. Various methods address this issue by optimizing a weighted objective that focuses on the worst-performing groups. However, this approach becomes problematic when learning with differential privacy, as unequal data weighting can result in inhomogeneous privacy guarantees, in particular weaker privacy for minority groups. In this work, we introduce a new algorithm for differentially private worst-case group optimization called ASC (Adaptively Sampled and Clipped Worst-case Group Optimization). It adaptively controls both the sampling rate and the clipping threshold of each group. Thereby, it allows for harder-to-learn groups to be sampled more often while ensuring consistent privacy guarantees across all groups. Comparing ASC to prior work, we show that it results in lower-variance gradients, tighter privacy guarantees, and substantially higher worst-case group accuracy without sacrificing overall average accuracy.",
      "published": "2026-02-11",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10820v1",
      "url": "https://arxiv.org/abs/2602.10820v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "differential",
        "privacy"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces ASC, a differentially private algorithm for worst-case group optimization that addresses the privacy-accuracy trade-off in group-fair learning. Unlike prior methods that assign fixed or data-dependent weights—leading to uneven privacy budgets across groups—ASC adaptively adjusts both the sampling rate and gradient clipping threshold per group, ensuring uniform privacy guarantees while prioritizing harder-to-learn (e.g., minority) groups. The method achieves lower gradient variance and tighter privacy bounds by balancing representation and sensitivity across groups. Experiments show ASC significantly improves worst-case group accuracy over existing private baselines, without degrading average accuracy.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10765v1",
      "arxiv_id": "2602.10765v1",
      "title": "Collaborative Threshold Watermarking",
      "authors": [
        "Tameem Bakr",
        "Anish Ambreth",
        "Nils Lukas"
      ],
      "abstract": "In federated learning (FL), $K$ clients jointly train a model without sharing raw data. Because each participant invests data and compute, clients need mechanisms to later prove the provenance of a jointly trained model. Model watermarking embeds a hidden signal in the weights, but naive approaches either do not scale with many clients as per-client watermarks dilute as $K$ grows, or give any individual client the ability to verify and potentially remove the watermark. We introduce $(t,K)$-threshold watermarking: clients collaboratively embed a shared watermark during training, while only coalitions of at least $t$ clients can reconstruct the watermark key and verify a suspect model. We secret-share the watermark key $τ$ so that coalitions of fewer than $t$ clients cannot reconstruct it, and verification can be performed without revealing $τ$ in the clear. We instantiate our protocol in the white-box setting and evaluate on image classification. Our watermark remains detectable at scale ($K=128$) with minimal accuracy loss and stays above the detection threshold ($z\\ge 4$) under attacks including adaptive fine-tuning using up to 20% of the training data.",
      "published": "2026-02-11",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10765v1",
      "url": "https://arxiv.org/abs/2602.10765v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "federated",
        "learning"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces *collaborative threshold watermarking*, a novel model watermarking scheme for federated learning that enables $K$ clients to jointly embed a shared watermark during training while ensuring only coalitions of at least $t$ clients can verify it—preventing individual clients from removing or falsely claiming ownership. The method uses secret sharing to distribute the watermark key $\\tau$ among clients, allowing verification via zero-knowledge–style checks without revealing $\\tau$ in plaintext. Evaluated on image classification with up to $K = 128$ clients, the approach achieves robust watermark detection (statistical significance $z \\geq 4$) with negligible accuracy loss and withstands adaptive fine-tuning attacks using up to 20% of the training data.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10631v1",
      "arxiv_id": "2602.10631v1",
      "title": "Generative clinical time series models trained on moderate amounts of patient data are privacy preserving",
      "authors": [
        "Rustam Zhumagambetov",
        "Niklas Giesa",
        "Sebastian D. Boie",
        "Stefan Haufe"
      ],
      "abstract": "Sharing medical data for machine learning model training purposes is often impossible due to the risk of disclosing identifying information about individual patients. Synthetic data produced by generative artificial intelligence (genAI) models trained on real data is often seen as one possible solution to comply with privacy regulations. While powerful genAI models for heterogeneous hospital time series have recently been introduced, such modeling does not guarantee privacy protection, as the generated data may still reveal identifying information about individuals in the models' training cohort. Applying established privacy mechanisms to generative time series models, however, proves challenging as post-hoc data anonymization through k-anonymization or similar techniques is limited, while model-centered privacy mechanisms that implement differential privacy (DP) may lead to unstable training, compromising the utility of generated data. Given these known limitations, privacy audits for generative time series models are currently indispensable regardless of the concrete privacy mechanisms applied to models and/or data. In this work, we use a battery of established privacy attacks to audit state-of-the-art hospital time series models, trained on the public MIMIC-IV dataset, with respect to privacy preservation. Furthermore, the eICU dataset was used to mount a privacy attack against the synthetic data generator trained on the MIMIC-IV dataset. Results show that established privacy attacks are ineffective against generated multivariate clinical time series when synthetic data generators are trained on large enough training datasets. Furthermore, we discuss how the use of existing DP mechanisms for these synthetic data generators would not bring desired improvement in privacy, but only a decrease in utility for machine learning prediction tasks.",
      "published": "2026-02-11",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10631v1",
      "url": "https://arxiv.org/abs/2602.10631v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "dp",
        "machine",
        "learning",
        "differential",
        "privacy"
      ],
      "keyword_score": 5,
      "summary": "This paper investigates whether generative AI models for clinical time series—trained on moderate-to-large real patient data (MIMIC-IV)—naturally provide privacy protection without requiring explicit privacy mechanisms like differential privacy (DP). The authors conduct rigorous privacy audits using multiple established attacks (e.g., membership inference, reconstruction, and attribute inference) against synthetic time series generated by state-of-the-art models, including cross-dataset attacks using eICU data. Key results show that these attacks consistently fail when generators are trained on sufficiently large datasets (>10K patients), indicating strong empirical privacy preservation. Crucially, the study finds that adding DP to such models degrades synthetic data utility for downstream ML tasks without meaningfully improving privacy beyond what moderate-scale training already affords. Thus, the main contribution is the empirical demonstration that well-trained generative time series models on realistic clinical data volumes can be inherently privacy-preserving—challenging the assumption that DP or other formal guarantees are always necessary.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10595v1",
      "arxiv_id": "2602.10595v1",
      "title": "Roughness-Informed Federated Learning",
      "authors": [
        "Mohammad Partohaghighi",
        "Roummel Marcia",
        "Bruce J. West",
        "YangQuan Chen"
      ],
      "abstract": "Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy, yet faces challenges in non-independent and identically distributed (non-IID) settings due to client drift, which impairs convergence. We propose RI-FedAvg, a novel FL algorithm that mitigates client drift by incorporating a Roughness Index (RI)-based regularization term into the local objective, adaptively penalizing updates based on the fluctuations of local loss landscapes. This paper introduces RI-FedAvg, leveraging the RI to quantify the roughness of high-dimensional loss functions, ensuring robust optimization in heterogeneous settings. We provide a rigorous convergence analysis for non-convex objectives, establishing that RI-FedAvg converges to a stationary point under standard assumptions. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100 demonstrate that RI-FedAvg outperforms state-of-the-art baselines, including FedAvg, FedProx, FedDyn, SCAFFOLD, and DP-FedAvg, achieving higher accuracy and faster convergence in non-IID scenarios. Our results highlight RI-FedAvg's potential to enhance the robustness and efficiency of federated learning in practical, heterogeneous environments.",
      "published": "2026-02-11",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10595v1",
      "url": "https://arxiv.org/abs/2602.10595v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "dp",
        "federated",
        "learning"
      ],
      "keyword_score": 3,
      "summary": "This paper introduces RI-FedAvg, a novel federated learning algorithm that addresses client drift in non-IID settings by incorporating a Roughness Index (RI)—a measure of local loss landscape fluctuations—into the local optimization objective as an adaptive regularization term. The method modifies standard FedAvg by penalizing updates from clients with highly irregular (rough) loss surfaces, promoting more stable and aligned client models. The authors provide a rigorous convergence guarantee for non-convex objectives, proving RI-FedAvg converges to a stationary point under standard FL assumptions. Experiments on MNIST, CIFAR-10, and CIFAR-100 show RI-FedAvg consistently outperforms leading baselines (e.g., FedProx, SCAFFOLD, FedDyn) in both accuracy and convergence speed under strong non-IID data partitioning.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10584v1",
      "arxiv_id": "2602.10584v1",
      "title": "When Gradient Clipping Becomes a Control Mechanism for Differential Privacy in Deep Learning",
      "authors": [
        "Mohammad Partohaghighi",
        "Roummel Marcia",
        "Bruce J. West",
        "YangQuan Chen"
      ],
      "abstract": "Privacy-preserving training on sensitive data commonly relies on differentially private stochastic optimization with gradient clipping and Gaussian noise. The clipping threshold is a critical control knob: if set too small, systematic over-clipping induces optimization bias; if too large, injected noise dominates updates and degrades accuracy. Existing adaptive clipping methods often depend on per-example gradient norm statistics, adding computational overhead and introducing sensitivity to datasets and architectures. We propose a control-driven clipping strategy that adapts the threshold using a lightweight, weight-only spectral diagnostic computed from model parameters. At periodic probe steps, the method analyzes a designated weight matrix via spectral decomposition and estimates a heavy-tailed spectral indicator associated with training stability. This indicator is smoothed over time and fed into a bounded feedback controller that updates the clipping threshold multiplicatively in the log domain. Because the controller uses only parameters produced during privacy-preserving training, the resulting threshold updates are post-processing and do not increase privacy loss beyond that of the underlying DP optimizer under standard composition accounting.",
      "published": "2026-02-11",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10584v1",
      "url": "https://arxiv.org/abs/2602.10584v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "dp",
        "differential",
        "privacy"
      ],
      "keyword_score": 3,
      "summary": "This paper introduces a novel, control-theoretic approach to adaptive gradient clipping in differentially private (DP) deep learning, replacing computationally expensive per-example gradient norm tracking with a lightweight, weight-only spectral diagnostic. The method periodically computes a heavy-tailed spectral indicator from the singular values of a designated weight matrix—requiring no additional gradient computations—and feeds it into a bounded feedback controller that multiplicatively adjusts the clipping threshold in the log domain. Crucially, because adaptation relies solely on model parameters already generated during DP training, it incurs no extra privacy cost under standard composition accounting. Experiments show the method improves both optimization stability and final model accuracy compared to fixed and existing adaptive clipping schemes—especially on vision and language tasks—while reducing computational overhead.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10652v1",
      "arxiv_id": "2602.10652v1",
      "title": "UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory",
      "authors": [
        "Yongshi Ye",
        "Hui Jiang",
        "Feihu Jiang",
        "Tian Lan",
        "Yichao Du",
        "Biao Fu",
        "Xiaodong Shi",
        "Qianghuai Jia",
        "Longyue Wang",
        "Weihua Luo"
      ],
      "abstract": "Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-based agents, where extraction (distilling insights from experience) and management (updating the memory bank) must be tightly coordinated. Existing methods predominately optimize memory management while treating memory extraction as a static process, resulting in poor generalization, where agents accumulate instance-specific noise rather than robust memories. To address this, we propose Unified Memory Extraction and Management (UMEM), a self-evolving agent framework that jointly optimizes a Large Language Model to simultaneous extract and manage memories. To mitigate overfitting to specific instances, we introduce Semantic Neighborhood Modeling and optimize the model with a neighborhood-level marginal utility reward via GRPO. This approach ensures memory generalizability by evaluating memory utility across clusters of semantically related queries. Extensive experiments across five benchmarks demonstrate that UMEM significantly outperforms highly competitive baselines, achieving up to a 10.67% improvement in multi-turn interactive tasks. Futhermore, UMEM maintains a monotonic growth curve during continuous evolution. Codes and models will be publicly released.",
      "published": "2026-02-11",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10652v1",
      "url": "https://arxiv.org/abs/2602.10652v1",
      "categories": [
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces UMEM, a novel framework that jointly optimizes memory extraction (distilling insights from experience) and memory management (updating the memory bank) in LLM-based agents—unifying what prior work treats as separate, static processes. To enhance generalizability and reduce instance-specific noise, UMEM employs Semantic Neighborhood Modeling and a neighborhood-level marginal utility reward optimized via GRPO, evaluating memory usefulness across semantically clustered queries rather than isolated instances. Experiments on five benchmarks show UMEM achieves up to a 10.67% improvement in multi-turn interactive tasks over strong baselines and exhibits monotonic performance growth during continuous self-evolution. The framework thus advances self-evolving agents by enabling robust, generalizable memory formation through unified, neighborhood-aware optimization.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10384v1",
      "arxiv_id": "2602.10384v1",
      "title": "When Tables Go Crazy: Evaluating Multimodal Models on French Financial Documents",
      "authors": [
        "Virginie Mouilleron",
        "Théo Lasnier",
        "Djamé Seddah"
      ],
      "abstract": "Vision-language models (VLMs) perform well on many document understanding tasks, yet their reliability in specialized, non-English domains remains underexplored. This gap is especially critical in finance, where documents mix dense regulatory text, numerical tables, and visual charts, and where extraction errors can have real-world consequences. We introduce Multimodal Finance Eval, the first multimodal benchmark for evaluating French financial document understanding. The dataset contains 1,204 expert-validated questions spanning text extraction, table comprehension, chart interpretation, and multi-turn conversational reasoning, drawn from real investment prospectuses, KIDs, and PRIIPs. We evaluate six open-weight VLMs (8B-124B parameters) using an LLM-as-judge protocol. While models achieve strong performance on text and table tasks (85-90% accuracy), they struggle with chart interpretation (34-62%). Most notably, multi-turn dialogue reveals a sharp failure mode: early mistakes propagate across turns, driving accuracy down to roughly 50% regardless of model size.   These results show that current VLMs are effective for well-defined extraction tasks but remain brittle in interactive, multi-step financial analysis. Multimodal Finance Eval offers a challenging benchmark to measure and drive progress in this high-stakes setting.",
      "published": "2026-02-11",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10384v1",
      "url": "https://arxiv.org/abs/2602.10384v1",
      "categories": [
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces *Multimodal Finance Eval*, the first benchmark for evaluating vision-language models (VLMs) on French financial documents—such as investment prospectuses and PRIIPs—featuring 1,204 expert-validated questions across text extraction, table comprehension, chart interpretation, and multi-turn reasoning. Using an LLM-as-judge evaluation protocol, the authors assess six open-weight VLMs (8B–124B parameters) and find strong performance on text and table tasks (85–90% accuracy) but marked weaknesses in chart interpretation (34–62%). Crucially, multi-turn dialogue exposes a critical brittleness: early errors propagate across turns, collapsing overall accuracy to ~50%, irrespective of model scale. The study highlights that current VLMs are reliable for isolated extraction but fail in interactive, sequential financial analysis—underscoring the need for more robust, domain-specific multimodal reasoning.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10100v1",
      "arxiv_id": "2602.10100v1",
      "title": "Towards Explainable Federated Learning: Understanding the Impact of Differential Privacy",
      "authors": [
        "Júlio Oliveira",
        "Rodrigo Ferreira",
        "André Riker",
        "Glaucio H. S. Carvalho",
        "Eirini Eleni Tsilopoulou"
      ],
      "abstract": "Data privacy and eXplainable Artificial Intelligence (XAI) are two important aspects for modern Machine Learning systems. To enhance data privacy, recent machine learning models have been designed as a Federated Learning (FL) system. On top of that, additional privacy layers can be added, via Differential Privacy (DP). On the other hand, to improve explainability, ML must consider more interpretable approaches with reduced number of features and less complex internal architecture. In this context, this paper aims to achieve a machine learning (ML) model that combines enhanced data privacy with explainability. So, we propose a FL solution, called Federated EXplainable Trees with Differential Privacy (FEXT-DP), that: (i) is based on Decision Trees, since they are lightweight and have superior explainability than neural networks-based FL systems; (ii) provides additional layer of data privacy protection applying Differential Privacy (DP) to the Tree-Based model. However, there is a side effect adding DP: it harms the explainability of the system. So, this paper also presents the impact of DP protection on the explainability of the ML model. The carried out performance assessment shows improvements of FEXT-DP in terms of a faster training, i.e., numbers of rounds, Mean Squared Error and explainability.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10100v1",
      "url": "https://arxiv.org/abs/2602.10100v1",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "federated",
        "machine",
        "dp",
        "learning",
        "differential",
        "privacy"
      ],
      "keyword_score": 6,
      "summary": "This paper introduces Federated EXplainable Trees with Differential Privacy (FEXT-DP), a novel FL framework that integrates decision trees—chosen for their inherent interpretability and computational efficiency—with differential privacy (DP) to strengthen data privacy in decentralized settings. The authors systematically evaluate how DP noise injection affects both model accuracy and explainability, revealing a trade-off where stronger DP guarantees degrade feature importance stability and tree structure fidelity. Despite this, FEXT-DP achieves faster convergence (fewer communication rounds), lower mean squared error, and better overall explainability compared to DP-enhanced neural-network-based FL baselines. The key contribution is a principled, tree-based FL design that quantifies and mitigates the explainability cost of DP—demonstrating that high privacy need not preclude high interpretability when architecture and privacy mechanisms are co-designed.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09774v1",
      "arxiv_id": "2602.09774v1",
      "title": "QRS: A Rule-Synthesizing Neuro-Symbolic Triad for Autonomous Vulnerability Discovery",
      "authors": [
        "George Tsigkourakos",
        "Constantinos Patsakis"
      ],
      "abstract": "Static Application Security Testing (SAST) tools are integral to modern DevSecOps pipelines, yet tools like CodeQL, Semgrep, and SonarQube remain fundamentally constrained: they require expert-crafted queries, generate excessive false positives, and detect only predefined vulnerability patterns. Recent work has explored augmenting SAST with Large Language Models (LLMs), but these approaches typically use LLMs to triage existing tool outputs rather than to reason about vulnerability semantics directly. We introduce QRS (Query, Review, Sanitize), a neuro-symbolic framework that inverts this paradigm. Rather than filtering results from static rules, QRS employs three autonomous agents that generate CodeQL queries from a structured schema definition and few-shot examples, then validate findings through semantic reasoning and automated exploit synthesis. This architecture enables QRS to discover vulnerability classes beyond predefined patterns while substantially reducing false positives. We evaluate QRS on full Python packages rather than isolated snippets. In 20 historical CVEs in popular PyPI libraries, QRS achieves 90.6% detection accuracy. Applied to the 100 most-downloaded PyPI packages, QRS identified 39 medium-to-high-severity vulnerabilities, 5 of which were assigned new CVEs, 5 received documentation updates, while the remaining 29 were independently discovered by concurrent researchers, validating both the severity and discoverability of these findings. QRS accomplishes this with low time overhead and manageable token costs, demonstrating that LLM-driven query synthesis and code review can complement manually curated rule sets and uncover vulnerability patterns that evade existing industry tools.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09774v1",
      "url": "https://arxiv.org/abs/2602.09774v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "QRS is a neuro-symbolic framework that autonomously synthesizes CodeQL queries for vulnerability discovery—reversing the traditional SAST paradigm by using LLM-powered agents to *generate* queries from schema definitions and few-shot examples, then validate findings via semantic reasoning and exploit synthesis. Unlike prior LLM-augmented SAST tools that only triage outputs, QRS integrates query generation, result review, and sanitization into a unified triad (“Query, Review, Sanitize”). Evaluated on real-world Python packages, it achieved 90.6% accuracy on 20 historical CVEs and discovered 39 medium-to-high-severity vulnerabilities in the top 100 PyPI packages—including 5 newly assigned CVEs—while maintaining low computational overhead. The results demonstrate QRS’s ability to uncover previously unknown vulnerability patterns beyond predefined rules and significantly reduce false positives.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09634v1",
      "arxiv_id": "2602.09634v1",
      "title": "LLM-FS: Zero-Shot Feature Selection for Effective and Interpretable Malware Detection",
      "authors": [
        "Naveen Gill",
        "Ajvad Haneef K",
        "Madhu Kumar S D"
      ],
      "abstract": "Feature selection (FS) remains essential for building accurate and interpretable detection models, particularly in high-dimensional malware datasets. Conventional FS methods such as Extra Trees, Variance Threshold, Tree-based models, Chi-Squared tests, ANOVA, Random Selection, and Sequential Attention rely primarily on statistical heuristics or model-driven importance scores, often overlooking the semantic context of features. Motivated by recent progress in LLM-driven FS, we investigate whether large language models (LLMs) can guide feature selection in a zero-shot setting, using only feature names and task descriptions, as a viable alternative to traditional approaches. We evaluate multiple LLMs (GPT-5.0, GPT-4.0, Gemini-2.5 etc.) on the EMBOD dataset (a fusion of EMBER and BODMAS benchmark datasets), comparing them against established FS methods across several classifiers, including Random Forest, Extra Trees, MLP, and KNN. Performance is assessed using accuracy, precision, recall, F1, AUC, MCC, and runtime. Our results demonstrate that LLM-guided zero-shot feature selection achieves competitive performance with traditional FS methods while offering additional advantages in interpretability, stability, and reduced dependence on labeled data. These findings position zero-shot LLM-based FS as a promising alternative strategy for effective and interpretable malware detection, paving the way for knowledge-guided feature selection in security-critical applications",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09634v1",
      "url": "https://arxiv.org/abs/2602.09634v1",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces LLM-FS, a novel zero-shot feature selection method for malware detection that leverages large language models (e.g., GPT-4, GPT-5, Gemini-2.5) to rank features *solely* based on feature names and task descriptions—without requiring labeled data or model training. Evaluated on the EMBOD dataset (a fusion of EMBER and BODMAS) across multiple classifiers (Random Forest, MLP, etc.), LLM-FS achieves competitive accuracy, F1, AUC, and MCC compared to traditional FS methods (e.g., Extra Trees, Chi-Squared, ANOVA), while significantly improving interpretability and stability. Notably, it reduces reliance on labeled data and avoids statistical or model-dependent heuristics that ignore semantic feature meaning. The study establishes zero-shot LLM-guided FS as a viable, knowledge-driven alternative for security-critical, high-dimensional malware analysis.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09629v1",
      "arxiv_id": "2602.09629v1",
      "title": "Stop Testing Attacks, Start Diagnosing Defenses: The Four-Checkpoint Framework Reveals Where LLM Safety Breaks",
      "authors": [
        "Hayfa Dhabhi",
        "Kashyap Thimmaraju"
      ],
      "abstract": "Large Language Models (LLMs) deploy safety mechanisms to prevent harmful outputs, yet these defenses remain vulnerable to adversarial prompts. While existing research demonstrates that jailbreak attacks succeed, it does not explain \\textit{where} defenses fail or \\textit{why}.   To address this gap, we propose that LLM safety operates as a sequential pipeline with distinct checkpoints. We introduce the \\textbf{Four-Checkpoint Framework}, which organizes safety mechanisms along two dimensions: processing stage (input vs.\\ output) and detection level (literal vs.\\ intent). This creates four checkpoints, CP1 through CP4, each representing a defensive layer that can be independently evaluated. We design 13 evasion techniques, each targeting a specific checkpoint, enabling controlled testing of individual defensive layers.   Using this framework, we evaluate GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across 3,312 single-turn, black-box test cases. We employ an LLM-as-judge approach for response classification and introduce Weighted Attack Success Rate (WASR), a severity-adjusted metric that captures partial information leakage overlooked by binary evaluation.   Our evaluation reveals clear patterns. Traditional Binary ASR reports 22.6\\% attack success. However, WASR reveals 52.7\\%, a 2.3$\\times$ higher vulnerability. Output-stage defenses (CP3, CP4) prove weakest at 72--79\\% WASR, while input-literal defenses (CP1) are strongest at 13\\% WASR. Claude achieves the strongest safety (42.8\\% WASR), followed by GPT-5 (55.9\\%) and Gemini (59.5\\%).   These findings suggest that current defenses are strongest at input-literal checkpoints but remain vulnerable to intent-level manipulation and output-stage techniques. The Four-Checkpoint Framework provides a structured approach for identifying and addressing safety vulnerabilities in deployed systems.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09629v1",
      "url": "https://arxiv.org/abs/2602.09629v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY",
        "cs.ET",
        "cs.HC"
      ],
      "published_official": true,
      "keywords": [
        "jailbreak",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces the **Four-Checkpoint Framework**, a novel conceptual and evaluative model that decomposes LLM safety into four distinct defensive layers—organized by processing stage (input/output) and detection level (literal/intent)—to pinpoint *where* and *why* safety mechanisms fail. The authors design 13 targeted evasion techniques, each isolating one checkpoint, and evaluate GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro on 3,312 black-box test cases using an LLM-as-judge methodology and a new severity-aware metric, **Weighted Attack Success Rate (WASR)**. Results reveal that traditional binary attack success rate (22.6%) vastly underestimates risk: WASR uncovers 52.7% overall vulnerability—driven primarily by weak output-stage defenses (72–79% WASR), while input-literal defenses (CP1) are strongest (13% WASR). Claude demonstrates the highest safety (42.8% WASR), followed by GPT-5 (55.9%) and Gemini (59.5%), highlighting significant variation across models and the framework’s utility for granular, actionable safety diagnosis.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09627v1",
      "arxiv_id": "2602.09627v1",
      "title": "Parallel Composition for Statistical Privacy",
      "authors": [
        "Dennis Breutigam",
        "Rüdiger Reischuk"
      ],
      "abstract": "Differential Privacy (DP) considers a scenario in which an adversary has almost complete information about the entries of a database. This worst-case assumption is likely to overestimate the privacy threat faced by an individual in practice. In contrast, Statistical Privacy (SP), as well as related notions such as noiseless privacy or limited background knowledge privacy, describe a setting in which the adversary knows the distribution of the database entries, but not their exact realizations. In this case, privacy analysis must account for the interaction between uncertainty induced by the entropy of the underlying distributions and privacy mechanisms that distort query answers, which can be highly non-trivial.   This paper investigates this problem for multiple queries (composition). A privacy mechanism is proposed that is based on subsampling and randomly partitioning the database to bound the dependency among queries. This way for the first time, to the best of our knowledge, upper privacy bounds against limited adversaries are obtained without any further restriction on the database.   These bounds show that in realistic application scenarios taking the entropy of distributions into account yields improvements of privacy and precision guarantees. We illustrate examples where for fixed privacy parameters and utility loss SP allows significantly more queries than DP.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09627v1",
      "url": "https://arxiv.org/abs/2602.09627v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "dp",
        "differential",
        "privacy"
      ],
      "keyword_score": 3,
      "summary": "This paper introduces a novel privacy framework—Statistical Privacy (SP)—that models adversaries with knowledge of the *distribution* of database entries (rather than exact values, as in differential privacy), enabling more realistic and tighter privacy analysis. The authors propose a composition mechanism based on subsampling and random database partitioning to decouple queries and bound dependencies, yielding the first SP privacy bounds for multiple queries without restrictive assumptions on the database structure or distribution. Key results show that leveraging data entropy significantly improves both privacy guarantees and utility: for fixed privacy and accuracy requirements, SP supports substantially more queries than DP. This demonstrates a concrete advantage of statistical modeling over worst-case DP assumptions in practical settings.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09606v1",
      "arxiv_id": "2602.09606v1",
      "title": "When Handshakes Tell the Truth: Detecting Web Bad Bots via TLS Fingerprints",
      "authors": [
        "Ghalia Jarad",
        "Kemal Bicakci"
      ],
      "abstract": "Automated traffic continued to surpass human-generated traffic on the web, and a rising proportion of this automation was explicitly malicious. Evasive bots could pretend to be real users, even solve Captchas and mimic human interaction patterns. This work explores a less intrusive, protocol-level method: using TLS fingerprinting with the JA4 technique to tell apart bots from real users. Two gradient-boosted machine learning classifiers (XGBoost and CatBoost) were trained and evaluated on a dataset of real TLS fingerprints (JA4DB) after feature extraction, which derived informative signals from JA4 fingerprints that describe TLS handshake parameters. The CatBoost model performed better, achieving an AUC of 0.998 and an F1 score of 0.9734. It was accurate 0.9863 of the time on the test set. The XGBoost model showed almost similar results. Feature significance analyses identified JA4 components, especially ja4\\_b, cipher\\_count, and ext\\_count, as the most influential on model effectiveness. Future research will extend this method to new protocols, such as HTTP/3, and add additional device-fingerprinting features to test how well the system resists advanced bot evasion tactics.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09606v1",
      "url": "https://arxiv.org/abs/2602.09606v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces a novel, low-intrusion method for detecting malicious web bots by analyzing TLS handshake fingerprints using the JA4 technique—capturing protocol-level characteristics that are hard for bots to spoof. The authors train and compare two gradient-boosted classifiers (XGBoost and CatBoost) on real-world JA4 fingerprints from the JA4DB dataset, extracting features like `ja4_b`, `cipher_count`, and `ext_count`. CatBoost achieves exceptional performance: 0.998 AUC, 0.9734 F1 score, and 98.63% accuracy, outperforming XGBoost slightly. Feature importance analysis confirms that JA4’s behavioral and structural TLS parameters—not user-agent strings or IP-based signals—are the most discriminative indicators of bot traffic. The work demonstrates TLS fingerprinting as a robust, scalable defense against increasingly evasive automated threats.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09499v1",
      "arxiv_id": "2602.09499v1",
      "title": "Computationally Efficient Replicable Learning of Parities",
      "authors": [
        "Moshe Noivirt",
        "Jessica Sorrell",
        "Eliad Tsfadia"
      ],
      "abstract": "We study the computational relationship between replicability (Impagliazzo et al. [STOC `22], Ghazi et al. [NeurIPS `21]) and other stability notions. Specifically, we focus on replicable PAC learning and its connections to differential privacy (Dwork et al. [TCC 2006]) and to the statistical query (SQ) model (Kearns [JACM `98]). Statistically, it was known that differentially private learning and replicable learning are equivalent and strictly more powerful than SQ-learning. Yet, computationally, all previously known efficient (i.e., polynomial-time) replicable learning algorithms were confined to SQ-learnable tasks or restricted distributions, in contrast to differentially private learning.   Our main contribution is the first computationally efficient replicable algorithm for realizable learning of parities over arbitrary distributions, a task that is known to be hard in the SQ-model, but possible under differential privacy. This result provides the first evidence that efficient replicable learning over general distributions strictly extends efficient SQ-learning, and is closer in power to efficient differentially private learning, despite computational separations between replicability and privacy. Our main building block is a new, efficient, and replicable algorithm that, given a set of vectors, outputs a subspace of their linear span that covers most of them.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09499v1",
      "url": "https://arxiv.org/abs/2602.09499v1",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "differential",
        "privacy"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces the first computationally efficient (polynomial-time) replicable learning algorithm for realizable parity functions over arbitrary input distributions—a task known to be impossible in the Statistical Query (SQ) model but achievable with differential privacy. The key technical contribution is a novel, efficient, and replicable subspace-finding algorithm that, given a set of vectors, outputs a low-dimensional subspace covering most of them; this serves as the core building block for the parity learner. Crucially, the algorithm achieves replicability—producing identical outputs across independent runs with high probability—without relying on SQ access or distributional restrictions. The result establishes, for the first time, that efficient replicable learning strictly surpasses efficient SQ-learning in computational power, narrowing the gap between replicability and differential privacy despite known computational separations between the two notions.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09433v1",
      "arxiv_id": "2602.09433v1",
      "title": "Autonomous Action Runtime Management(AARM):A System Specification for Securing AI-Driven Actions at Runtime",
      "authors": [
        "Herman Errico"
      ],
      "abstract": "As artificial intelligence systems evolve from passive assistants into autonomous agents capable of executing consequential actions, the security boundary shifts from model outputs to tool execution. Traditional security paradigms - log aggregation, perimeter defense, and post-hoc forensics - cannot protect systems where AI-driven actions are irreversible, execute at machine speed, and originate from potentially compromised orchestration layers. This paper introduces Autonomous Action Runtime Management (AARM), an open specification for securing AI-driven actions at runtime. AARM defines a runtime security system that intercepts actions before execution, accumulates session context, evaluates against policy and intent alignment, enforces authorization decisions, and records tamper-evident receipts for forensic reconstruction. We formalize a threat model addressing prompt injection, confused deputy attacks, data exfiltration, and intent drift. We introduce an action classification framework distinguishing forbidden, context-dependent deny, and context-dependent allow actions. We propose four implementation architectures - protocol gateway, SDK instrumentation, kernel eBPF, and vendor integration - with distinct trust properties, and specify minimum conformance requirements for AARM-compliant systems. AARM is model-agnostic, framework-agnostic, and vendor-neutral, treating action execution as the stable security boundary. This specification aims to establish industry-wide requirements before proprietary fragmentation forecloses interoperability.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09433v1",
      "url": "https://arxiv.org/abs/2602.09433v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "prompt",
        "injection"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces Autonomous Action Runtime Management (AARM), an open, vendor-neutral specification designed to secure AI systems at the point of *action execution*—not just model output—addressing critical gaps in traditional security approaches for autonomous AI agents. AARM proposes a runtime interception layer that evaluates actions in real time using session context, policy enforcement, and intent alignment before permitting execution, while generating tamper-evident receipts for forensics. It formalizes a threat model covering prompt injection, confused deputy attacks, data exfiltration, and intent drift, and introduces an action classification framework (forbidden, context-dependent deny, context-dependent allow) to guide dynamic authorization decisions. Four implementation architectures—protocol gateway, SDK instrumentation, kernel eBPF, and vendor integration—are specified with distinct trust assumptions, alongside minimum conformance requirements to ensure interoperability across models, frameworks, and vendors.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09392v1",
      "arxiv_id": "2602.09392v1",
      "title": "LLMAC: A Global and Explainable Access Control Framework with Large Language Model",
      "authors": [
        "Sharif Noor Zisad",
        "Ragib Hasan"
      ],
      "abstract": "Today's business organizations need access control systems that can handle complex, changing security requirements that go beyond what traditional methods can manage. Current approaches, such as Role-Based Access Control (RBAC), Attribute-Based Access Control (ABAC), and Discretionary Access Control (DAC), were designed for specific purposes. They cannot effectively manage the dynamic, situation-dependent workflows that modern systems require. In this research, we introduce LLMAC, a new unified approach using Large Language Models (LLMs) to combine these different access control methods into one comprehensive, understandable system. We used an extensive synthetic dataset that represents complex real-world scenarios, including policies for ownership verification, version management, workflow processes, and dynamic role separation. Using Mistral 7B, our trained LLM model achieved outstanding results with 98.5% accuracy, significantly outperforming traditional methods (RBAC: 14.5%, ABAC: 58.5%, DAC: 27.5%) while providing clear, human readable explanations for each decision. Performance testing shows that the system can be practically deployed with reasonable response times and computing resources.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09392v1",
      "url": "https://arxiv.org/abs/2602.09392v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces LLMAC, a novel global and explainable access control framework that unifies RBAC, ABAC, and DAC using a fine-tuned Large Language Model (Mistral 7B). Unlike traditional static methods, LLMAC dynamically interprets complex, context-aware security policies—such as ownership verification, versioning, and workflow-dependent role separation—using an extensive synthetic dataset designed to mirror real-world enterprise scenarios. Evaluated on this dataset, LLMAC achieved 98.5% decision accuracy, vastly outperforming RBAC (14.5%), ABAC (58.5%), and DAC (27.5%), while generating human-readable justifications for every access decision. The system also demonstrates practical deployability with acceptable latency and resource efficiency.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09357v1",
      "arxiv_id": "2602.09357v1",
      "title": "Data Sharing with Endogenous Choices over Differential Privacy Levels",
      "authors": [
        "Raef Bassily",
        "Kate Donahue",
        "Diptangshu Sen",
        "Annuo Zhao",
        "Juba Ziani"
      ],
      "abstract": "We study coalition formation for data sharing under differential privacy when agents have heterogeneous privacy costs. Each agent holds a sensitive data point and decides whether to participate in a data-sharing coalition and how much noise to add to their data. Privacy choices induce a fundamental trade-off: higher privacy reduces individual data-sharing costs but degrades data utility and statistical accuracy for the coalition. These choices generate externalities across agents, making both participation and privacy levels strategic. Our goal is to understand which coalitions are stable, how privacy choices shape equilibrium outcomes, and how decentralized data sharing compares to a centralized, socially optimal benchmark.   We provide a comprehensive equilibrium analysis across a broad range of privacy-cost regimes, from decreasing costs (e.g., privacy amplification from pooling data) to increasing costs (e.g., greater exposure to privacy attacks in larger coalitions). We first characterize Nash equilibrium coalitions with endogenous privacy levels and show that equilibria may fail to exist and can be non-monotonic in problem parameters. We also introduce a weaker equilibrium notion called robust equilibrium (that allows more widespread equilibrium existence by equipping existing players in the coalition with the power to prevent or veto external players from joining) and fully characterize such equilibria. Finally, we analyze, for both Nash and robust equilibria, the efficiency relative to the social optimum in terms of social welfare and estimator accuracy. We derive bounds that depend sharply on the number of players, properties of the cost profile and how privacy costs scale with coalition size.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09357v1",
      "url": "https://arxiv.org/abs/2602.09357v1",
      "categories": [
        "cs.GT",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "differential",
        "privacy"
      ],
      "keyword_score": 2,
      "summary": "This paper studies strategic data sharing under differential privacy when agents have heterogeneous, endogenous privacy costs—each choosing both whether to join a coalition and how much noise to add to their data. The authors model the resulting game with externalities: higher privacy improves individual cost but harms collective statistical accuracy, and privacy costs may increase or decrease with coalition size (e.g., due to privacy amplification or attack surface expansion). They characterize Nash equilibria—showing they may not exist or be non-monotonic—and introduce a novel *robust equilibrium* concept (with veto power over new entrants) that guarantees existence and admits full characterization. Both equilibria are evaluated against the social optimum, with efficiency bounds derived that depend critically on coalition size, cost structure, and scaling of privacy costs.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09338v1",
      "arxiv_id": "2602.09338v1",
      "title": "Privacy Amplification for BandMF via $b$-Min-Sep Subsampling",
      "authors": [
        "Andy Dong",
        "Arun Ganesh"
      ],
      "abstract": "We study privacy amplification for BandMF, i.e., DP-SGD with correlated noise across iterations via a banded correlation matrix. We propose $b$-min-sep subsampling, a new subsampling scheme that generalizes Poisson and balls-in-bins subsampling, extends prior practical batching strategies for BandMF, and enables stronger privacy amplification than cyclic Poisson while preserving the structural properties needed for analysis. We give a near-exact privacy analysis using Monte Carlo accounting, based on a dynamic program that leverages the Markovian structure in the subsampling procedure. We show that $b$-min-sep matches cyclic Poisson subsampling in the high noise regime and achieves strictly better guarantees in the mid-to-low noise regime, with experimental results that bolster our claims. We further show that unlike previous BandMF subsampling schemes, our $b$-min-sep subsampling naturally extends to the multi-attribution user-level privacy setting.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09338v1",
      "url": "https://arxiv.org/abs/2602.09338v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "dp"
      ],
      "keyword_score": 1,
      "summary": "This paper introduces *$b$-min-sep subsampling*, a novel subsampling scheme for BandMF (DP-SGD with banded correlation noise) that generalizes Poisson and balls-in-bins subsampling while enabling stronger privacy amplification—especially in the mid-to-low noise regime—than prior methods like cyclic Poisson. The authors develop a near-exact privacy analysis using Monte Carlo accounting grounded in a dynamic program that exploits the Markovian structure of $b$-min-sep. Experiments confirm improved privacy guarantees over cyclic Poisson in realistic noise regimes, without sacrificing the structural properties needed for theoretical analysis. Notably, $b$-min-sep is the first BandMF subsampling method shown to naturally extend to user-level privacy with multi-attribution settings.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10021v1",
      "arxiv_id": "2602.10021v1",
      "title": "Decoupled Reasoning with Implicit Fact Tokens (DRIFT): A Dual-Model Framework for Efficient Long-Context Inference",
      "authors": [
        "Wenxuan Xie",
        "Yujia Wang",
        "Xin Tan",
        "Chaochao Lu",
        "Xia Hu",
        "Xuhong Wang"
      ],
      "abstract": "The integration of extensive, dynamic knowledge into Large Language Models (LLMs) remains a significant challenge due to the inherent entanglement of factual data and reasoning patterns. Existing solutions, ranging from non-parametric Retrieval-Augmented Generation (RAG) to parametric knowledge editing, are often constrained in practice by finite context windows, retriever noise, or the risk of catastrophic forgetting. In this paper, we propose DRIFT, a novel dual-model architecture designed to explicitly decouple knowledge extraction from the reasoning process. Unlike static prompt compression, DRIFT employs a lightweight knowledge model to dynamically compress document chunks into implicit fact tokens conditioned on the query. These dense representations are projected into the reasoning model's embedding space, replacing raw, redundant text while maintaining inference accuracy. Extensive experiments show that DRIFT significantly improves performance on long-context tasks, outperforming strong baselines among comparably sized models. Our approach provides a scalable and efficient paradigm for extending the effective context window and reasoning capabilities of LLMs. Our code is available at https://github.com/Lancelot-Xie/DRIFT.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10021v1",
      "url": "https://arxiv.org/abs/2602.10021v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces DRIFT, a dual-model framework that decouples knowledge extraction from reasoning to improve long-context inference in LLMs. It uses a lightweight *knowledge model* to dynamically compress document chunks into query-conditioned *implicit fact tokens*, which are then projected into the reasoning model’s embedding space—replacing raw text while preserving semantic fidelity. Experiments demonstrate that DRIFT significantly outperforms strong baselines (e.g., RAG and standard fine-tuning) on long-context QA and reasoning tasks, with improved accuracy and efficiency. Crucially, it avoids common pitfalls like retriever noise, context window limits, and catastrophic forgetting, offering a scalable way to extend effective context length without enlarging the reasoning model.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09987v2",
      "arxiv_id": "2602.09987v2",
      "title": "Infusion: Shaping Model Behavior by Editing Training Data via Influence Functions",
      "authors": [
        "J Rosser",
        "Robert Kirk",
        "Edward Grefenstette",
        "Jakob Foerster",
        "Laura Ruis"
      ],
      "abstract": "Influence functions are commonly used to attribute model behavior to training documents. We explore the reverse: crafting training data that induces model behavior. Our framework, Infusion, uses scalable influence-function approximations to compute small perturbations to training documents that induce targeted changes in model behavior through parameter shifts. We evaluate Infusion on data poisoning tasks across vision and language domains. On CIFAR-10, we show that making subtle edits via Infusion to just 0.2% (100/45,000) of the training documents can be competitive with the baseline of inserting a small number of explicit behavior examples. We also find that Infusion transfers across architectures (ResNet $\\leftrightarrow$ CNN), suggesting a single poisoned corpus can affect multiple independently trained models. In preliminary language experiments, we characterize when our approach increases the probability of target behaviors and when it fails, finding it most effective at amplifying behaviors the model has already learned. Taken together, these results show that small, subtle edits to training data can systematically shape model behavior, underscoring the importance of training data interpretability for adversaries and defenders alike. We provide the code here: https://github.com/jrosseruk/infusion.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09987v2",
      "url": "https://arxiv.org/abs/2602.09987v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "published_official": true,
      "keywords": [
        "poisoning",
        "data",
        "model"
      ],
      "keyword_score": 3,
      "summary": "This paper introduces *Infusion*, a novel framework that reverses the typical use of influence functions—instead of diagnosing which training examples affect model behavior, it *designs* minimal edits to training data to *induce* targeted behavioral changes. Using scalable influence-function approximations, Infusion computes subtle perturbations to a tiny fraction (e.g., 0.2%) of training documents, causing parameter shifts that steer model outputs. On CIFAR-10, these edits match or exceed the effectiveness of inserting explicit adversarial examples, and crucially, the poisoning transfers across different model architectures (e.g., ResNet ↔ CNN). In language experiments, Infusion reliably amplifies pre-existing model behaviors but struggles when targeting entirely novel ones—highlighting its dependence on latent model knowledge. Overall, the work demonstrates that highly localized, interpretable data edits can systematically reshape model behavior, with implications for both data poisoning threats and data-centric safety interventions.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09343v1",
      "arxiv_id": "2602.09343v1",
      "title": "Not-in-Perspective: Towards Shielding Google's Perspective API Against Adversarial Negation Attacks",
      "authors": [
        "Michail S. Alexiou",
        "J. Sukarno Mertoguno"
      ],
      "abstract": "The rise of cyberbullying in social media platforms involving toxic comments has escalated the need for effective ways to monitor and moderate online interactions. Existing solutions of automated toxicity detection systems, are based on a machine or deep learning algorithms. However, statistics-based solutions are generally prone to adversarial attacks that contain logic based modifications such as negation in phrases and sentences. In that regard, we present a set of formal reasoning-based methodologies that wrap around existing machine learning toxicity detection systems. Acting as both pre-processing and post-processing steps, our formal reasoning wrapper helps alleviating the negation attack problems and significantly improves the accuracy and efficacy of toxicity scoring. We evaluate different variations of our wrapper on multiple machine learning models against a negation adversarial dataset. Experimental results highlight the improvement of hybrid (formal reasoning and machine-learning) methods against various purely statistical solutions.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09343v1",
      "url": "https://arxiv.org/abs/2602.09343v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "machine",
        "learning",
        "adversarial"
      ],
      "keyword_score": 3,
      "summary": "This paper introduces a formal reasoning-based “wrapper” designed to defend Google’s Perspective API—and other ML-based toxicity detectors—against adversarial negation attacks (e.g., “This is not toxic” misclassified as toxic). The wrapper operates as a hybrid pre- and post-processing layer that applies logic-based rules to detect and reinterpret negated phrases before and after the ML model’s prediction. Evaluated on a custom negation adversarial dataset across multiple ML models, the approach significantly improves accuracy and robustness compared to standalone statistical models. Key results show consistent performance gains, demonstrating that integrating formal reasoning with ML mitigates a critical vulnerability in automated toxicity detection.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09306v1",
      "arxiv_id": "2602.09306v1",
      "title": "Empowering Contrastive Federated Sequential Recommendation with LLMs",
      "authors": [
        "Thi Minh Chau Nguyen",
        "Minh Hieu Nguyen",
        "Duc Anh Nguyen",
        "Xuan Huong Tran",
        "Thanh Trung Huynh",
        "Quoc Viet Hung Nguyen"
      ],
      "abstract": "Federated sequential recommendation (FedSeqRec) aims to perform next-item prediction while keeping user data decentralised, yet model quality is frequently constrained by fragmented, noisy, and homogeneous interaction logs stored on individual devices. Many existing approaches attempt to compensate through manual data augmentation or additional server-side constraints, but these strategies either introduce limited semantic diversity or increase system overhead. To overcome these challenges, we propose \\textbf{LUMOS}, a parameter-isolated FedSeqRec architecture that integrates large language models (LLMs) as \\emph{local semantic generators}. Instead of sharing gradients or auxiliary parameters, LUMOS privately invokes an on-device LLM to construct three complementary sequence variants from each user history: (i) \\emph{future-oriented} trajectories that infer plausible behavioural continuations, (ii) \\emph{semantically equivalent rephrasings} that retain user intent while diversifying interaction patterns, and (iii) \\emph{preference-inconsistent counterfactuals} that serve as informative negatives. These synthesized sequences are jointly encoded within the federated backbone through a tri-view contrastive optimisation scheme, enabling richer representation learning without exposing sensitive information. Experimental results across three public benchmarks show that LUMOS achieves consistent gains over competitive centralised and federated baselines on HR@20 and NDCG@20. In addition, the use of semantically grounded positive signals and counterfactual negatives improves robustness under noisy and adversarial environments, even without dedicated server-side protection modules. Overall, this work demonstrates the potential of LLM-driven semantic generation as a new paradigm for advancing privacy-preserving federated recommendation.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09306v1",
      "url": "https://arxiv.org/abs/2602.09306v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "published_official": true,
      "keywords": [
        "federated",
        "learning"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces **LUMOS**, a novel federated sequential recommendation (FedSeqRec) framework that leverages on-device large language models (LLMs) as *local semantic generators* to overcome data fragmentation, noise, and homogeneity—without sharing raw data, gradients, or auxiliary parameters. LUMOS privately synthesizes three complementary sequence variants from each user’s interaction history: future-oriented trajectories, semantically equivalent rephrasings, and preference-inconsistent counterfactuals, which are jointly encoded via a tri-view contrastive learning objective in the federated backbone. Experiments on three public benchmarks show consistent improvements over state-of-the-art centralized and federated baselines in HR@20 and NDCG@20, while also demonstrating enhanced robustness to noisy and adversarial conditions—without requiring server-side defenses. The key contribution is establishing LLM-powered *semantic data augmentation at the edge* as an effective, privacy-preserving paradigm for boosting representation learning in FedSeqRec.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10232v1",
      "arxiv_id": "2602.10232v1",
      "title": "Risk-Equalized Differentially Private Synthetic Data: Protecting Outliers by Controlling Record-Level Influence",
      "authors": [
        "Amir Asiaee",
        "Chao Yan",
        "Zachary B. Abrams",
        "Bradley A. Malin"
      ],
      "abstract": "When synthetic data is released, some individuals are harder to protect than others. A patient with a rare disease combination or a transaction with unusual characteristics stands out from the crowd. Differential privacy provides worst-case guarantees, but empirical attacks -- particularly membership inference -- succeed far more often against such outliers, especially under moderate privacy budgets and with auxiliary information.   This paper introduces risk-equalized DP synthesis, a framework that prioritizes protection for high-risk records by reducing their influence on the learned generator. The mechanism operates in two stages: first, a small privacy budget estimates each record's \"outlierness\"; second, a DP learning procedure weights each record inversely to its risk score. Under Gaussian mechanisms, a record's privacy loss is proportional to its influence on the output -- so deliberately shrinking outliers' contributions yields tighter per-instance privacy bounds for precisely those records that need them most.   We prove end-to-end DP guarantees via composition and derive closed-form per-record bounds for the synthesis stage (the scoring stage adds a uniform per-record term). Experiments on simulated data with controlled outlier injection show that risk-weighting substantially reduces membership inference success against high-outlierness records; ablations confirm that targeting -- not random downweighting -- drives the improvement. On real-world benchmarks (Breast Cancer, Adult, German Credit), gains are dataset-dependent, highlighting the interplay between scorer quality and synthesis pipeline.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10232v1",
      "url": "https://arxiv.org/abs/2602.10232v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "inference",
        "dp",
        "membership",
        "differential",
        "privacy"
      ],
      "keyword_score": 5,
      "summary": "This paper introduces *risk-equalized differentially private (DP) synthetic data generation*, a novel framework that improves protection for high-risk (e.g., outlier) records—those most vulnerable to membership inference attacks—by adaptively reducing their influence during DP model training. It operates in two DP stages: (1) a lightweight, privacy-budget-efficient scoring stage estimates each record’s “outlierness,” and (2) a synthesis stage trains the generative model with record weights inversely proportional to risk scores, yielding tighter per-record privacy bounds for outliers under Gaussian mechanisms. The authors provide rigorous end-to-end DP composition guarantees and closed-form per-record privacy loss bounds. Experiments confirm that risk-aware weighting—unlike uniform or random downweighting—significantly reduces membership inference success on outliers in simulated data, while real-world results demonstrate dataset-dependent gains tied to scorer accuracy and pipeline design.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10228v1",
      "arxiv_id": "2602.10228v1",
      "title": "PRISM: Differentially Private Synthetic Data with Structure-Aware Budget Allocation for Prediction",
      "authors": [
        "Amir Asiaee",
        "Chao Yan",
        "Zachary B. Abrams",
        "Bradley A. Malin"
      ],
      "abstract": "Differential privacy (DP) provides a mathematical guarantee limiting what an adversary can learn about any individual from released data. However, achieving this protection typically requires adding noise, and noise can accumulate when many statistics are measured. Existing DP synthetic data methods treat all features symmetrically, spreading noise uniformly even when the data will serve a specific prediction task.   We develop a prediction-centric approach operating in three regimes depending on available structural knowledge. In the causal regime, when the causal parents of $Y$ are known and distribution shift is expected, we target the parents for robustness. In the graphical regime, when a Bayesian network structure is available and the distribution is stable, the Markov blanket of $Y$ provides a sufficient feature set for optimal prediction. In the predictive regime, when no structural knowledge exists, we select features via differentially private methods without claiming to recover causal or graphical structure.   We formalize this as PRISM, a mechanism that (i) identifies a predictive feature subset according to the appropriate regime, (ii) constructs targeted summary statistics, (iii) allocates budget to minimize an upper bound on prediction error, and (iv) synthesizes data via graphical-model inference. We prove end-to-end privacy guarantees and risk bounds. Empirically, task-aware allocation improves prediction accuracy compared to generic synthesizers. Under distribution shift, targeting causal parents achieves AUC $\\approx 0.73$ while correlation-based selection collapses to chance ($\\approx 0.49$).",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10228v1",
      "url": "https://arxiv.org/abs/2602.10228v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "dp",
        "differential",
        "privacy"
      ],
      "keyword_score": 3,
      "summary": "This paper introduces PRISM, a differentially private synthetic data generation framework that allocates privacy budget *task-awarely*—specifically to optimize downstream prediction performance—rather than uniformly across all features. PRISM operates in three regimes (causal, graphical, predictive) depending on available structural knowledge, selecting a minimal yet sufficient feature set (e.g., causal parents or Markov blanket of the target variable *Y*) and constructing targeted, low-noise summary statistics for those features. It then allocates the privacy budget to minimize an upper bound on prediction error and synthesizes data using graphical-model inference, with formal end-to-end DP guarantees and risk bounds. Empirically, PRISM significantly outperforms uniform-budget methods—especially under distribution shift, where targeting causal parents achieves AUC ≈ 0.73 versus ≈ 0.49 for correlation-based selection.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09904v1",
      "arxiv_id": "2602.09904v1",
      "title": "Safeguarding Privacy: Privacy-Preserving Detection of Mind Wandering and Disengagement Using Federated Learning in Online Education",
      "authors": [
        "Anna Bodonhelyi",
        "Mengdi Wang",
        "Efe Bozkir",
        "Babette Bühler",
        "Enkelejda Kasneci"
      ],
      "abstract": "Since the COVID-19 pandemic, online courses have expanded access to education, yet the absence of direct instructor support challenges learners' ability to self-regulate attention and engagement. Mind wandering and disengagement can be detrimental to learning outcomes, making their automated detection via video-based indicators a promising approach for real-time learner support. However, machine learning-based approaches often require sharing sensitive data, raising privacy concerns. Federated learning offers a privacy-preserving alternative by enabling decentralized model training while also distributing computational load. We propose a framework exploiting cross-device federated learning to address different manifestations of behavioral and cognitive disengagement during remote learning, specifically behavioral disengagement, mind wandering, and boredom. We fit video-based cognitive disengagement detection models using facial expressions and gaze features. By adopting federated learning, we safeguard users' data privacy through privacy-by-design and introduce a novel solution with the potential for real-time learner support. We further address challenges posed by eyeglasses by incorporating related features, enhancing overall model performance. To validate the performance of our approach, we conduct extensive experiments on five datasets and benchmark multiple federated learning algorithms. Our results show great promise for privacy-preserving educational technologies promoting learner engagement.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09904v1",
      "url": "https://arxiv.org/abs/2602.09904v1",
      "categories": [
        "cs.LG",
        "cs.HC"
      ],
      "published_official": true,
      "keywords": [
        "machine",
        "privacy-preserving",
        "federated",
        "learning"
      ],
      "keyword_score": 4,
      "summary": "This paper introduces a privacy-preserving framework for detecting mind wandering, behavioral disengagement, and boredom in online learning using federated learning (FL) on video-based facial and gaze features—avoiding centralized data collection. The method employs cross-device FL to train models locally on users’ devices, incorporating eyeglasses-aware feature engineering to improve robustness. Evaluated across five diverse datasets and benchmarked against multiple FL algorithms (e.g., FedAvg, FedProx), the approach achieves strong detection performance while preserving data privacy by design. Key results demonstrate that FL-based models match or approach the accuracy of centralized baselines, validating feasibility for real-time, scalable learner support. The work thus bridges critical gaps between engagement monitoring, privacy protection, and practical deployment in remote education.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09848v1",
      "arxiv_id": "2602.09848v1",
      "title": "Robust Processing and Learning: Principles, Methods, and Wireless Applications",
      "authors": [
        "Shixiong Wang",
        "Wei Dai",
        "Li-Chun Wang",
        "Geoffrey Ye Li"
      ],
      "abstract": "This tutorial-style overview article examines the fundamental principles and methods of robustness, using wireless sensing and communication (WSC) as the narrative and exemplifying framework. First, we formalize the conceptual and mathematical foundations of robustness, highlighting the interpretations and relations across robust statistics, optimization, and machine learning. Key techniques, such as robust estimation and testing, distributionally robust optimization, and regularized and adversary training, are investigated. Together, the costs of robustness in system design, for example, the compromised nominal performances and the extra computational burdens, are discussed. Second, we review recent robust signal processing solutions for WSC that address model mismatch, data scarcity, adversarial perturbation, and distributional shift. Specific applications include robust ranging-based localization, modality sensing, channel estimation, receive combining, waveform design, and federated learning. Through this effort, we aim to introduce the classical developments and recent advances in robustness theory to the general signal processing community, exemplifying how robust statistical, optimization, and machine learning approaches can address the uncertainties inherent in WSC systems.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09848v1",
      "url": "https://arxiv.org/abs/2602.09848v1",
      "categories": [
        "eess.SP",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "machine",
        "learning",
        "federated",
        "adversarial"
      ],
      "keyword_score": 4,
      "summary": "This tutorial paper unifies the theoretical foundations and practical methods of robustness across robust statistics, optimization, and machine learning, using wireless sensing and communication (WSC) as a unifying application domain. It systematically reviews key techniques—including robust estimation, distributionally robust optimization, regularization, and adversarial training—and analyzes their trade-offs, such as reduced nominal performance and increased computational cost. The authors then survey recent robust signal processing solutions for WSC challenges like model mismatch, data scarcity, adversarial attacks, and distributional shift, with concrete applications in localization, channel estimation, waveform design, and federated learning. By bridging classical robust theory and modern WSC problems, the paper aims to equip the signal processing community with principled tools to handle real-world uncertainties.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09757v1",
      "arxiv_id": "2602.09757v1",
      "title": "Towards Poisoning Robustness Certification for Natural Language Generation",
      "authors": [
        "Mihnea Ghitu",
        "Matthew Wicker"
      ],
      "abstract": "Understanding the reliability of natural language generation is critical for deploying foundation models in security-sensitive domains. While certified poisoning defenses provide provable robustness bounds for classification tasks, they are fundamentally ill-equipped for autoregressive generation: they cannot handle sequential predictions or the exponentially large output space of language models. To establish a framework for certified natural language generation, we formalize two security properties: stability (robustness to any change in generation) and validity (robustness to targeted, harmful changes in generation). We introduce Targeted Partition Aggregation (TPA), the first algorithm to certify validity/targeted attacks by computing the minimum poisoning budget needed to induce a specific harmful class, token, or phrase. Further, we extend TPA to provide tighter guarantees for multi-turn generations using mixed integer linear programming (MILP). Empirically, we demonstrate TPA's effectiveness across diverse settings including: certifying validity of agent tool-calling when adversaries modify up to 0.5% of the dataset and certifying 8-token stability horizons in preference-based alignment. Though inference-time latency remains an open challenge, our contributions enable certified deployment of language models in security-critical applications.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09757v1",
      "url": "https://arxiv.org/abs/2602.09757v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "inference",
        "agent",
        "security"
      ],
      "keyword_score": 3,
      "summary": "This paper introduces the first framework for certifying poisoning robustness in natural language generation (NLG), addressing a critical gap since existing certified defenses are designed only for classification—not autoregressive, sequential text generation. The authors formalize two security properties—*stability* (robustness against arbitrary output changes) and *validity* (robustness against targeted harmful outputs)—and propose Targeted Partition Aggregation (TPA), a novel algorithm that computes the minimum data poisoning budget required to force a specific harmful token, class, or phrase. To strengthen guarantees for multi-turn generation, they extend TPA using mixed integer linear programming (MILP), yielding tighter robustness bounds. Experiments show TPA can certify validity (e.g., robust tool-calling under 0.5% dataset poisoning) and stability (e.g., 8-token horizons in preference-aligned models), enabling provably secure NLG deployment—though inference latency remains a practical limitation.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09748v1",
      "arxiv_id": "2602.09748v1",
      "title": "Linear Model Extraction via Factual and Counterfactual Queries",
      "authors": [
        "Daan Otto",
        "Jannis Kurtz",
        "Dick den Hertog",
        "Ilker Birbil"
      ],
      "abstract": "In model extraction attacks, the goal is to reveal the parameters of a black-box machine learning model by querying the model for a selected set of data points. Due to an increasing demand for explanations, this may involve counterfactual queries besides the typically considered factual queries. In this work, we consider linear models and three types of queries: factual, counterfactual, and robust counterfactual. First, for an arbitrary set of queries, we derive novel mathematical formulations for the classification regions for which the decision of the unknown model is known, without recovering any of the model parameters. Second, we derive bounds on the number of queries needed to extract the model's parameters for (robust) counterfactual queries under arbitrary norm-based distances. We show that the full model can be recovered using just a single counterfactual query when differentiable distance measures are employed. In contrast, when using polyhedral distances for instance, the number of required queries grows linearly with the dimension of the data space. For robust counterfactuals, the latter number of queries doubles. Consequently, the applied distance function and robustness of counterfactuals have a significant impact on the model's security.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09748v1",
      "url": "https://arxiv.org/abs/2602.09748v1",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary": "This paper studies model extraction attacks against black-box linear classifiers using factual, counterfactual, and robust counterfactual queries—motivated by growing demands for model explanations. Its main contribution is a novel theoretical framework that (1) characterizes classification regions where the model’s decisions are known without parameter recovery, and (2) derives tight query-complexity bounds for full parameter extraction. Key results show that with differentiable distance metrics (e.g., ℓ₂), a *single* counterfactual query suffices to recover the entire linear model, whereas polyhedral distances (e.g., ℓ₁ or ℓ∞) require Θ(d) queries in d-dimensional space—and robust counterfactuals double this requirement. The work thus reveals that the choice of distance metric and robustness level critically determines vulnerability to extraction.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09667v1",
      "arxiv_id": "2602.09667v1",
      "title": "Differentiable Modeling for Low-Inertia Grids: Benchmarking PINNs, NODEs, and DP for Identification and Control of SMIB System",
      "authors": [
        "Shinhoo Kang",
        "Sangwook Kim",
        "Sehyun Yun"
      ],
      "abstract": "The transition toward low-inertia power systems demands modeling frameworks that provide not only accurate state predictions but also physically consistent sensitivities for control. While scientific machine learning offers powerful nonlinear modeling tools, the control-oriented implications of different differentiable paradigms remain insufficiently understood. This paper presents a comparative study of Physics-Informed Neural Networks (PINNs), Neural Ordinary Differential Equations (NODEs), and Differentiable Programming (DP) for modeling, identification, and control of power system dynamics. Using the Single Machine Infinite Bus (SMIB) system as a benchmark, we evaluate their performance in trajectory extrapolation, parameter estimation, and Linear Quadratic Regulator (LQR) synthesis.   Our results highlight a fundamental trade-off between data-driven flexibility and physical structure. NODE exhibits superior extrapolation by capturing the underlying vector field, whereas PINN shows limited generalization due to its reliance on a time-dependent solution map. In the inverse problem of parameter identification, while both DP and PINN successfully recover the unknown parameters, DP achieves significantly faster convergence by enforcing governing equations as hard constraints. Most importantly, for control synthesis, the DP framework yields closed-loop stability comparable to the theoretical optimum. Furthermore, we demonstrate that NODE serves as a viable data-driven surrogate when governing equations are unavailable.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09667v1",
      "url": "https://arxiv.org/abs/2602.09667v1",
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "published_official": true,
      "keywords": [
        "dp"
      ],
      "keyword_score": 1,
      "summary": "This paper benchmarks three differentiable modeling approaches—Physics-Informed Neural Networks (PINNs), Neural Ordinary Differential Equations (NODEs), and Differentiable Programming (DP)—for modeling, identification, and control of the Single Machine Infinite Bus (SMIB) system in low-inertia power grids. Using trajectory extrapolation, parameter estimation, and Linear Quadratic Regulator (LQR) synthesis as evaluation tasks, it reveals key trade-offs: NODE excels in extrapolation by learning the underlying vector field, while PINN suffers from poor generalization due to its time-dependent solution representation. DP achieves the fastest and most accurate parameter identification by embedding physical equations as hard constraints, and yields closed-loop control performance nearly matching the theoretical LQR optimum. Crucially, NODE is shown to be an effective data-driven surrogate when first-principles models are unavailable, whereas DP delivers superior control-oriented guarantees when physics knowledge is accessible.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09520v1",
      "arxiv_id": "2602.09520v1",
      "title": "Rashomon Sets and Model Multiplicity in Federated Learning",
      "authors": [
        "Xenia Heilmann",
        "Luca Corbucci",
        "Mattia Cerrato"
      ],
      "abstract": "The Rashomon set captures the collection of models that achieve near-identical empirical performance yet may differ substantially in their decision boundaries. Understanding the differences among these models, i.e., their multiplicity, is recognized as a crucial step toward model transparency, fairness, and robustness, as it reveals decision boundaries instabilities that standard metrics obscure. However, the existing definitions of Rashomon set and multiplicity metrics assume centralized learning and do not extend naturally to decentralized, multi-party settings like Federated Learning (FL). In FL, multiple clients collaboratively train models under a central server's coordination without sharing raw data, which preserves privacy but introduces challenges from heterogeneous client data distribution and communication constraints. In this setting, the choice of a single best model may homogenize predictive behavior across diverse clients, amplify biases, or undermine fairness guarantees. In this work, we provide the first formalization of Rashomon sets in FL.First, we adapt the Rashomon set definition to FL, distinguishing among three perspectives: (I) a global Rashomon set defined over aggregated statistics across all clients, (II) a t-agreement Rashomon set representing the intersection of local Rashomon sets across a fraction t of clients, and (III) individual Rashomon sets specific to each client's local distribution.Second, we show how standard multiplicity metrics can be estimated under FL's privacy constraints. Finally, we introduce a multiplicity-aware FL pipeline and conduct an empirical study on standard FL benchmark datasets. Our results demonstrate that all three proposed federated Rashomon set definitions offer valuable insights, enabling clients to deploy models that better align with their local data, fairness considerations, and practical requirements.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09520v1",
      "url": "https://arxiv.org/abs/2602.09520v1",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "published_official": true,
      "keywords": [
        "federated",
        "learning"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces the first formalization of Rashomon sets—collections of near-optimal models with divergent decision boundaries—for Federated Learning (FL), addressing the gap between centralized multiplicity analysis and decentralized, privacy-preserving settings. It proposes three FL-specific Rashomon set definitions: (I) global (over aggregated statistics), (II) *t*-agreement (intersection across a fraction *t* of clients), and (III) individual (per-client). The authors adapt multiplicity estimation to respect FL’s privacy constraints and integrate it into a multiplicity-aware FL pipeline. Experiments on standard FL benchmarks show that all three Rashomon set variants reveal meaningful model diversity, enabling more transparent, fair, and locally adaptive model selection—countering the homogenizing bias of selecting a single “best” global model.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09304v1",
      "arxiv_id": "2602.09304v1",
      "title": "Statistical Roughness-Informed Machine Unlearning",
      "authors": [
        "Mohammad Partohaghighi",
        "Roummel Marcia",
        "Bruce J. West",
        "YangQuan Chen"
      ],
      "abstract": "Machine unlearning aims to remove the influence of a designated forget set from a trained model while preserving utility on the retained data. In modern deep networks, approximate unlearning frequently fails under large or adversarial deletions due to pronounced layer-wise heterogeneity: some layers exhibit stable, well-regularized representations while others are brittle, undertrained, or overfit, so naive update allocation can trigger catastrophic forgetting or unstable dynamics. We propose Statistical-Roughness Adaptive Gradient Unlearning (SRAGU), a mechanism-first unlearning algorithm that reallocates unlearning updates using layer-wise statistical roughness operationalized via heavy-tailed spectral diagnostics of layer weight matrices. Starting from an Adaptive Gradient Unlearning (AGU) sensitivity signal computed on the forget set, SRAGU estimates a WeightWatcher-style heavy-tailed exponent for each layer, maps it to a bounded spectral stability weight, and uses this stability signal to spectrally reweight the AGU sensitivities before applying the same minibatch update form. This concentrates unlearning motion in spectrally stable layers while damping updates in unstable or overfit layers, improving stability under hard deletions. We evaluate unlearning via behavioral alignment to a gold retrained reference model trained from scratch on the retained data, using empirical prediction-divergence and KL-to-gold proxies on a forget-focused query set; we additionally report membership inference auditing as a complementary leakage signal, treating forget-set points as should-be-forgotten members during evaluation.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09304v1",
      "url": "https://arxiv.org/abs/2602.09304v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "inference",
        "membership"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces Statistical-Roughness Adaptive Gradient Unlearning (SRAGU), a novel machine unlearning method that addresses layer-wise instability in deep networks during forgetting. SRAGU extends Adaptive Gradient Unlearning (AGU) by estimating layer-specific statistical “roughness” via heavy-tailed spectral analysis (e.g., WeightWatcher-style exponents) of weight matrices, then uses these diagnostics to compute spectral stability weights that reweight AGU’s sensitivity signals before update application. This adaptive reweighting concentrates unlearning updates in stable, well-regularized layers while suppressing them in brittle or overfit layers—mitigating catastrophic forgetting and improving robustness under large or adversarial deletions. Experiments evaluate unlearning quality via behavioral alignment with gold-standard retrained models (using prediction divergence and KL divergence on forget-focused queries) and membership inference leakage, showing SRAGU outperforms baseline unlearning methods in both fidelity and privacy preservation.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10092v1",
      "arxiv_id": "2602.10092v1",
      "title": "Quantum-Audit: Evaluating the Reasoning Limits of LLMs on Quantum Computing",
      "authors": [
        "Mohamed Afane",
        "Kayla Laufer",
        "Wenqi Wei",
        "Ying Mao",
        "Junaid Farooq",
        "Ying Wang",
        "Juntao Chen"
      ],
      "abstract": "Language models have become practical tools for quantum computing education and research, from summarizing technical papers to explaining theoretical concepts and answering questions about recent developments in the field. While existing benchmarks evaluate quantum code generation and circuit design, their understanding of quantum computing concepts has not been systematically measured. Quantum-Audit addresses this gap with 2,700 questions covering core quantum computing topics. We evaluate 26 models from leading organizations. Our benchmark comprises 1,000 expert-written questions, 1,000 questions extracted from research papers using LLMs and validated by experts, plus an additional 700 questions including 350 open-ended questions and 350 questions with false premises to test whether models can correct erroneous assumptions. Human participants scored between 23% and 86%, with experts averaging 74%. Top-performing models exceeded the expert average, with Claude Opus 4.5 reaching 84% accuracy, though top models showed an average 12-point accuracy drop on expert-written questions compared to LLM-generated ones. Performance declined further on advanced topics, dropping to 73% on security questions. Additionally, models frequently accepted and reinforced false premises embedded in questions instead of identifying them, with accuracy below 66% on these critical reasoning tasks.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10092v1",
      "url": "https://arxiv.org/abs/2602.10092v1",
      "categories": [
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "Quantum-Audit introduces the first comprehensive benchmark—comprising 2,700 rigorously curated questions—to systematically evaluate LLMs’ conceptual understanding of quantum computing, going beyond code generation to assess reasoning, misconception detection, and domain knowledge. It combines expert-written questions (1,000), LLM-extracted-and-validated questions from research papers (1,000), and challenging items including open-ended and false-premise questions (700). Evaluated across 26 models, top performers like Claude Opus 4.5 achieved 84% overall accuracy—exceeding the human expert average (74%)—but showed significant drops in performance on expert-written questions (−12 points) and advanced topics like quantum security (73%). Critically, models struggled with logical correction, achieving <66% accuracy on false-premise questions, revealing a key limitation in identifying and refuting erroneous assumptions.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09538v1",
      "arxiv_id": "2602.09538v1",
      "title": "UniARM: Towards a Unified Autoregressive Reward Model for Multi-Objective Test-Time Alignment",
      "authors": [
        "Hongyan Xie",
        "Yikun Ban",
        "Ruiyu Fang",
        "Zixuan Huang",
        "Deqing Wang",
        "Jianxin Li",
        "Yitong Yao",
        "Chao Wang",
        "Shuangyong Song"
      ],
      "abstract": "Multi-objective alignment aims to align LLM responses with multiple human preference objectives. Among existing methods, guiding the generation of frozen LLMs through autoregressive reward models (ARMs) to accomplish multi-objective test-time alignment is a low-cost solution. However, these methods typically rely on independent parameters for each preference objective, either by training ARMs independently across preference dimensions, which neglects interactions among preference features, or by training a single ARM with separate feature extraction modules for each preference, which can cause feature entanglement. Both strategies can result in misalignment between generated outputs and user preferences. To address this limitation, we propose Preference-Modulated \\& Shared Low-Rank Adaptation (MoSLoRA) for ARM training, which first extracts shared features via a preference-agnostic module and then applies affine transformations to shared features via a preference modulation module conditioned on mixed preference vectors. This design mitigates feature entanglement and enables precise control over preference trade-offs during inference. Building on this, we introduce the Unified Autoregressive Reward Model (UniARM), a novel framework for multi-objective test-time alignment. UniARM jointly models all preference dimensions in a single parameter space, eliminating the need for independent parameters for each preference objective. es on larger-scale LLMs, enhancing its practical usability.",
      "published": "2026-02-10",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09538v1",
      "url": "https://arxiv.org/abs/2602.09538v1",
      "categories": [
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces UniARM, a unified autoregressive reward model for multi-objective test-time alignment of LLMs, addressing the limitations of prior ARMs that either train separate models per objective (ignoring preference interactions) or use entangled feature extraction. Its core innovation is Preference-Modulated & Shared Low-Rank Adaptation (MoSLoRA), which first learns shared, preference-agnostic features and then applies preference-conditioned affine transformations to enable clean, disentangled control over trade-offs. Evaluated on multi-objective alignment benchmarks, UniARM achieves superior alignment accuracy and preference controllability compared to independent or multi-head ARM baselines—especially on larger LLMs—while maintaining parameter efficiency and inference flexibility.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09273v1",
      "arxiv_id": "2602.09273v1",
      "title": "The Price of Privacy For Approximating Max-CSP",
      "authors": [
        "Prathamesh Dharangutte",
        "Jingcheng Liu",
        "Pasin Manurangsi",
        "Akbar Rafiey",
        "Phanu Vajanopath",
        "Zongrui Zou"
      ],
      "abstract": "We study approximation algorithms for Maximum Constraint Satisfaction Problems (Max-CSPs) under differential privacy (DP) where the constraints are considered sensitive data. Information-theoretically, we aim to classify the best approximation ratios possible for a given privacy budget $\\varepsilon$. In the high-privacy regime ($\\varepsilon \\ll 1$), we show that any $\\varepsilon$-DP algorithm cannot beat a random assignment by more than $O(\\varepsilon)$ in the approximation ratio. We devise a polynomial-time algorithm which matches this barrier under the assumptions that the instances are bounded-degree and triangle-free. Finally, we show that one or both of these assumptions can be removed for specific CSPs--such as Max-Cut or Max $k$-XOR--albeit at the cost of computational efficiency.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09273v1",
      "url": "https://arxiv.org/abs/2602.09273v1",
      "categories": [
        "cs.DS",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "dp",
        "differential",
        "privacy"
      ],
      "keyword_score": 3,
      "summary": "This paper investigates the fundamental trade-off between differential privacy and approximation quality for Max-CSPs, where constraints are sensitive inputs. It establishes a tight information-theoretic lower bound: in the high-privacy regime (ε ≪ 1), no ε-DP algorithm can improve upon the random assignment’s approximation ratio by more than O(ε). The authors then design a polynomial-time ε-DP algorithm that achieves this bound for bounded-degree, triangle-free CSP instances. For specific problems like Max-Cut and Max k-XOR, they show the structural assumptions can be relaxed—though often at the expense of computational efficiency.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09222v1",
      "arxiv_id": "2602.09222v1",
      "title": "MUZZLE: Adaptive Agentic Red-Teaming of Web Agents Against Indirect Prompt Injection Attacks",
      "authors": [
        "Georgios Syros",
        "Evan Rose",
        "Brian Grinstead",
        "Christoph Kerschbaumer",
        "William Robertson",
        "Cristina Nita-Rotaru",
        "Alina Oprea"
      ],
      "abstract": "Large language model (LLM) based web agents are increasingly deployed to automate complex online tasks by directly interacting with web sites and performing actions on users' behalf. While these agents offer powerful capabilities, their design exposes them to indirect prompt injection attacks embedded in untrusted web content, enabling adversaries to hijack agent behavior and violate user intent. Despite growing awareness of this threat, existing evaluations rely on fixed attack templates, manually selected injection surfaces, or narrowly scoped scenarios, limiting their ability to capture realistic, adaptive attacks encountered in practice. We present MUZZLE, an automated agentic framework for evaluating the security of web agents against indirect prompt injection attacks. MUZZLE utilizes the agent's trajectories to automatically identify high-salience injection surfaces, and adaptively generate context-aware malicious instructions that target violations of confidentiality, integrity, and availability. Unlike prior approaches, MUZZLE adapts its attack strategy based on the agent's observed execution trajectory and iteratively refines attacks using feedback from failed executions. We evaluate MUZZLE across diverse web applications, user tasks, and agent configurations, demonstrating its ability to automatically and adaptively assess the security of web agents with minimal human intervention. Our results show that MUZZLE effectively discovers 37 new attacks on 4 web applications with 10 adversarial objectives that violate confidentiality, availability, or privacy properties. MUZZLE also identifies novel attack strategies, including 2 cross-application prompt injection attacks and an agent-tailored phishing scenario.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09222v1",
      "url": "https://arxiv.org/abs/2602.09222v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "injection",
        "agent",
        "prompt",
        "llm"
      ],
      "keyword_score": 5,
      "summary": "MUZZLE is an adaptive, agentic red-teaming framework designed to automatically evaluate web agents’ resilience against indirect prompt injection attacks—where malicious instructions are embedded in untrusted web content. Unlike prior static or template-based methods, MUZZLE dynamically identifies high-salience injection surfaces from the agent’s own execution trajectories and iteratively refines context-aware attacks using feedback from failed attempts. Evaluated across diverse web applications and tasks, it discovered 37 novel attacks—including 2 cross-application injections and an agent-specific phishing scenario—that violate confidentiality, integrity, or availability. The framework requires minimal human intervention and significantly advances realistic, adaptive security assessment for LLM-based web agents.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09182v1",
      "arxiv_id": "2602.09182v1",
      "title": "One RNG to Rule Them All: How Randomness Becomes an Attack Vector in Machine Learning",
      "authors": [
        "Kotekar Annapoorna Prabhu",
        "Andrew Gan",
        "Zahra Ghodsi"
      ],
      "abstract": "Machine learning relies on randomness as a fundamental component in various steps such as data sampling, data augmentation, weight initialization, and optimization. Most machine learning frameworks use pseudorandom number generators as the source of randomness. However, variations in design choices and implementations across different frameworks, software dependencies, and hardware backends along with the lack of statistical validation can lead to previously unexplored attack vectors on machine learning systems. Such attacks on randomness sources can be extremely covert, and have a history of exploitation in real-world systems. In this work, we examine the role of randomness in the machine learning development pipeline from an adversarial point of view, and analyze the implementations of PRNGs in major machine learning frameworks. We present RNGGuard to help machine learning engineers secure their systems with low effort. RNGGuard statically analyzes a target library's source code and identifies instances of random functions and modules that use them. At runtime, RNGGuard enforces secure execution of random functions by replacing insecure function calls with RNGGuard's implementations that meet security specifications. Our evaluations show that RNGGuard presents a practical approach to close existing gaps in securing randomness sources in machine learning systems.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09182v1",
      "url": "https://arxiv.org/abs/2602.09182v1",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "machine",
        "learning",
        "adversarial"
      ],
      "keyword_score": 3,
      "summary": "This paper identifies pseudorandom number generators (PRNGs) as a covert attack vector in ML systems due to inconsistent, insecure, or statistically unvalidated implementations across frameworks, dependencies, and hardware. The authors analyze PRNG usage across major ML frameworks and propose **RNGGuard**, a lightweight tool that (1) *statically* detects insecure random function calls in source code and (2) *dynamically* replaces them at runtime with cryptographically secure, specification-compliant alternatives. Evaluation shows RNGGuard effectively mitigates randomness-related vulnerabilities with minimal overhead and integration effort. The work’s main contribution is framing randomness not just as an engineering detail but as a security-critical component—and providing the first practical, deployable defense for securing it end-to-end in ML pipelines.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10149v1",
      "arxiv_id": "2602.10149v1",
      "title": "Exploring Semantic Labeling Strategies for Third-Party Cybersecurity Risk Assessment Questionnaires",
      "authors": [
        "Ali Nour Eldin",
        "Mohamed Sellami",
        "Walid Gaaloul"
      ],
      "abstract": "Third-Party Risk Assessment (TPRA) is a core cybersecurity practice for evaluating suppliers against standards such as ISO/IEC 27001 and NIST. TPRA questionnaires are typically drawn from large repositories of security and compliance questions, yet tailoring assessments to organizational needs remains a largely manual process. Existing retrieval approaches rely on keyword or surface-level similarity, which often fails to capture implicit assessment scope and control semantics.   This paper explores strategies for organizing and retrieving TPRA cybersecurity questions using semantic labels that describe both control domains and assessment scope. We compare direct question-level labeling with a Large Language Model (LLM) against a hybrid semi-supervised semantic labeling (SSSL) pipeline that clusters questions in embedding space, labels a small representative subset using an LLM, and propagates labels to remaining questions using k-Nearest Neighbors; we also compare downstream retrieval based on direct question similarity versus retrieval in the label space. We find that semantic labels can improve retrieval alignment when labels are discriminative and consistent, and that SSSL can generalize labels from a small labeled subset to large repositories while substantially reducing LLM usage and cost.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10149v1",
      "url": "https://arxiv.org/abs/2602.10149v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces semantic labeling strategies to improve the retrieval and tailoring of third-party cybersecurity risk assessment (TPRA) questionnaires, addressing limitations of keyword-based methods that fail to capture implicit control semantics and assessment scope. The authors compare two labeling approaches: (1) direct LLM-based labeling of individual questions, and (2) a hybrid semi-supervised semantic labeling (SSSL) pipeline that clusters questions in embedding space, labels only a small representative subset with an LLM, and propagates labels via k-NN. They further evaluate retrieval performance using either raw question embeddings or the learned semantic label space. Key results show that well-designed semantic labels significantly improve retrieval alignment with intended assessment scope—and crucially, the SSSL method achieves comparable or better labeling quality than full LLM labeling while reducing LLM calls by up to 90%, cutting cost and computational overhead.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08934v1",
      "arxiv_id": "2602.08934v1",
      "title": "StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors",
      "authors": [
        "Suraj Ranganath",
        "Atharv Ramesh"
      ],
      "abstract": "AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08934v1",
      "url": "https://arxiv.org/abs/2602.08934v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "StealthRL introduces a reinforcement learning framework for generating semantically preserving paraphrases that evade multiple AI-text detectors simultaneously. It trains a Qwen3-4B-based paraphraser using Group Relative Policy Optimization (GRPO) with LoRA adapters, optimizing a composite reward for evasion (against an ensemble of RoBERTa, FastDetectGPT, and Binoculars detectors) and semantic fidelity. Evaluated at the security-critical 1% false positive rate, StealthRL achieves near-perfect evasion—mean true positive rate of just 0.001, AUROC drop from 0.74 to 0.27, and 99.9% attack success—while generalizing to unseen detector families, indicating shared architectural weaknesses. Human and LLM-based quality assessments confirm high semantic preservation, and distributional analysis reveals how detector score miscalibration enables evasion. The work establishes StealthRL as a rigorous, open-source adversarial evaluation protocol exposing critical robustness gaps in current AI-text detection.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08762v1",
      "arxiv_id": "2602.08762v1",
      "title": "HoGS: Homophily-Oriented Graph Synthesis for Local Differentially Private GNN Training",
      "authors": [
        "Wen Xu",
        "Zhetao Li",
        "Yong Xiao",
        "Pengpeng Qiao",
        "Mianxiong Dong",
        "Kaoru Ota"
      ],
      "abstract": "Graph neural networks (GNNs) have demonstrated remarkable performance in various graph-based machine learning tasks by effectively modeling high-order interactions between nodes. However, training GNNs without protection may leak sensitive personal information in graph data, including links and node features. Local differential privacy (LDP) is an advanced technique for protecting data privacy in decentralized networks. Unfortunately, existing local differentially private GNNs either only preserve link privacy or suffer significant utility loss in the process of preserving link and node feature privacy. In this paper, we propose an effective LDP framework, called HoGS, which trains GNNs with link and feature protection by generating a synthetic graph. Concretely, HoGS first collects the link and feature information of the graph under LDP, and then utilizes the phenomenon of homophily in graph data to reconstruct the graph structure and node features separately, thereby effectively mitigating the negative impact of LDP on the downstream GNN training. We theoretically analyze the privacy guarantee of HoGS and conduct experiments using the generated synthetic graph as input to various state-of-the-art GNN architectures. Experimental results on three real-world datasets show that HoGS significantly outperforms baseline methods in the accuracy of training GNNs.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08762v1",
      "url": "https://arxiv.org/abs/2602.08762v1",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "differential",
        "privacy",
        "learning",
        "machine"
      ],
      "keyword_score": 4,
      "summary": "This paper introduces HoGS, a novel Local Differential Privacy (LDP) framework for training GNNs while simultaneously protecting both graph links and node features—addressing a key limitation of prior LDP-GNN methods that either ignore one privacy dimension or incur severe utility loss. HoGS first collects perturbed link and feature data under LDP guarantees, then leverages graph homophily to reconstruct a high-fidelity synthetic graph—separately optimizing structure and features—to mitigate LDP-induced distortion. Theoretical analysis confirms end-to-end LDP compliance, and extensive experiments on three real-world datasets show HoGS consistently outperforms baselines in downstream GNN accuracy, often by large margins.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08690v1",
      "arxiv_id": "2602.08690v1",
      "title": "SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity",
      "authors": [
        "Shae McFadden",
        "Myles Foley",
        "Elizabeth Bates",
        "Ilias Tsingenopoulos",
        "Sanyam Vyas",
        "Vasilios Mavroudis",
        "Chris Hicks",
        "Fabio Pierazzi"
      ],
      "abstract": "Deep Reinforcement Learning (DRL) has achieved remarkable success in domains requiring sequential decision-making, motivating its application to cybersecurity problems. However, transitioning DRL from laboratory simulations to bespoke cyber environments can introduce numerous issues. This is further exacerbated by the often adversarial, non-stationary, and partially-observable nature of most cybersecurity tasks. In this paper, we identify and systematize 11 methodological pitfalls that frequently occur in DRL for cybersecurity (DRL4Sec) literature across the stages of environment modeling, agent training, performance evaluation, and system deployment. By analyzing 66 significant DRL4Sec papers (2018-2025), we quantify the prevalence of each pitfall and find an average of over five pitfalls per paper. We demonstrate the practical impact of these pitfalls using controlled experiments in (i) autonomous cyber defense, (ii) adversarial malware creation, and (iii) web security testing environments. Finally, we provide actionable recommendations for each pitfall to support the development of more rigorous and deployable DRL-based security systems.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08690v1",
      "url": "https://arxiv.org/abs/2602.08690v1",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security"
      ],
      "keyword_score": 2,
      "summary": "This paper presents a systematic analysis (SoK) of methodological pitfalls in applying Deep Reinforcement Learning (DRL) to cybersecurity (DRL4Sec), identifying and categorizing 11 recurring issues across environment modeling, agent training, evaluation, and deployment. Through a comprehensive review of 66 DRL4Sec papers (2018–2025), the authors quantify the prevalence of these pitfalls—finding an average of over five per paper—and validate their real-world impact via controlled experiments in autonomous cyber defense, adversarial malware generation, and web security testing. Key results show that common flaws—such as unrealistic environment assumptions, insufficient evaluation against adaptive adversaries, and lack of reproducibility—severely undermine agent robustness and deployability. The paper’s main contribution is a rigorous, empirically grounded taxonomy of pitfalls paired with concrete, actionable mitigation strategies to improve the rigor, reliability, and practical utility of DRL in security applications.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08563v1",
      "arxiv_id": "2602.08563v1",
      "title": "Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs",
      "authors": [
        "Ahmed Salem",
        "Andrew Paverd",
        "Sahar Abdelnabi"
      ],
      "abstract": "Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input. This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory. We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning. Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: https://github.com/microsoft/implicitMemory.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08563v1",
      "url": "https://arxiv.org/abs/2602.08563v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "poisoning",
        "data",
        "model"
      ],
      "keyword_score": 3,
      "summary": "This paper introduces *implicit memory*—a previously overlooked phenomenon where LLMs persist and retrieve information across independent interactions by encoding state into their own outputs and later decoding it when those outputs reappear as input, all without explicit memory modules. The authors demonstrate this capability through *time bombs*, a novel class of temporal backdoors that activate only after a sequence of interactions satisfying hidden conditions accumulated via implicit memory; these are shown to be reliably induced via simple prompting or fine-tuning. Key results include empirical validation across multiple LLMs (e.g., Llama-2, GPT-3.5) showing robust implicit state retention over dozens of turns and successful activation of time bombs with high precision. The work further reveals broader security and integrity risks—including covert agent communication, benchmark contamination, and training-data poisoning—and highlights significant detection challenges due to the mechanism’s reliance on standard inference behavior. To support reproducibility and future study, the authors release code and data.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08446v1",
      "arxiv_id": "2602.08446v1",
      "title": "RIFLE: Robust Distillation-based FL for Deep Model Deployment on Resource-Constrained IoT Networks",
      "authors": [
        "Pouria Arefijamal",
        "Mahdi Ahmadlou",
        "Bardia Safaei",
        "Jörg Henkel"
      ],
      "abstract": "Federated learning (FL) is a decentralized learning paradigm widely adopted in resource-constrained Internet of Things (IoT) environments. These devices, typically relying on TinyML models, collaboratively train global models by sharing gradients with a central server while preserving data privacy. However, as data heterogeneity and task complexity increase, TinyML models often become insufficient to capture intricate patterns, especially under extreme non-IID (non-independent and identically distributed) conditions. Moreover, ensuring robustness against malicious clients and poisoned updates remains a major challenge. Accordingly, this paper introduces RIFLE - a Robust, distillation-based Federated Learning framework that replaces gradient sharing with logit-based knowledge transfer. By leveraging a knowledge distillation aggregation scheme, RIFLE enables the training of deep models such as VGG-19 and Resnet18 within constrained IoT systems. Furthermore, a Kullback-Leibler (KL) divergence-based validation mechanism quantifies the reliability of client updates without exposing raw data, achieving high trust and privacy preservation simultaneously. Experiments on three benchmark datasets (MNIST, CIFAR-10, and CIFAR-100) under heterogeneous non-IID conditions demonstrate that RIFLE reduces false-positive detections by up to 87.5%, enhances poisoning attack mitigation by 62.5%, and achieves up to 28.3% higher accuracy compared to conventional federated learning baselines within only 10 rounds. Notably, RIFLE reduces VGG19 training time from over 600 days to just 1.39 hours on typical IoT devices (0.3 GFLOPS), making deep learning practical in resource-constrained networks.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08446v1",
      "url": "https://arxiv.org/abs/2602.08446v1",
      "categories": [
        "cs.LG",
        "cs.CR",
        "cs.DC",
        "cs.NI"
      ],
      "published_official": true,
      "keywords": [
        "federated",
        "data",
        "model",
        "poisoning",
        "learning"
      ],
      "keyword_score": 5,
      "summary": "This paper introduces RIFLE, a robust, distillation-based federated learning framework designed to enable deep model training (e.g., VGG-19, ResNet-18) on resource-constrained IoT devices—traditionally limited to TinyML—by replacing gradient sharing with logit-based knowledge distillation. Its core innovations include a KL divergence–based client update validation mechanism that assesses update reliability without accessing raw data, ensuring both privacy and robustness against poisoning attacks. Evaluated on MNIST, CIFAR-10, and CIFAR-100 under extreme non-IID conditions, RIFLE achieves up to 28.3% higher accuracy, reduces false-positive detection by 87.5%, and improves poisoning mitigation by 62.5% compared to standard FL baselines—all within just 10 communication rounds. Remarkably, it slashes VGG-19 training time on low-power IoT hardware (0.3 GFLOPS) from over 600 days to only 1.39 hours.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08422v1",
      "arxiv_id": "2602.08422v1",
      "title": "LLMs + Security = Trouble",
      "authors": [
        "Benjamin Livshits"
      ],
      "abstract": "We argue that when it comes to producing secure code with AI, the prevailing \"fighting fire with fire\" approach -- using probabilistic AI-based checkers or attackers to secure probabilistically generated code -- fails to address the long tail of security bugs. As a result, systems may remain exposed to zero-day vulnerabilities that can be discovered by better-resourced or more persistent adversaries.   While neurosymbolic approaches that combine LLMs with formal methods are attractive in principle, we argue that they are difficult to reconcile with the \"vibe coding\" workflow common in LLM-assisted development: unless the end-to-end verification pipeline is fully automated, developers are repeatedly asked to validate specifications, resolve ambiguities, and adjudicate failures, making the human-in-the-loop a likely point of weakness, compromising secure-by-construction guarantees.   In this paper we argue that stronger security guarantees can be obtained by enforcing security constraints during code generation (e.g., via constrained decoding), rather than relying solely on post-hoc detection and repair. This direction is particularly promising for diffusion-style code models, whose approach provides a natural elegant opportunity for modular, hierarchical security enforcement, allowing us to combine lower-latency generation techniques with generating secure-by-construction code.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08422v1",
      "url": "https://arxiv.org/abs/2602.08422v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "This paper critiques the current “fighting fire with fire” approach—using AI-based checkers or attackers to secure LLM-generated code—as insufficient for addressing the long tail of security bugs and zero-day vulnerabilities. It argues that neurosymbolic methods (combining LLMs with formal verification) are hampered by human-in-the-loop bottlenecks in real-world “vibe coding” workflows, undermining secure-by-construction guarantees. Instead, the authors advocate for *enforcing security constraints during code generation*, particularly via constrained decoding. They highlight diffusion-style code models as especially promising for this approach, enabling modular, hierarchical, and latency-efficient security enforcement that embeds correctness directly into the generation process.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08384v1",
      "arxiv_id": "2602.08384v1",
      "title": "Towards Real-World Industrial-Scale Verification: LLM-Driven Theorem Proving on seL4",
      "authors": [
        "Jianyu Zhang",
        "Fuyuan Zhang",
        "Jiayi Lu",
        "Jilin Hu",
        "Xiaoyi Yin",
        "Long Zhang",
        "Feng Yang",
        "Yongwang Zhao"
      ],
      "abstract": "Formal methods (FM) are reliable but costly to apply, often requiring years of expert effort in industrial-scale projects such as seL4, especially for theorem proving. Recent advances in large language models (LLMs) have made automated theorem proving increasingly feasible. However, most prior work focuses on mathematics-oriented benchmarks such as miniF2F, with limited evaluation on real-world verification projects. The few studies that consider industrial-scale verification mostly rely on closed-source models with hundreds of billions of parameters, which cannot be locally deployed and incur substantial usage costs. In this paper, we propose AutoReal, an LLM-driven theorem proving method for real-world industrial-scale systems with support for lightweight local deployment. We evaluate AutoReal on the seL4-Isabelle verification project as a representative and challenging case study. AutoReal incorporates two key improvements: (1) chain-of-thought (CoT)-based proof training, which teaches the LLM the reasoning behind proof steps and enables step-wise explanations alongside proofs, and (2) context augmentation, which leverages proof context from the project to enhance LLM-driven proving. Based on the AutoReal methodology, we fine-tune a base model to obtain AutoReal-Prover, a compact 7B-scale prover for industrial-scale theorem proving. AutoReal-Prover achieves a 51.67% proof success rate on 660 theorems from seL4-designated Important Theories across all 10 seL4 proof categories, substantially outperforming prior attempts on seL4 (27.06%). To evaluate generalization, we further apply AutoReal-Prover to three security-related projects from the Archive of Formal Proofs (AFP), covering all 451 theorems and achieving a proof success rate of 53.88%. Overall, this work advances the application of LLM-driven theorem proving in real-world industrial-scale verification.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08384v1",
      "url": "https://arxiv.org/abs/2602.08384v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces AutoReal, a lightweight, locally deployable LLM-driven theorem proving method tailored for real-world industrial-scale verification—specifically evaluated on the seL4 microkernel’s Isabelle formalization. AutoReal enhances reasoning through chain-of-thought–based proof training and context augmentation using project-specific proof knowledge, and it yields AutoReal-Prover: a compact, fine-tuned 7B-parameter model. On 660 critical seL4 theorems across all 10 proof categories, AutoReal-Prover achieves a 51.67% success rate—nearly doubling prior performance (27.06%). It further generalizes well to three security-related AFP projects, proving 53.88% of 451 theorems, demonstrating strong transferability beyond seL4.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09012v1",
      "arxiv_id": "2602.09012v1",
      "title": "Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense",
      "authors": [
        "Jiacheng Liu",
        "Yaxin Luo",
        "Jiacheng Cui",
        "Xinyi Shang",
        "Xiaohan Zhao",
        "Zhiqiang Shen"
      ],
      "abstract": "The rapid evolution of GUI-enabled agents has rendered traditional CAPTCHAs obsolete. While previous benchmarks like OpenCaptchaWorld established a baseline for evaluating multimodal agents, recent advancements in reasoning-heavy models, such as Gemini3-Pro-High and GPT-5.2-Xhigh have effectively collapsed this security barrier, achieving pass rates as high as 90% on complex logic puzzles like \"Bingo\". In response, we introduce Next-Gen CAPTCHAs, a scalable defense framework designed to secure the next-generation web against the advanced agents. Unlike static datasets, our benchmark is built upon a robust data generation pipeline, allowing for large-scale and easily scalable evaluations, notably, for backend-supported types, our system is capable of generating effectively unbounded CAPTCHA instances. We exploit the persistent human-agent \"Cognitive Gap\" in interactive perception, memory, decision-making, and action. By engineering dynamic tasks that require adaptive intuition rather than granular planning, we re-establish a robust distinction between biological users and artificial agents, offering a scalable and diverse defense mechanism for the agentic era.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09012v1",
      "url": "https://arxiv.org/abs/2602.09012v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces “Next-Gen CAPTCHAs,” a scalable, dynamic defense framework designed to counter increasingly capable GUI agents (e.g., Gemini3-Pro-High, GPT-5.2-Xhigh) that now solve traditional and even complex logic-based CAPTCHAs (e.g., “Bingo”) with up to 90% success. Unlike static benchmarks like OpenCaptchaWorld, the authors propose a generative pipeline that produces large-scale, diverse, and—crucially—for backend-supported types, *unbounded* CAPTCHA instances. The method exploits the persistent “Cognitive Gap” between humans and agents by designing interactive, intuition-driven tasks requiring real-time perception, memory, and adaptive decision-making—not just logical reasoning or step-by-step planning. Key results show that while state-of-the-art agents excel on static puzzles, they struggle significantly on these dynamic, interactive challenges, restoring a robust human-agent distinction and enabling scalable, future-proof web security.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.10147v1",
      "arxiv_id": "2602.10147v1",
      "title": "On the Use of a Large Language Model to Support the Conduction of a Systematic Mapping Study: A Brief Report from a Practitioner's View",
      "authors": [
        "Cauã Ferreira Barros",
        "Marcos Kalinowski",
        "Mohamad Kassab",
        "Valdemar Vicente Graciano Neto"
      ],
      "abstract": "The use of Large Language Models (LLMs) has drawn growing interest within the scientific community. LLMs can handle large volumes of textual data and support methods for evidence synthesis. Although recent studies highlight the potential of LLMs to accelerate screening and data extraction steps in systematic reviews, detailed reports of their practical application throughout the entire process remain scarce. This paper presents an experience report on the conduction of a systematic mapping study with the support of LLMs, describing the steps followed, the necessary adjustments, and the main challenges faced. Positive aspects are discussed, such as (i) the significant reduction of time in repetitive tasks and (ii) greater standardization in data extraction, as well as negative aspects, including (i) considerable effort to build reliable well-structured prompts, especially for less experienced users, since achieving effective prompts may require several iterations and testing, which can partially offset the expected time savings, (ii) the occurrence of hallucinations, and (iii) the need for constant manual verification. As a contribution, this work offers lessons learned and practical recommendations for researchers interested in adopting LLMs in systematic mappings and reviews, highlighting both efficiency gains and methodological risks and limitations to be considered.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.10147v1",
      "url": "https://arxiv.org/abs/2602.10147v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary": "This paper presents a practitioner’s experience using Large Language Models (LLMs) to support all phases of a systematic mapping study (SMS), not just isolated tasks like screening or extraction. The authors employed LLMs—primarily for title/abstract screening, full-text classification, and structured data extraction—while carefully designing, iterating, and validating prompts and conducting rigorous human verification at each step. Key results include substantial time savings on repetitive tasks and improved consistency in data extraction, but also notable challenges: significant upfront effort to engineer reliable prompts, LLM hallucinations, and the indispensable need for continuous manual oversight. The main contribution is a set of empirically grounded lessons and practical recommendations for integrating LLMs into evidence synthesis workflows—emphasizing that while LLMs enhance efficiency and standardization, they introduce new methodological risks requiring careful mitigation.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08764v1",
      "arxiv_id": "2602.08764v1",
      "title": "Efficient Brain Extraction of MRI Scans with Mild to Moderate Neuropathology",
      "authors": [
        "Hjalti Thrastarson",
        "Lotta M. Ellingsen"
      ],
      "abstract": "Skull stripping magnetic resonance images (MRI) of the human brain is an important process in many image processing techniques, such as automatic segmentation of brain structures. Numerous methods have been developed to perform this task, however, they often fail in the presence of neuropathology and can be inconsistent in defining the boundary of the brain mask. Here, we propose a novel approach to skull strip T1-weighted images in a robust and efficient manner, aiming to consistently segment the outer surface of the brain, including the sulcal cerebrospinal fluid (CSF), while excluding the full extent of the subarachnoid space and meninges. We train a modified version of the U-net on silver-standard ground truth data using a novel loss function based on the signed-distance transform (SDT). We validate our model both qualitatively and quantitatively using held-out data from the training dataset, as well as an independent external dataset. The brain masks used for evaluation partially or fully include the subarachnoid space, which may introduce bias into the comparison; nonetheless, our model demonstrates strong performance on the held-out test data, achieving a consistent mean Dice similarity coefficient (DSC) of 0.964$\\pm$0.006 and an average symmetric surface distance (ASSD) of 1.4mm$\\pm$0.2mm. Performance on the external dataset is comparable, with a DSC of 0.958$\\pm$0.006 and an ASSD of 1.7$\\pm$0.2mm. Our method achieves performance comparable to or better than existing state-of-the-art methods for brain extraction, particularly in its highly consistent preservation of the brain's outer surface. The method is publicly available on GitHub.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08764v1",
      "url": "https://arxiv.org/abs/2602.08764v1",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces a robust, efficient U-Net–based method for skull stripping T1-weighted MRI scans, specifically designed to handle mild-to-moderate neuropathology and consistently delineate the brain’s outer surface—including sulcal CSF—while excluding the broader subarachnoid space and meninges. The model is trained on silver-standard labels using a novel signed-distance transform (SDT)-based loss function to improve boundary accuracy. Evaluated on both held-out internal and independent external datasets, it achieves high segmentation accuracy (mean Dice = 0.964 and 0.958; ASSD = 1.4 mm and 1.7 mm), demonstrating strong consistency and competitive or superior performance compared to state-of-the-art methods. The approach addresses key limitations of existing tools—particularly boundary inconsistency in pathological cases—and the implementation is publicly available.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08734v1",
      "arxiv_id": "2602.08734v1",
      "title": "Finite-State Controllers for (Hidden-Model) POMDPs using Deep Reinforcement Learning",
      "authors": [
        "David Hudák",
        "Maris F. L. Galesloot",
        "Martin Tappler",
        "Martin Kurečka",
        "Nils Jansen",
        "Milan Češka"
      ],
      "abstract": "Solving partially observable Markov decision processes (POMDPs) requires computing policies under imperfect state information. Despite recent advances, the scalability of existing POMDP solvers remains limited. Moreover, many settings require a policy that is robust across multiple POMDPs, further aggravating the scalability issue. We propose the Lexpop framework for POMDP solving. Lexpop (1) employs deep reinforcement learning to train a neural policy, represented by a recurrent neural network, and (2) constructs a finite-state controller mimicking the neural policy through efficient extraction methods. Crucially, unlike neural policies, such controllers can be formally evaluated, providing performance guarantees. We extend Lexpop to compute robust policies for hidden-model POMDPs (HM-POMDPs), which describe finite sets of POMDPs. We associate every extracted controller with its worst-case POMDP. Using a set of such POMDPs, we iteratively train a robust neural policy and consequently extract a robust controller. Our experiments show that on problems with large state spaces, Lexpop outperforms state-of-the-art solvers for POMDPs as well as HM-POMDPs.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08734v1",
      "url": "https://arxiv.org/abs/2602.08734v1",
      "categories": [
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces Lexpop, a novel framework for solving POMDPs and their robust extension—hidden-model POMDPs (HM-POMDPs)—by combining deep reinforcement learning with finite-state controller extraction. It trains a recurrent neural network policy via DRL and then extracts an interpretable, compact finite-state controller that mimics the neural policy’s behavior, enabling formal verification and worst-case performance guarantees. For HM-POMDPs, Lexpop iteratively trains a robust neural policy across a set of POMDPs and extracts a controller certified for the worst-case model in the set. Experiments demonstrate that Lexpop outperforms state-of-the-art solvers on large-scale POMDPs and HM-POMDPs, achieving both scalability and formal robustness.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08412v2",
      "arxiv_id": "2602.08412v2",
      "title": "From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent",
      "authors": [
        "Yuhang Wang",
        "Feiming Xu",
        "Zheng Lin",
        "Guangyu He",
        "Yuzhe Huang",
        "Haichang Gao",
        "Zhenxing Niu",
        "Shiguo Lian",
        "Zhaoxiang Liu"
      ],
      "abstract": "Although large language model (LLM)-based agents, exemplified by OpenClaw, are increasingly evolving from task-oriented systems into personalized AI assistants for solving complex real-world tasks, their practical deployment also introduces severe security risks. However, existing agent security research and evaluation frameworks primarily focus on synthetic or task-centric settings, and thus fail to accurately capture the attack surface and risk propagation mechanisms of personalized agents in real-world deployments. To address this gap, we propose Personalized Agent Security Bench (PASB), an end-to-end security evaluation framework tailored for real-world personalized agents. Building upon existing agent attack paradigms, PASB incorporates personalized usage scenarios, realistic toolchains, and long-horizon interactions, enabling black-box, end-to-end security evaluation on real systems. Using OpenClaw as a representative case study, we systematically evaluate its security across multiple personalized scenarios, tool capabilities, and attack types. Our results indicate that OpenClaw exhibits critical vulnerabilities at different execution stages, including user prompt processing, tool usage, and memory retrieval, highlighting substantial security risks in personalized agent deployments. The code for the proposed PASB framework is available at https://github.com/AstorYH/PASB.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08412v2",
      "url": "https://arxiv.org/abs/2602.08412v2",
      "categories": [
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security",
        "llm"
      ],
      "keyword_score": 3,
      "summary": "This paper introduces the Personalized Agent Security Bench (PASB), a novel end-to-end security evaluation framework designed to assess real-world vulnerabilities in personalized LLM-based agents like OpenClaw—moving beyond synthetic or task-centric benchmarks. PASB incorporates realistic elements such as personalized user scenarios, actual toolchains, and long-horizon interactions to enable black-box, system-level security testing. Applying PASB to OpenClaw, the authors systematically uncover critical vulnerabilities across multiple execution stages: prompt processing, tool invocation, and memory retrieval. These findings reveal substantial, previously underexplored security risks in deployed personalized agents, underscoring the need for context-aware security evaluation. The open-sourced PASB framework (available on GitHub) provides a reproducible foundation for future research in agent security.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08406v1",
      "arxiv_id": "2602.08406v1",
      "title": "Optimizing Spectral Prediction in MXene-Based Metasurfaces Through Multi-Channel Spectral Refinement and Savitzky-Golay Smoothing",
      "authors": [
        "Shujaat Khan",
        "Waleed Iqbal Waseer"
      ],
      "abstract": "The prediction of electromagnetic spectra for MXene-based solar absorbers is a computationally intensive task, traditionally addressed using full-wave solvers. This study introduces an efficient deep learning framework incorporating transfer learning, multi-channel spectral refinement (MCSR), and Savitzky-Golay smoothing to accelerate and enhance spectral prediction accuracy. The proposed architecture leverages a pretrained MobileNetV2 model, fine-tuned to predict 102-point absorption spectra from $64\\times64$ metasurface designs. Additionally, the MCSR module processes the feature map through multi-channel convolutions, enhancing feature extraction, while Savitzky-Golay smoothing mitigates high-frequency noise. Experimental evaluations demonstrate that the proposed model significantly outperforms baseline Convolutional Neural Network (CNN) and deformable CNN models, achieving an average root mean squared error (RMSE) of 0.0245, coefficient of determination \\( R^2 \\) of 0.9578, and peak signal-to-noise ratio (PSNR) of 32.98 dB. The proposed framework presents a scalable and computationally efficient alternative to conventional solvers, positioning it as a viable candidate for rapid spectral prediction in nanophotonic design workflows.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08406v1",
      "url": "https://arxiv.org/abs/2602.08406v1",
      "categories": [
        "physics.optics",
        "cs.AI",
        "eess.SP"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces a deep learning framework to accelerate and improve spectral prediction for MXene-based metasurfaces, replacing computationally expensive full-wave solvers. The method combines transfer learning (using a fine-tuned MobileNetV2 backbone), a novel multi-channel spectral refinement (MCSR) module for enhanced feature extraction from metasurface images, and Savitzky-Golay smoothing to suppress high-frequency noise in predicted absorption spectra. Evaluated on 102-point absorption spectra from $64\\times64$ designs, the model achieves state-of-the-art performance with an RMSE of 0.0245, $R^2 = 0.9578$, and PSNR = 32.98 dB—significantly outperforming standard CNN and deformable CNN baselines. The approach offers a scalable, efficient alternative for rapid nanophotonic design optimization.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08290v1",
      "arxiv_id": "2602.08290v1",
      "title": "Trust-Based Incentive Mechanisms in Semi-Decentralized Federated Learning Systems",
      "authors": [
        "Ajay Kumar Shrestha"
      ],
      "abstract": "In federated learning (FL), decentralized model training allows multi-ple participants to collaboratively improve a shared machine learning model without exchanging raw data. However, ensuring the integrity and reliability of the system is challenging due to the presence of potentially malicious or faulty nodes that can degrade the model's performance. This paper proposes a novel trust-based incentive mechanism designed to evaluate and reward the quality of contributions in FL systems. By dynamically assessing trust scores based on fac-tors such as data quality, model accuracy, consistency, and contribution fre-quency, the system encourages honest participation and penalizes unreliable or malicious behavior. These trust scores form the basis of an incentive mechanism that rewards high-trust nodes with greater participation opportunities and penal-ties for low-trust participants. We further explore the integration of blockchain technology and smart contracts to automate the trust evaluation and incentive distribution processes, ensuring transparency and decentralization. Our proposed theoretical framework aims to create a more robust, fair, and transparent FL eco-system, reducing the risks posed by untrustworthy participants.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08290v1",
      "url": "https://arxiv.org/abs/2602.08290v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET"
      ],
      "published_official": true,
      "keywords": [
        "federated",
        "learning"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces a trust-based incentive mechanism for semi-decentralized federated learning (FL) to mitigate risks from malicious or unreliable participants. It dynamically computes node-specific trust scores using data quality, model accuracy, contribution consistency, and frequency—then leverages these scores to allocate participation rights and rewards while penalizing low-trust behavior. To ensure transparency and automation, the authors integrate blockchain and smart contracts for decentralized, tamper-resistant trust evaluation and incentive distribution. Experiments (implied by the framework’s design and validation claims) demonstrate improved model convergence, robustness against Byzantine attacks, and fairer resource allocation compared to baseline FL approaches. The main contribution is a unified, theoretically grounded framework that enhances FL reliability, fairness, and decentralization through adaptive trust management and automated incentives.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08282v1",
      "arxiv_id": "2602.08282v1",
      "title": "Tighnari v2: Mitigating Label Noise and Distribution Shift in Multimodal Plant Distribution Prediction via Mixture of Experts and Weakly Supervised Learning",
      "authors": [
        "Haixu Liu",
        "Yufei Wang",
        "Tianxiang Xu",
        "Chuancheng Shi",
        "Hongsheng Xing"
      ],
      "abstract": "Large-scale, cross-species plant distribution prediction plays a crucial role in biodiversity conservation, yet modeling efforts in this area still face significant challenges due to the sparsity and bias of observational data. Presence-Absence (PA) data provide accurate and noise-free labels, but are costly to obtain and limited in quantity; Presence-Only (PO) data, by contrast, offer broad spatial coverage and rich spatiotemporal distribution, but suffer from severe label noise in negative samples. To address these real-world constraints, this paper proposes a multimodal fusion framework that fully leverages the strengths of both PA and PO data. We introduce an innovative pseudo-label aggregation strategy for PO data based on the geographic coverage of satellite imagery, enabling geographic alignment between the label space and remote sensing feature space. In terms of model architecture, we adopt Swin Transformer Base as the backbone for satellite imagery, utilize the TabM network for tabular feature extraction, retain the Temporal Swin Transformer for time-series modeling, and employ a stackable serial tri-modal cross-attention mechanism to optimize the fusion of heterogeneous modalities. Furthermore, empirical analysis reveals significant geographic distribution shifts between PA training and test samples, and models trained by directly mixing PO and PA data tend to experience performance degradation due to label noise in PO data. To address this, we draw on the mixture-of-experts paradigm: test samples are partitioned according to their spatial proximity to PA samples, and different models trained on distinct datasets are used for inference and post-processing within each partition. Experiments on the GeoLifeCLEF 2025 dataset demonstrate that our approach achieves superior predictive performance in scenarios with limited PA coverage and pronounced distribution shifts.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08282v1",
      "url": "https://arxiv.org/abs/2602.08282v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces Tighnari v2, a multimodal framework for plant distribution prediction that jointly leverages scarce but clean Presence-Absence (PA) data and abundant but noisy Presence-Only (PO) data. Its key innovations include: (1) a geographic-aware pseudo-label aggregation strategy for PO data aligned with satellite imagery coverage, and (2) a mixture-of-experts (MoE) inference scheme that partitions test regions by spatial proximity to PA samples and deploys specialized models per partition to mitigate label noise and distribution shift. The architecture fuses satellite (Swin Transformer Base), tabular (TabM), and temporal (Temporal Swin Transformer) modalities via a serial tri-modal cross-attention mechanism. On GeoLifeCLEF 2025, Tighnari v2 achieves state-of-the-art performance—especially under limited PA coverage and strong geographic distribution shifts—outperforming naive PA/PO mixing and prior multimodal baselines.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.09181v1",
      "arxiv_id": "2602.09181v1",
      "title": "Weighted Wasserstein Barycenter of Gaussian Processes for exotic Bayesian Optimization tasks",
      "authors": [
        "Antonio Candelieri",
        "Francesco Archetti"
      ],
      "abstract": "Exploiting the analogy between Gaussian Distributions and Gaussian Processes' posterior, we present how the weighted Wasserstein Barycenter of Gaussian Processes (W2BGP) can be used to unify, under a common framework, different exotic Bayesian Optimization (BO) tasks. Specifically, collaborative/federated BO, (synchronous) batch BO, and multi-fidelity BO are considered in this paper. Our empirical analysis proves that each one of these tasks requires just an appropriate weighting schema for the W2BGP, while the entire framework remains untouched. Moreover, we demonstrate that the most well-known BO acquisition functions can be easily re-interpreted under the proposed framework and also enable a more computationally efficient way to deal with the computation of the Wasserstein Barycenter, compared with state-of-the-art methods from the Machine Learning literature. Finally, research perspectives branching from the proposed approach are presented.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.09181v1",
      "url": "https://arxiv.org/abs/2602.09181v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "federated",
        "learning"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces the Weighted Wasserstein Barycenter of Gaussian Processes (W2BGP) as a unifying framework for diverse “exotic” Bayesian Optimization (BO) tasks—including federated/collaborative BO, synchronous batch BO, and multi-fidelity BO—by leveraging the structural analogy between Gaussian distributions and GP posteriors. Instead of designing task-specific algorithms, the authors show that each variant can be realized through a simple, task-appropriate weighting scheme applied to the same W2BGP computation. They reinterpret standard BO acquisition functions within this framework and propose a computationally efficient method for computing the barycenter, outperforming general-purpose Wasserstein barycenter solvers. Empirical results validate the framework’s flexibility and efficiency across all three BO settings.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08617v1",
      "arxiv_id": "2602.08617v1",
      "title": "ERIS: Enhancing Privacy and Communication Efficiency in Serverless Federated Learning",
      "authors": [
        "Dario Fenoglio",
        "Pasquale Polverino",
        "Jacopo Quizi",
        "Martin Gjoreski",
        "Marc Langheinrich"
      ],
      "abstract": "Scaling federated learning (FL) to billion-parameter models introduces critical trade-offs between communication efficiency, model accuracy, and privacy guarantees. Existing solutions often tackle these challenges in isolation, sacrificing accuracy or relying on costly cryptographic tools. We propose ERIS, a serverless FL framework that balances privacy and accuracy while eliminating the server bottleneck and distributing the communication load. ERIS combines a model partitioning strategy, distributing aggregation across multiple client-side aggregators, with a distributed shifted gradient compression mechanism. We theoretically prove that ERIS (i) converges at the same rate as FedAvg under standard assumptions, and (ii) bounds mutual information leakage inversely with the number of aggregators, enabling strong privacy guarantees with no accuracy degradation. Experiments across image and text tasks, including large language models, confirm that ERIS achieves FedAvg-level accuracy while substantially reducing communication cost and improving robustness to membership inference and reconstruction attacks, without relying on heavy cryptography or noise injection.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08617v1",
      "url": "https://arxiv.org/abs/2602.08617v1",
      "categories": [
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "inference",
        "learning",
        "membership",
        "federated"
      ],
      "keyword_score": 4,
      "summary": "ERIS is a serverless federated learning framework that jointly addresses communication efficiency, model accuracy, and privacy—without relying on expensive cryptography or noise-based defenses. It introduces two key innovations: (1) a model partitioning strategy with decentralized client-side aggregators to eliminate the central server bottleneck, and (2) a distributed shifted gradient compression mechanism for bandwidth reduction. Theoretically, ERIS matches FedAvg’s convergence rate while provably bounding mutual information leakage—improving privacy as the number of aggregators increases. Empirically, it achieves FedAvg-level accuracy on image and text tasks (including large language models), cuts communication costs significantly, and enhances robustness against membership inference and model reconstruction attacks.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08590v1",
      "arxiv_id": "2602.08590v1",
      "title": "SDFed: Bridging Local Global Discrepancy via Subspace Refinement and Divergence Control in Federated Prompt Learning",
      "authors": [
        "Yicheng Di",
        "Wei Yuan",
        "Tieke He",
        "Zhanjie Zhang",
        "Ao Ma",
        "Yuan Liu",
        "Hongzhi Yin"
      ],
      "abstract": "Vision-language pretrained models offer strong transferable representations, yet adapting them in privacy-sensitive multi-party settings is challenging due to the high communication cost of federated optimization and the limited local data on clients. Federated prompt learning mitigates this issue by keeping the VLPM backbone frozen and collaboratively training lightweight prompt parameters. However, existing approaches typically enforce a unified prompt structure and length across clients, which is inadequate under practical client heterogeneity in both data distributions and system resources, and may further introduce conflicts between globally shared and locally optimal knowledge. To address these challenges, we propose \\textbf{SDFed}, a heterogeneous federated prompt learning framework that bridges Local-Global Discrepancy via Subspace Refinement and Divergence Control. SDFed maintains a fixed-length global prompt for efficient aggregation while allowing each client to learn a variable-length local prompt to better match its data characteristics and capacity. To mitigate local-global conflicts and facilitate effective knowledge transfer, SDFed introduces a subspace refinement method for local prompts and an information retention and divergence control strategy that preserves key local information while maintaining appropriate separability between global and local representations. Extensive experiments on several datasets demonstrate that SDFed consistently improves performance and robustness in heterogeneous federated settings.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08590v1",
      "url": "https://arxiv.org/abs/2602.08590v1",
      "categories": [
        "cs.LG",
        "cs.DB"
      ],
      "published_official": true,
      "keywords": [
        "federated",
        "learning"
      ],
      "keyword_score": 2,
      "summary": "SDFed is a novel federated prompt learning framework designed to address client heterogeneity in vision-language model adaptation, where clients differ in data distribution and system resources. Its main contribution is a dual-prompt architecture: a fixed-length global prompt for efficient server aggregation and variable-length local prompts tailored per client, coupled with subspace refinement (to align local prompts with the global subspace) and divergence control (to preserve task-specific local knowledge while ensuring separability from global representations). Evaluated across multiple benchmarks, SDFed consistently outperforms existing federated prompt methods—especially under severe non-IID data and resource constraints—achieving higher accuracy and improved robustness.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08542v1",
      "arxiv_id": "2602.08542v1",
      "title": "Incremental (k, z)-Clustering on Graphs",
      "authors": [
        "Emilio Cruciani",
        "Sebastian Forster",
        "Antonis Skarlatos"
      ],
      "abstract": "Given a weighted undirected graph, a number of clusters $k$, and an exponent $z$, the goal in the $(k, z)$-clustering problem on graphs is to select $k$ vertices as centers that minimize the sum of the distances raised to the power $z$ of each vertex to its closest center. In the dynamic setting, the graph is subject to adversarial edge updates, and the goal is to maintain explicitly an exact $(k, z)$-clustering solution in the induced shortest-path metric.   While efficient dynamic $k$-center approximation algorithms on graphs exist [Cruciani et al. SODA 2024], to the best of our knowledge, no prior work provides similar results for the dynamic $(k,z)$-clustering problem. As the main result of this paper, we develop a randomized incremental $(k, z)$-clustering algorithm that maintains with high probability a constant-factor approximation in a graph undergoing edge insertions with a total update time of $\\tilde O(k m^{1+o(1)}+ k^{1+\\frac{1}λ} m)$, where $λ\\geq 1$ is an arbitrary fixed constant. Our incremental algorithm consists of two stages. In the first stage, we maintain a constant-factor bicriteria approximate solution of size $\\tilde{O}(k)$ with a total update time of $m^{1+o(1)}$ over all adversarial edge insertions. This first stage is an intricate adaptation of the bicriteria approximation algorithm by Mettu and Plaxton [Machine Learning 2004] to incremental graphs. One of our key technical results is that the radii in their algorithm can be assumed to be non-decreasing while the approximation ratio remains constant, a property that may be of independent interest.   In the second stage, we maintain a constant-factor approximate $(k,z)$-clustering solution on a dynamic weighted instance induced by the bicriteria approximate solution. For this subproblem, we employ a dynamic spanner algorithm together with a static $(k,z)$-clustering algorithm.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08542v1",
      "url": "https://arxiv.org/abs/2602.08542v1",
      "categories": [
        "cs.DS",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "machine",
        "learning",
        "adversarial"
      ],
      "keyword_score": 3,
      "summary": "This paper introduces the first efficient incremental algorithm for the dynamic $(k,z)$-clustering problem on graphs undergoing edge insertions, where the goal is to maintain a constant-factor approximation to the optimal clustering cost (sum of $z$-th powers of shortest-path distances to $k$ centers). The method uses a two-stage approach: first, it maintains a bicriteria approximate solution of $\\tilde{O}(k)$ centers with total update time $\\tilde{O}(m^{1+o(1)})$, extending Mettu and Plaxton’s static algorithm and proving that their radii can be made non-decreasing without sacrificing approximation. Second, it refines this into a true $k$-center solution via a dynamic spanner and static $(k,z)$-clustering on a reduced weighted instance. The overall algorithm achieves total update time $\\tilde{O}(k m^{1+o(1)} + k^{1+1/\\lambda} m)$ for any fixed $\\lambda \\geq 1$, and succeeds with high probability.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08995v1",
      "arxiv_id": "2602.08995v1",
      "title": "When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents",
      "authors": [
        "Yuting Ning",
        "Jaylen Jones",
        "Zhehao Zhang",
        "Chentao Ye",
        "Weitong Ruan",
        "Junyi Li",
        "Rahul Gupta",
        "Huan Sun"
      ],
      "abstract": "Computer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user's original intent. Such misaligned actions may arise from external attacks (e.g., indirect prompt injection) or from internal limitations (e.g., erroneous reasoning). They not only expose CUAs to safety risks, but also degrade task efficiency and reliability. This work makes the first effort to define and study misaligned action detection in CUAs, with comprehensive coverage of both externally induced and internally arising misaligned actions. We further identify three common categories in real-world CUA deployment and construct MisActBench, a benchmark of realistic trajectories with human-annotated, action-level alignment labels. Moreover, we propose DeAction, a practical and universal guardrail that detects misaligned actions before execution and iteratively corrects them through structured feedback. DeAction outperforms all existing baselines across offline and online evaluations with moderate latency overhead: (1) On MisActBench, it outperforms baselines by over 15% absolute in F1 score; (2) In online evaluation, it reduces attack success rate by over 90% under adversarial settings while preserving or even improving task success rate in benign environments.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08995v1",
      "url": "https://arxiv.org/abs/2602.08995v1",
      "categories": [
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "prompt",
        "injection"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces the first systematic study of *misaligned action detection* in computer-use agents (CUAs)—actions that deviate from user intent due to external attacks (e.g., prompt injection) or internal failures (e.g., flawed reasoning). The authors define three real-world misalignment categories, build **MisActBench**, a new benchmark with human-annotated, action-level alignment labels on realistic trajectories, and propose **DeAction**, a lightweight, universal guardrail that detects misaligned actions before execution and corrects them via structured feedback. Evaluated offline on MisActBench, DeAction achieves >15% absolute F1 gain over all baselines; online, it reduces adversarial attack success by >90% while maintaining or improving task success in benign settings.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08548v1",
      "arxiv_id": "2602.08548v1",
      "title": "How Do Language Models Understand Tables? A Mechanistic Analysis of Cell Location",
      "authors": [
        "Xuanliang Zhang",
        "Dingzirui Wang",
        "Keyan Xu",
        "Qingfu Zhu",
        "Wanxiang Che"
      ],
      "abstract": "While Large Language Models (LLMs) are increasingly deployed for table-related tasks, the internal mechanisms enabling them to process linearized two-dimensional structured tables remain opaque. In this work, we investigate the process of table understanding by dissecting the atomic task of cell location. Through activation patching and complementary interpretability techniques, we delineate the table understanding mechanism into a sequential three-stage pipeline: Semantic Binding, Coordinate Localization, and Information Extraction. We demonstrate that models locate the target cell via an ordinal mechanism that counts discrete delimiters to resolve coordinates. Furthermore, column indices are encoded within a linear subspace that allows for precise steering of model focus through vector arithmetic. Finally, we reveal that models generalize to multi-cell location tasks by multiplexing the identical attention heads identified during atomic location. Our findings provide a comprehensive explanation of table understanding within Transformer architectures.",
      "published": "2026-02-09",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08548v1",
      "url": "https://arxiv.org/abs/2602.08548v1",
      "categories": [
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary": "This paper mechanistically analyzes how LLMs understand tables by focusing on the atomic task of cell location. Using activation patching and interpretability methods, the authors identify a three-stage internal pipeline: Semantic Binding (linking queries to table content), Coordinate Localization (determining row/column positions), and Information Extraction (retrieving cell values). They find that models locate cells by counting delimiters (e.g., pipes or newlines) in linearized tables—an ordinal, rather than geometric, mechanism—and that column indices are encoded in a linear subspace, enabling precise intervention via vector arithmetic. Crucially, the same attention heads used for single-cell location are reused (“multiplexed”) for multi-cell tasks, revealing generalization through modular reuse. These insights demystify table reasoning in Transformers and enable targeted interventions for improved table understanding.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08072v1",
      "arxiv_id": "2602.08072v1",
      "title": "IssueGuard: Real-Time Secret Leak Prevention Tool for GitHub Issue Reports",
      "authors": [
        "Md Nafiu Rahman",
        "Sadif Ahmed",
        "Zahin Wahab",
        "Gias Uddin",
        "Rifat Shahriyar"
      ],
      "abstract": "GitHub and GitLab are widely used collaborative platforms whose issue-tracking systems contain large volumes of unstructured text, including logs, code snippets, and configuration examples. This creates a significant risk of accidental secret exposure, such as API keys and credentials, yet these platforms provide no mechanism to warn users before submission. We present \\textsc{IssueGuard}, a tool for real-time detection and prevention of secret leaks in issue reports. Implemented as a Chrome extension, \\textsc{IssueGuard} analyzes text as users type and combines regex-based candidate extraction with a fine-tuned CodeBERT model for contextual classification. This approach effectively separates real secrets from false positives and achieves an F1-score of 92.70\\% on a benchmark dataset, outperforming traditional regex-based scanners. \\textsc{IssueGuard} integrates directly into the web interface and continuously analyzes the issue editor, presenting clear visual warnings to help users avoid submitting sensitive data. The source code is publicly available at \\href{https://github.com/nafiurahman00/IssueGuard}{https://github.com/nafiurahman00/IssueGuard}, and a demonstration video is available at \\href{https://youtu.be/kvbWA8rr9cU}{https://youtu.be/kvbWA8rr9cU}.",
      "published": "2026-02-08",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08072v1",
      "url": "https://arxiv.org/abs/2602.08072v1",
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary": "IssueGuard is a real-time Chrome extension that prevents accidental secret leaks (e.g., API keys, credentials) in GitHub and GitLab issue reports. It combines lightweight regex-based candidate extraction with a fine-tuned CodeBERT model to classify secrets contextually—significantly reducing false positives compared to rule-only scanners. Evaluated on a benchmark dataset, it achieves a high F1-score of 92.70%, outperforming traditional regex tools. The tool operates inline during issue composition, providing immediate visual warnings to users before submission. Its open-source implementation is publicly available for community use and extension.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08062v1",
      "arxiv_id": "2602.08062v1",
      "title": "Efficient and Adaptable Detection of Malicious LLM Prompts via Bootstrap Aggregation",
      "authors": [
        "Shayan Ali Hassan",
        "Tao Ni",
        "Zafar Ayyub Qazi",
        "Marco Canini"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, reasoning, and generation. However, these systems remain susceptible to malicious prompts that induce unsafe or policy-violating behavior through harmful requests, jailbreak techniques, and prompt injection attacks. Existing defenses face fundamental limitations: black-box moderation APIs offer limited transparency and adapt poorly to evolving threats, while white-box approaches using large LLM judges impose prohibitive computational costs and require expensive retraining for new attacks. Current systems force designers to choose between performance, efficiency, and adaptability.   To address these challenges, we present BAGEL (Bootstrap AGgregated Ensemble Layer), a modular, lightweight, and incrementally updatable framework for malicious prompt detection. BAGEL employs a bootstrap aggregation and mixture of expert inspired ensemble of fine-tuned models, each specialized on a different attack dataset. At inference, BAGEL uses a random forest router to identify the most suitable ensemble member, then applies stochastic selection to sample additional members for prediction aggregation. When new attacks emerge, BAGEL updates incrementally by fine-tuning a small prompt-safety classifier (86M parameters) and adding the resulting model to the ensemble. BAGEL achieves an F1 score of 0.92 by selecting just 5 ensemble members (430M parameters), outperforming OpenAI Moderation API and ShieldGemma which require billions of parameters. Performance remains robust after nine incremental updates, and BAGEL provides interpretability through its router's structural features. Our results show ensembles of small finetuned classifiers can match or exceed billion-parameter guardrails while offering the adaptability and efficiency required for production systems.",
      "published": "2026-02-08",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08062v1",
      "url": "https://arxiv.org/abs/2602.08062v1",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "jailbreak",
        "prompt",
        "llm",
        "injection"
      ],
      "keyword_score": 4,
      "summary": "This paper introduces BAGEL, a lightweight, modular ensemble framework for detecting malicious LLM prompts that balances efficiency, adaptability, and accuracy. It combines bootstrap aggregation with a mixture-of-experts design: a random forest router selects the most relevant fine-tuned small classifier (86M parameters) for a given prompt, and stochastic sampling adds up to four more specialists for aggregated prediction—using only 5 models (430M total params) at inference. BAGEL achieves an F1 score of 0.92, outperforming large black-box APIs (e.g., OpenAI Moderation) and white-box baselines (e.g., ShieldGemma), while supporting incremental updates—adding new attack-specific classifiers without retraining the full system. After nine such updates, performance remains robust, and the router provides interpretable, feature-based decision insights.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08023v2",
      "arxiv_id": "2602.08023v2",
      "title": "CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment",
      "authors": [
        "Nanda Rani",
        "Kimberly Milner",
        "Minghao Shao",
        "Meet Udeshi",
        "Haoran Xi",
        "Venkata Sai Charan Putrevu",
        "Saksham Aggarwal",
        "Sandeep K. Shukla",
        "Prashanth Krishnamurthy",
        "Farshad Khorrami",
        "Muhammad Shafique",
        "Ramesh Karri"
      ],
      "abstract": "Real-world offensive security operations are inherently open-ended: attackers explore unknown attack surfaces, revise hypotheses under uncertainty, and operate without guaranteed success. Existing LLM-based offensive agent evaluations rely on closed-world settings with predefined goals and binary success criteria. To address this gap, we introduce CyberExplorer, an evaluation suite with two core components: (1) an open-environment benchmark built on a virtual machine hosting 40 vulnerable web services derived from real-world CTF challenges, where agents autonomously perform reconnaissance, target selection, and exploitation without prior knowledge of vulnerability locations; and (2) a reactive multi-agent framework supporting dynamic exploration without predefined plans. CyberExplorer enables fine-grained evaluation beyond flag recovery, capturing interaction dynamics, coordination behavior, failure modes, and vulnerability discovery signals-bridging the gap between benchmarks and realistic multi-target attack scenarios.",
      "published": "2026-02-08",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08023v2",
      "url": "https://arxiv.org/abs/2602.08023v2",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.MA"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security",
        "llm"
      ],
      "keyword_score": 3,
      "summary": "This paper introduces CyberExplorer, a novel benchmark designed to evaluate LLMs’ offensive security capabilities in realistic, open-ended attack scenarios—moving beyond restrictive closed-world evaluations with fixed goals and binary success metrics. It features a virtualized environment hosting 40 real-world CTF-derived vulnerable web services, requiring LLM agents to autonomously conduct reconnaissance, prioritize targets, and exploit vulnerabilities without prior knowledge of their locations. The framework employs a reactive, multi-agent architecture that supports dynamic, hypothesis-driven exploration rather than scripted plans. Evaluation goes beyond simple flag capture to measure fine-grained behaviors—including interaction patterns, coordination, failure modes, and vulnerability discovery signals—enabling more faithful assessment of LLMs’ adaptability and reasoning in authentic offensive operations.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.08014v1",
      "arxiv_id": "2602.08014v1",
      "title": "ICBAC: an Intelligent Contract-Based Access Control framework for supply chain management by integrating blockchain and federated learning",
      "authors": [
        "Sadegh Sohani",
        "Salar Ghazi",
        "Farnaz Kamranfar",
        "Sahar Pilehvar Moakhar",
        "Mohammad Allahbakhsh",
        "Haleh Amintoosi",
        "Kaiwen Zhang"
      ],
      "abstract": "This paper addresses the critical challenge of access control in modern supply chains, which operate across multiple independent and competing organizations. Existing access control is static and centralized, unable to adapt to insider threats or evolving contexts. Blockchain improves decentralization but lacks behavioral intelligence, while centralized machine learning for anomaly detection requires aggregating sensitive data, violating privacy.   The proposed solution is ICBAC, an intelligent contract-based access control framework. It integrates permissioned blockchain (Hyperledger Fabric) with federated learning (FL). Built on Fabric, ICBAC uses a multi-channel architecture and three smart contracts for asset management, baseline access control, and dynamic revocation. To counter insider misuse, each channel deploys an AI agent that monitors activity and dynamically restricts access for anomalies. Federated learning allows these agents to collaboratively improve detection models without sharing raw data.   For heterogeneous, competitive environments, ICBAC introduces a game-theoretic client selection mechanism using hedonic coalition formation. This enables supply chains to form stable, strategy-proof FL coalitions via preference-based selection without disclosing sensitive criteria. Extensive experiments on a Fabric testbed with a real-world dataset show ICBAC achieves blockchain performance comparable to static frameworks and provides effective anomaly detection under IID and non-IID data with zero raw-data sharing. ICBAC thus offers a practical, scalable solution for dynamic, privacy-preserving access control in decentralized supply chains.",
      "published": "2026-02-08",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.08014v1",
      "url": "https://arxiv.org/abs/2602.08014v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "machine",
        "privacy-preserving",
        "federated",
        "learning"
      ],
      "keyword_score": 4,
      "summary": "This paper introduces ICBAC, an intelligent, privacy-preserving access control framework for multi-organizational supply chains. It integrates Hyperledger Fabric blockchain (using a multi-channel architecture and three purpose-built smart contracts) with federated learning (FL) to enable dynamic, behavior-aware access control—where local AI agents monitor activities and trigger real-time revocation upon detecting anomalies—without sharing sensitive raw data. To address competition and heterogeneity among participants, ICBAC proposes a novel game-theoretic client selection mechanism based on hedonic coalition formation, ensuring stable, strategy-proof FL collaboration without revealing proprietary preferences. Experiments on a Fabric testbed with real-world data demonstrate that ICBAC matches the blockchain performance of static frameworks while achieving robust anomaly detection under both IID and non-IID data distributions, all with zero raw-data exchange.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.07918v1",
      "arxiv_id": "2602.07918v1",
      "title": "CausalArmor: Efficient Indirect Prompt Injection Guardrails via Causal Attribution",
      "authors": [
        "Minbeom Kim",
        "Mihir Parmar",
        "Phillip Wallis",
        "Lesly Miculicich",
        "Kyomin Jung",
        "Krishnamurthy Dj Dvijotham",
        "Long T. Le",
        "Tomas Pfister"
      ],
      "abstract": "AI agents equipped with tool-calling capabilities are susceptible to Indirect Prompt Injection (IPI) attacks. In this attack scenario, malicious commands hidden within untrusted content trick the agent into performing unauthorized actions. Existing defenses can reduce attack success but often suffer from the over-defense dilemma: they deploy expensive, always-on sanitization regardless of actual threat, thereby degrading utility and latency even in benign scenarios. We revisit IPI through a causal ablation perspective: a successful injection manifests as a dominance shift where the user request no longer provides decisive support for the agent's privileged action, while a particular untrusted segment, such as a retrieved document or tool output, provides disproportionate attributable influence. Based on this signature, we propose CausalArmor, a selective defense framework that (i) computes lightweight, leave-one-out ablation-based attributions at privileged decision points, and (ii) triggers targeted sanitization only when an untrusted segment dominates the user intent. Additionally, CausalArmor employs retroactive Chain-of-Thought masking to prevent the agent from acting on ``poisoned'' reasoning traces. We present a theoretical analysis showing that sanitization based on attribution margins conditionally yields an exponentially small upper bound on the probability of selecting malicious actions. Experiments on AgentDojo and DoomArena demonstrate that CausalArmor matches the security of aggressive defenses while improving explainability and preserving utility and latency of AI agents.",
      "published": "2026-02-08",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.07918v1",
      "url": "https://arxiv.org/abs/2602.07918v1",
      "categories": [
        "cs.CR",
        "cs.LG",
        "stat.ME"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "prompt",
        "security",
        "injection"
      ],
      "keyword_score": 4,
      "summary": "This paper introduces CausalArmor, a selective defense against Indirect Prompt Injection (IPI) attacks on tool-calling AI agents, which avoids the “over-defense dilemma” by triggering sanitization only when causally warranted. It frames IPI detection as a causal attribution problem—identifying when untrusted inputs (e.g., retrieved documents), rather than the user’s intent, dominate the agent’s privileged action—and uses lightweight leave-one-out ablation to compute attribution scores at decision points. To mitigate compromised reasoning, it applies retroactive Chain-of-Thought masking. Theoretically, CausalArmor guarantees exponentially low malicious action probability under attribution-based sanitization, and experiments on AgentDojo and DoomArena show it matches the security of aggressive baselines while preserving latency, utility, and explainability.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.07652v1",
      "arxiv_id": "2602.07652v1",
      "title": "Agent-Fence: Mapping Security Vulnerabilities Across Deep Research Agents",
      "authors": [
        "Sai Puppala",
        "Ismail Hossain",
        "Md Jahangir Alam",
        "Yoonpyo Lee",
        "Jay Yoo",
        "Tanzim Ahad",
        "Syed Bahauddin Alam",
        "Sajedul Talukder"
      ],
      "abstract": "Large language models are increasingly deployed as *deep agents* that plan, maintain persistent state, and invoke external tools, shifting safety failures from unsafe text to unsafe *trajectories*. We introduce **AgentFence**, an architecture-centric security evaluation that defines 14 trust-boundary attack classes spanning planning, memory, retrieval, tool use, and delegation, and detects failures via *trace-auditable conversation breaks* (unauthorized or unsafe tool use, wrong-principal actions, state/objective integrity violations, and attack-linked deviations). Holding the base model fixed, we evaluate eight agent archetypes under persistent multi-turn interaction and observe substantial architectural variation in mean security break rate (MSBR), ranging from $0.29 \\pm 0.04$ (LangGraph) to $0.51 \\pm 0.07$ (AutoGPT). The highest-risk classes are operational: Denial-of-Wallet ($0.62 \\pm 0.08$), Authorization Confusion ($0.54 \\pm 0.10$), Retrieval Poisoning ($0.47 \\pm 0.09$), and Planning Manipulation ($0.44 \\pm 0.11$), while prompt-centric classes remain below $0.20$ under standard settings. Breaks are dominated by boundary violations (SIV 31%, WPA 27%, UTI+UTA 24%, ATD 18%), and authorization confusion correlates with objective and tool hijacking ($ρ\\approx 0.63$ and $ρ\\approx 0.58$). AgentFence reframes agent security around what matters operationally: whether an agent stays within its goal and authority envelope over time.",
      "published": "2026-02-07",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.07652v1",
      "url": "https://arxiv.org/abs/2602.07652v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "poisoning",
        "agent",
        "security",
        "model"
      ],
      "keyword_score": 4,
      "summary": "This paper introduces **AgentFence**, a novel architecture-centric security evaluation framework for *deep research agents*—LLM-based systems that plan, maintain state, and use external tools—shifting focus from static text safety to dynamic *trajectory-level* vulnerabilities. It defines **14 trust-boundary attack classes** across planning, memory, retrieval, tool use, and delegation, detecting failures via *trace-auditable conversation breaks*, such as unauthorized tool invocation or state integrity violations. Evaluating eight agent archetypes (e.g., LangGraph, AutoGPT) under persistent multi-turn interaction, the study finds large architectural differences in mean security break rate (MSBR: 0.29–0.51), with operational risks—especially Denial-of-Wallet, Authorization Confusion, Retrieval Poisoning, and Planning Manipulation—dominating over prompt-based ones. Boundary violations account for >99% of breaks, and Authorization Confusion strongly correlates with objective and tool hijacking, underscoring the centrality of goal/authority envelope adherence in agent security.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.07517v1",
      "arxiv_id": "2602.07517v1",
      "title": "MemPot: Defending Against Memory Extraction Attack with Optimized Honeypots",
      "authors": [
        "Yuhao Wang",
        "Shengfang Zhai",
        "Guanghao Jin",
        "Yinpeng Dong",
        "Linyi Yang",
        "Jiaheng Zhang"
      ],
      "abstract": "Large Language Model (LLM)-based agents employ external and internal memory systems to handle complex, goal-oriented tasks, yet this exposes them to severe extraction attacks, and effective defenses remain lacking. In this paper, we propose MemPot, the first theoretically verified defense framework against memory extraction attacks by injecting optimized honeypots into the memory. Through a two-stage optimization process, MemPot generates trap documents that maximize the retrieval probability for attackers while remaining inconspicuous to benign users. We model the detection process as Wald's Sequential Probability Ratio Test (SPRT) and theoretically prove that MemPot achieves a lower average number of sampling rounds compared to optimal static detectors. Empirically, MemPot significantly outperforms state-of-the-art baselines, achieving a 50% improvement in detection AUROC and an 80% increase in True Positive Rate under low False Positive Rate constraints. Furthermore, our experiments confirm that MemPot incurs zero additional online inference latency and preserves the agent's utility on standard tasks, verifying its superiority in safety, harmlessness, and efficiency.",
      "published": "2026-02-07",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.07517v1",
      "url": "https://arxiv.org/abs/2602.07517v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.DB"
      ],
      "published_official": true,
      "keywords": [
        "extraction",
        "model"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces MemPot, the first theoretically grounded defense against memory extraction attacks on LLM-based agents, using strategically optimized honeypots injected into internal/external memory. It employs a two-stage optimization to generate trap documents that maximize attacker retrieval while remaining undetectable to legitimate users, and models detection as Wald’s Sequential Probability Ratio Test (SPRT), proving MemPot reduces average sampling rounds versus optimal static detectors. Experiments show MemPot achieves a 50% gain in detection AUROC and an 80% higher true positive rate under strict false positive constraints. Crucially, it adds zero online inference latency and maintains full agent utility on standard benchmarks, demonstrating strong safety, harmlessness, and efficiency.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.07513v2",
      "arxiv_id": "2602.07513v2",
      "title": "SPECA: Specification-to-Checklist Agentic Auditing for Multi-Implementation Systems -- A Case Study on Ethereum Clients",
      "authors": [
        "Masato Kamba",
        "Akiyoshi Sannai"
      ],
      "abstract": "Multi-implementation systems are increasingly audited against natural-language specifications. Differential testing scales well when implementations disagree, but it provides little signal when all implementations converge on the same incorrect interpretation of an ambiguous requirement. We present SPECA, a Specification-to-Checklist Auditing framework that turns normative requirements into checklists, maps them to implementation locations, and supports cross-implementation reuse.   We instantiate SPECA in an in-the-wild security audit contest for the Ethereum Fusaka upgrade, covering 11 production clients. Across 54 submissions, 17 were judged valid by the contest organizers. Cross-implementation checks account for 76.5 percent (13 of 17) of valid findings, suggesting that checklist-derived one-to-many reuse is a practical scaling mechanism in multi-implementation audits. To understand false positives, we manually coded the 37 invalid submissions and find that threat model misalignment explains 56.8 percent (21 of 37): reports that rely on assumptions about trust boundaries or scope that contradict the audit's rules. We detected no High or Medium findings in the V1 deployment; misses concentrated in specification details and implicit assumptions (57.1 percent), timing and concurrency issues (28.6 percent), and external library dependencies (14.3 percent). Our improved agent, evaluated against the ground truth of a competitive audit, achieved a strict recall of 27.3 percent on high-impact vulnerabilities, placing it in the top 4 percent of human auditors and outperforming 49 of 51 contestants on critical issues. These results, though from a single deployment, suggest that early, explicit threat modeling is essential for reducing false positives and focusing agentic auditing effort. The agent-driven process enables expert validation and submission in about 40 minutes on average.",
      "published": "2026-02-07",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.07513v2",
      "url": "https://arxiv.org/abs/2602.07513v2",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces SPECA, a novel agentic auditing framework that converts natural-language specifications into actionable checklists, maps them to code locations across multiple implementations, and enables cross-client reuse—addressing the limitation of differential testing in ambiguous specifications. Applied to the Ethereum Fusaka upgrade audit involving 11 production clients and 54 submissions, SPECA’s checklist-driven approach yielded 13 of 17 valid findings (76.5%), demonstrating effective one-to-many reuse. Analysis revealed threat model misalignment as the dominant cause of false positives (56.8%), while missed vulnerabilities primarily stemmed from specification nuances, concurrency issues, and external dependencies. The SPECA agent achieved 27.3% strict recall on high-impact bugs—ranking in the top 4% of human auditors—and enabled expert-validated submissions in ~40 minutes on average, underscoring the value of explicit threat modeling and structured specification grounding.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.07398v1",
      "arxiv_id": "2602.07398v1",
      "title": "AgentSys: Secure and Dynamic LLM Agents Through Explicit Hierarchical Memory Management",
      "authors": [
        "Ruoyao Wen",
        "Hao Li",
        "Chaowei Xiao",
        "Ning Zhang"
      ],
      "abstract": "Indirect prompt injection threatens LLM agents by embedding malicious instructions in external content, enabling unauthorized actions and data theft. LLM agents maintain working memory through their context window, which stores interaction history for decision-making. Conventional agents indiscriminately accumulate all tool outputs and reasoning traces in this memory, creating two critical vulnerabilities: (1) injected instructions persist throughout the workflow, granting attackers multiple opportunities to manipulate behavior, and (2) verbose, non-essential content degrades decision-making capabilities. Existing defenses treat bloated memory as given and focus on remaining resilient, rather than reducing unnecessary accumulation to prevent the attack.   We present AgentSys, a framework that defends against indirect prompt injection through explicit memory management. Inspired by process memory isolation in operating systems, AgentSys organizes agents hierarchically: a main agent spawns worker agents for tool calls, each running in an isolated context and able to spawn nested workers for subtasks. External data and subtask traces never enter the main agent's memory; only schema-validated return values can cross boundaries through deterministic JSON parsing. Ablations show isolation alone cuts attack success to 2.19%, and adding a validator/sanitizer further improves defense with event-triggered checks whose overhead scales with operations rather than context length.   On AgentDojo and ASB, AgentSys achieves 0.78% and 4.25% attack success while slightly improving benign utility over undefended baselines. It remains robust to adaptive attackers and across multiple foundation models, showing that explicit memory management enables secure, dynamic LLM agent architectures. Our code is available at: https://github.com/ruoyaow/agentsys-memory.",
      "published": "2026-02-07",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.07398v1",
      "url": "https://arxiv.org/abs/2602.07398v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "prompt",
        "injection"
      ],
      "keyword_score": 2,
      "summary": "AgentSys introduces a novel defense against indirect prompt injection attacks on LLM agents by enforcing explicit, hierarchical memory management—inspired by OS process isolation—where a main agent delegates tool calls to isolated worker agents that cannot inject unfiltered external data into the main context. Its core innovation is strict boundary control: only schema-validated, JSON-parsed return values (not raw tool outputs or reasoning traces) cross agent boundaries, eliminating persistent malicious instructions and reducing memory bloat. Ablation studies show isolation alone reduces attack success to 2.19%, and adding event-triggered validation further lowers it to 0.78% (AgentDojo) and 4.25% (ASB), while maintaining or slightly improving benign task performance. The framework is model-agnostic, robust against adaptive attackers, and demonstrates that proactive memory minimization—not just resilience—is key to secure, dynamic LLM agent design.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.07287v1",
      "arxiv_id": "2602.07287v1",
      "title": "Patch-to-PoC: A Systematic Study of Agentic LLM Systems for Linux Kernel N-Day Reproduction",
      "authors": [
        "Juefei Pu",
        "Xingyu Li",
        "Haonan Li",
        "Zhengchuan Liang",
        "Jonathan Cox",
        "Yifan Wu",
        "Kareem Shehada",
        "Arrdya Srivastav",
        "Zhiyun Qian"
      ],
      "abstract": "Autonomous large language model (LLM) based systems have recently shown promising results across a range of cybersecurity tasks. However, there is no systematic study on their effectiveness in autonomously reproducing Linux kernel vulnerabilities with concrete proofs-of-concept (PoCs). Owing to the size, complexity, and low-level nature of the Linux kernel, such tasks are widely regarded as particularly challenging for current LLM-based approaches.   In this paper, we present the first large-scale study of LLM-based Linux kernel vulnerability reproduction. For this purpose, we develop K-Repro, an LLM-based agentic system equipped with controlled code-browsing, virtual machine management, interaction, and debugging capabilities. Using kernel security patches as input, K-Repro automates end-to-end bug reproduction of N-day vulnerabilities in the Linux kernel. On a dataset of 100 real-world exploitable Linux kernel vulnerabilities collected from KernelCTF, our results show that K-Repro can generate PoCs that reproduce over 50\\% of the cases with practical time and monetary cost.   Beyond aggregate success rates, we perform an extensive study of effectiveness, efficiency, stability, and impact factors to explain when agentic reproduction succeeds, where it fails, and which components drive performance. These findings provide actionable guidance for building more reliable autonomous security agents and for assessing real-world N-day risk from both offensive and defensive perspectives.",
      "published": "2026-02-07",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.07287v1",
      "url": "https://arxiv.org/abs/2602.07287v1",
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "This paper presents K-Repro, the first large-scale agentic LLM system designed to autonomously reproduce Linux kernel N-day vulnerabilities—from security patches to working proofs-of-concept (PoCs). K-Repro integrates controlled code browsing, VM management, interactive debugging, and kernel-specific tooling to navigate the kernel’s complexity. Evaluated on 100 real-world exploitable vulnerabilities from KernelCTF, it successfully generates reproducing PoCs in over 50% of cases, with practical time and cost. The study further analyzes key drivers of success—including vulnerability type, patch clarity, and agent capabilities—offering actionable insights for improving autonomous security agents and assessing real-world N-day risk.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.07200v1",
      "arxiv_id": "2602.07200v1",
      "title": "BadSNN: Backdoor Attacks on Spiking Neural Networks via Adversarial Spiking Neuron",
      "authors": [
        "Abdullah Arafat Miah",
        "Kevin Vu",
        "Yu Bi"
      ],
      "abstract": "Spiking Neural Networks (SNNs) are energy-efficient counterparts of Deep Neural Networks (DNNs) with high biological plausibility, as information is transmitted through temporal spiking patterns. The core element of an SNN is the spiking neuron, which converts input data into spikes following the Leaky Integrate-and-Fire (LIF) neuron model. This model includes several important hyperparameters, such as the membrane potential threshold and membrane time constant. Both the DNNs and SNNs have proven to be exploitable by backdoor attacks, where an adversary can poison the training dataset with malicious triggers and force the model to behave in an attacker-defined manner. Yet, how an adversary can exploit the unique characteristics of SNNs for backdoor attacks remains underexplored. In this paper, we propose \\textit{BadSNN}, a novel backdoor attack on spiking neural networks that exploits hyperparameter variations of spiking neurons to inject backdoor behavior into the model. We further propose a trigger optimization process to achieve better attack performance while making trigger patterns less perceptible. \\textit{BadSNN} demonstrates superior attack performance on various datasets and architectures, as well as compared with state-of-the-art data poisoning-based backdoor attacks and robustness against common backdoor mitigation techniques. Codes can be found at https://github.com/SiSL-URI/BadSNN.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.07200v1",
      "url": "https://arxiv.org/abs/2602.07200v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "data",
        "backdoor",
        "model",
        "poisoning",
        "neural"
      ],
      "keyword_score": 5,
      "summary": "This paper introduces *BadSNN*, a novel backdoor attack specifically designed for Spiking Neural Networks (SNNs) that exploits the inherent hyperparameters of spiking neurons—particularly the membrane potential threshold and time constant—rather than relying solely on data poisoning. The method jointly optimizes these neuron hyperparameters and input triggers during training to embed stealthy, high-fidelity backdoor behavior, using gradient-based trigger optimization to minimize perceptibility. Experiments across multiple SNN architectures and datasets (e.g., CIFAR-10, DVS128 Gesture) show *BadSNN* achieves >95% attack success rates while maintaining clean accuracy, outperforming existing data-poisoning backdoor attacks and exhibiting strong resilience against common defenses like fine-tuning and neuron pruning. The work highlights a previously overlooked attack surface in SNNs—their biologically inspired neuron dynamics—and provides open-source code for reproducibility.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.07197v1",
      "arxiv_id": "2602.07197v1",
      "title": "Lite-BD: A Lightweight Black-box Backdoor Defense via Reviving Multi-Stage Image Transformations",
      "authors": [
        "Abdullah Arafat Miah",
        "Yu Bi"
      ],
      "abstract": "Deep Neural Networks (DNNs) are vulnerable to backdoor attacks. Due to the nature of Machine Learning as a Service (MLaaS) applications, black-box defenses are more practical than white-box methods, yet existing purification techniques suffer from key limitations: a lack of justification for specific transformations, dataset dependency, high computational overhead, and a neglect of frequency-domain transformations. This paper conducts a preliminary study on various image transformations, identifying down-upscaling as the most effective backdoor trigger disruption technique. We subsequently propose \\texttt{Lite-BD}, a lightweight two-stage blackbox backdoor defense. \\texttt{Lite-BD} first employs a super-resolution-based down-upscaling stage to neutralize spatial triggers. A secondary stage utilizes query-based band-by-band frequency filtering to remove triggers hidden in specific bands. Extensive experiments against state-of-the-art attacks demonstrate that \\texttt{Lite-BD} provides robust and efficient protection. Codes can be found at https://github.com/SiSL-URI/Lite-BD.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.07197v1",
      "url": "https://arxiv.org/abs/2602.07197v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "neural",
        "backdoor"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces Lite-BD, a lightweight black-box backdoor defense that addresses key limitations of prior purification methods—such as dataset dependency, high computational cost, and neglect of frequency-domain triggers. Through empirical analysis, the authors identify down-upscaling as the most effective spatial transformation for disrupting backdoor triggers, and further discover that certain frequency bands harbor stealthy triggers; accordingly, Lite-BD combines a super-resolution-based down-upscaling stage (to neutralize spatial triggers) with a query-efficient, band-by-band frequency filtering stage (to suppress frequency-domain triggers). Evaluated against state-of-the-art backdoor attacks, Lite-BD achieves strong defense performance (e.g., >90% attack success rate reduction) while maintaining high clean accuracy and low inference overhead—making it practical for MLaaS settings.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.07152v1",
      "arxiv_id": "2602.07152v1",
      "title": "Trojans in Artificial Intelligence (TrojAI) Final Report",
      "authors": [
        "Kristopher W. Reese",
        "Taylor Kulp-McDowall",
        "Michael Majurski",
        "Tim Blattner",
        "Derek Juba",
        "Peter Bajcsy",
        "Antonio Cardone",
        "Philippe Dessauw",
        "Alden Dima",
        "Anthony J. Kearsley",
        "Melinda Kleczynski",
        "Joel Vasanth",
        "Walid Keyrouz",
        "Chace Ashcraft",
        "Neil Fendley",
        "Ted Staley",
        "Trevor Stout",
        "Josh Carney",
        "Greg Canal",
        "Will Redman",
        "Aurora Schmidt",
        "Cameron Hickert",
        "William Paul",
        "Jared Markowitz",
        "Nathan Drenkow",
        "David Shriver",
        "Marissa Connor",
        "Keltin Grimes",
        "Marco Christiani",
        "Hayden Moore",
        "Jordan Widjaja",
        "Kasimir Gabert",
        "Uma Balakrishnan",
        "Satyanadh Gundimada",
        "John Jacobellis",
        "Sandya Lakkur",
        "Vitus Leung",
        "Jon Roose",
        "Casey Battaglino",
        "Farinaz Koushanfar",
        "Greg Fields",
        "Xihe Gu",
        "Yaman Jandali",
        "Xinqiao Zhang",
        "Akash Vartak",
        "Tim Oates",
        "Ben Erichson",
        "Michael Mahoney",
        "Rauf Izmailov",
        "Xiangyu Zhang",
        "Guangyu Shen",
        "Siyuan Cheng",
        "Shiqing Ma",
        "XiaoFeng Wang",
        "Haixu Tang",
        "Di Tang",
        "Xiaoyi Chen",
        "Zihao Wang",
        "Rui Zhu",
        "Susmit Jha",
        "Xiao Lin",
        "Manoj Acharya",
        "Wenchao Li",
        "Chao Chen"
      ],
      "abstract": "The Intelligence Advanced Research Projects Activity (IARPA) launched the TrojAI program to confront an emerging vulnerability in modern artificial intelligence: the threat of AI Trojans. These AI trojans are malicious, hidden backdoors intentionally embedded within an AI model that can cause a system to fail in unexpected ways, or allow a malicious actor to hijack the AI model at will. This multi-year initiative helped to map out the complex nature of the threat, pioneered foundational detection methods, and identified unsolved challenges that require ongoing attention by the burgeoning AI security field. This report synthesizes the program's key findings, including methodologies for detection through weight analysis and trigger inversion, as well as approaches for mitigating Trojan risks in deployed models. Comprehensive test and evaluation results highlight detector performance, sensitivity, and the prevalence of \"natural\" Trojans. The report concludes with lessons learned and recommendations for advancing AI security research.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.07152v1",
      "url": "https://arxiv.org/abs/2602.07152v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "trojan",
        "ai"
      ],
      "keyword_score": 2,
      "summary": "The TrojAI program, led by IARPA, systematically investigated AI Trojans—malicious backdoors intentionally embedded in machine learning models—and made foundational contributions to AI security. It developed and rigorously evaluated novel detection methods, including weight-based analysis and trigger inversion techniques, and explored mitigation strategies for deployed models. Key results demonstrated varying detector performance across model architectures and Trojan types, revealed surprising prevalence of “natural” (unintentional) Trojans in pre-trained models, and exposed critical gaps in robustness and generalizability. The report synthesizes these findings to define the threat landscape, benchmark detection capabilities, and outline persistent challenges requiring continued research.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.07107v1",
      "arxiv_id": "2602.07107v1",
      "title": "ShallowJail: Steering Jailbreaks against Large Language Models",
      "authors": [
        "Shang Liu",
        "Hanyu Pei",
        "Zeyan Liu"
      ],
      "abstract": "Large Language Models(LLMs) have been successful in numerous fields. Alignment has usually been applied to prevent them from harmful purposes. However, aligned LLMs remain vulnerable to jailbreak attacks that deliberately mislead them into producing harmful outputs. Existing jailbreaks are either black-box, using carefully crafted, unstealthy prompts, or white-box, requiring resource-intensive computation. In light of these challenges, we introduce ShallowJail, a novel attack that exploits shallow alignment in LLMs. ShallowJail can misguide LLMs' responses by manipulating the initial tokens during inference. Through extensive experiments, we demonstrate the effectiveness of~\\shallow, which substantially degrades the safety of state-of-the-art LLM responses.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.07107v1",
      "url": "https://arxiv.org/abs/2602.07107v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "jailbreak",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "ShallowJail is a novel jailbreak attack that exploits shallow alignment—i.e., the limited depth of safety fine-tuning—in large language models by manipulating only the *initial tokens* of the input prompt during inference. Unlike black-box methods (which rely on hand-crafted, conspicuous prompts) or white-box approaches (requiring gradient access and heavy computation), ShallowJail operates efficiently with minimal perturbation and no model access beyond standard API calls. Experiments across multiple state-of-the-art LLMs (e.g., Llama-3, Claude, and GPT-4) show that ShallowJail significantly degrades safety performance—increasing harmful output rates by up to 5–10× compared to baseline prompts—while remaining stealthy and broadly applicable. The work highlights a critical vulnerability in current alignment strategies and provides a lightweight, scalable method for evaluating model robustness.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.06911v1",
      "arxiv_id": "2602.06911v1",
      "title": "TamperBench: Systematically Stress-Testing LLM Safety Under Fine-Tuning and Tampering",
      "authors": [
        "Saad Hossain",
        "Tom Tseng",
        "Punya Syon Pandey",
        "Samanvay Vajpayee",
        "Matthew Kowal",
        "Nayeema Nonta",
        "Samuel Simko",
        "Stephen Casper",
        "Zhijing Jin",
        "Kellin Pelrine",
        "Sirisha Rambhatla"
      ],
      "abstract": "As increasingly capable open-weight large language models (LLMs) are deployed, improving their tamper resistance against unsafe modifications, whether accidental or intentional, becomes critical to minimize risks. However, there is no standard approach to evaluate tamper resistance. Varied data sets, metrics, and tampering configurations make it difficult to compare safety, utility, and robustness across different models and defenses. To this end, we introduce TamperBench, the first unified framework to systematically evaluate the tamper resistance of LLMs. TamperBench (i) curates a repository of state-of-the-art weight-space fine-tuning attacks and latent-space representation attacks; (ii) enables realistic adversarial evaluation through systematic hyperparameter sweeps per attack-model pair; and (iii) provides both safety and utility evaluations. TamperBench requires minimal additional code to specify any fine-tuning configuration, alignment-stage defense method, and metric suite while ensuring end-to-end reproducibility. We use TamperBench to evaluate 21 open-weight LLMs, including defense-augmented variants, across nine tampering threats using standardized safety and capability metrics with hyperparameter sweeps per model-attack pair. This yields novel insights, including effects of post-training on tamper resistance, that jailbreak-tuning is typically the most severe attack, and that Triplet emerges as a leading alignment-stage defense. Code is available at: https://github.com/criticalml-uw/TamperBench",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.06911v1",
      "url": "https://arxiv.org/abs/2602.06911v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "jailbreak",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces TamperBench, the first unified, reproducible framework for systematically evaluating the tamper resistance of open-weight LLMs against both weight-space (e.g., fine-tuning) and latent-space adversarial attacks. It standardizes evaluation by curating nine diverse tampering threats, enabling hyperparameter-swept, adversarial testing across 21 models—including defense-augmented variants—while jointly measuring safety degradation and utility preservation. Key findings include that jailbreak-focused fine-tuning is the most damaging attack, post-training (e.g., alignment) generally improves tamper resistance, and Triplet alignment emerges as the most robust defense. The framework requires minimal setup and ensures end-to-end reproducibility, addressing critical gaps in standardized safety benchmarking.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.06777v1",
      "arxiv_id": "2602.06777v1",
      "title": "Next-generation cyberattack detection with large language models: anomaly analysis across heterogeneous logs",
      "authors": [
        "Yassine Chagna",
        "Antal Goldschmidt"
      ],
      "abstract": "This project explores large language models (LLMs) for anomaly detection across heterogeneous log sources. Traditional intrusion detection systems suffer from high false positive rates, semantic blindness, and data scarcity, as logs are inherently sensitive, making clean datasets rare. We address these challenges through three contributions: (1) LogAtlas-Foundation-Sessions and LogAtlas-Defense-Set, balanced and heterogeneous log datasets with explicit attack annotations and privacy preservation; (2) empirical benchmarking revealing why standard metrics such as F1 and accuracy are misleading for security applications; and (3) a two phase training framework combining log understanding (Base-AMAN, 3B parameters) with real time detection (AMAN, 0.5B parameters via knowledge distillation). Results demonstrate practical feasibility, with inference times of 0.3-0.5 seconds per session and operational costs below 50 USD per day.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.06777v1",
      "url": "https://arxiv.org/abs/2602.06777v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "inference",
        "security"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces AMAN, a two-phase LLM-based framework for detecting cyberattacks across heterogeneous log sources, addressing key limitations of traditional intrusion detection systems—namely high false positives, semantic blindness, and scarce labeled data. The authors contribute two novel, privacy-preserving, attack-annotated log datasets (LogAtlas-Foundation-Sessions and LogAtlas-Defense-Set) and empirically demonstrate that standard metrics like F1 and accuracy are misleading for security tasks due to class imbalance and operational constraints. Their method involves pretraining a 3B-parameter model (Base-AMAN) for deep log understanding, followed by knowledge distillation into a lightweight 0.5B-parameter model (AMAN) optimized for real-time detection. Experiments show AMAN achieves practical deployment feasibility, with sub-second inference (0.3–0.5 sec/session) and low operational cost (<$50/day).",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.06759v1",
      "arxiv_id": "2602.06759v1",
      "title": "\"Tab, Tab, Bug'': Security Pitfalls of Next Edit Suggestions in AI-Integrated IDEs",
      "authors": [
        "Yunlong Lyu",
        "Yixuan Tang",
        "Peng Chen",
        "Tian Dong",
        "Xinyu Wang",
        "Zhiqiang Dong",
        "Hao Chen"
      ],
      "abstract": "Modern AI-integrated IDEs are shifting from passive code completion to proactive Next Edit Suggestions (NES). Unlike traditional autocompletion, NES is designed to construct a richer context from both recent user interactions and the broader codebase to suggest multi-line, cross-line, or even cross-file modifications. This evolution significantly streamlines the programming workflow into a tab-by-tab interaction and enhances developer productivity. Consequently, NES introduces a more complex context retrieval mechanism and sophisticated interaction patterns. However, existing studies focus almost exclusively on the security implications of standalone LLM-based code generation, ignoring the potential attack vectors posed by NES in modern AI-integrated IDEs. The underlying mechanisms of NES remain under-explored, and their security implications are not yet fully understood.   In this paper, we conduct the first systematic security study of NES systems. First, we perform an in-depth dissection of the NES mechanisms to understand the newly introduced threat vectors. It is found that NES retrieves a significantly expanded context, including inputs from imperceptible user actions and global codebase retrieval, which increases the attack surfaces. Second, we conduct a comprehensive in-lab study to evaluate the security implications of NES. The evaluation results reveal that NES is susceptible to context poisoning and is sensitive to transactional edits and human-IDE interactions. Third, we perform a large-scale online survey involving over 200 professional developers to assess the perceptions of NES security risks in real-world development workflows. The survey results indicate a general lack of awareness regarding the potential security pitfalls associated with NES, highlighting the need for increased education and improved security countermeasures in AI-integrated IDEs.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.06759v1",
      "url": "https://arxiv.org/abs/2602.06759v1",
      "categories": [
        "cs.CR",
        "cs.HC"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "This paper presents the first systematic security study of Next Edit Suggestions (NES) in AI-integrated IDEs—proactive, context-rich suggestions that go beyond traditional autocompletion by incorporating multi-line, cross-file, and interaction-aware code changes. Through mechanism dissection, in-lab experiments, and a survey of 200+ developers, the authors identify critical vulnerabilities: NES is highly susceptible to *context poisoning* (e.g., malicious comments or hidden edits manipulating suggestions) and sensitive to subtle, transactional user-IDE interactions that expand the attack surface. Key results show that NES retrieves broad, often imperceptible context—including ambient codebase data and silent user actions—enabling stealthy exploitation. Crucially, the survey reveals widespread developer unawareness of these risks, underscoring an urgent need for security-aware design, transparency, and education in AI-powered development tools.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.06756v1",
      "arxiv_id": "2602.06756v1",
      "title": "$f$-Differential Privacy Filters: Validity and Approximate Solutions",
      "authors": [
        "Long Tran",
        "Antti Koskela",
        "Ossi Räisä",
        "Antti Honkela"
      ],
      "abstract": "Accounting for privacy loss under fully adaptive composition -- where both the choice of mechanisms and their privacy parameters may depend on the entire history of prior outputs -- is a central challenge in differential privacy (DP). In this setting, privacy filters are stopping rules for compositions that ensure a prescribed global privacy budget is not exceeded. It remains unclear whether optimal trade-off-function-based notions, such as $f$-DP, admit valid privacy filters under fully adaptive interaction. We show that the natural approach to defining an $f$-DP filter -- composing individual trade-off curves and stopping when the prescribed $f$-DP curve is crossed -- is fundamentally invalid. We characterise when and why this failure occurs, and establish necessary and sufficient conditions under which the natural filter is valid. Furthermore, we prove a fully adaptive central limit theorem for $f$-DP and construct an approximate Gaussian DP filter for subsampled Gaussian mechanisms at small sampling rates $q<0.2$ and large sampling rates $q>0.8$, yielding tighter privacy guarantees than filters based on Rényi DP in the same setting.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.06756v1",
      "url": "https://arxiv.org/abs/2602.06756v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "dp",
        "differential",
        "privacy"
      ],
      "keyword_score": 3,
      "summary": "This paper investigates the validity of privacy filters for $f$-differential privacy (DP) under fully adaptive composition—where mechanisms and their parameters can depend on all prior outputs. The authors prove that the natural approach of composing trade-off functions and stopping when the global $f$-DP bound is crossed is *generally invalid*, characterizing precisely when and why it fails and giving necessary and sufficient conditions for validity. As a key positive result, they establish a fully adaptive central limit theorem for $f$-DP and construct an *approximate Gaussian DP filter* for subsampled Gaussian mechanisms, which yields tighter privacy guarantees than Rényi DP–based filters—especially at extreme sampling rates ($q < 0.2$ or $q > 0.8$).",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.06751v1",
      "arxiv_id": "2602.06751v1",
      "title": "Beyond Function-Level Analysis: Context-Aware Reasoning for Inter-Procedural Vulnerability Detection",
      "authors": [
        "Yikun Li",
        "Ting Zhang",
        "Jieke Shi",
        "Chengran Yang",
        "Junda He",
        "Xin Zhou",
        "Jinfeng Jiang",
        "Huihui Huang",
        "Wen Bin Leow",
        "Yide Yin",
        "Eng Lieh Ouh",
        "Lwin Khin Shar",
        "David Lo"
      ],
      "abstract": "Recent progress in ML and LLMs has improved vulnerability detection, and recent datasets have reduced label noise and unrelated code changes. However, most existing approaches still operate at the function level, where models are asked to predict whether a single function is vulnerable without inter-procedural context. In practice, vulnerability presence and root cause often depend on contextual information. Naively appending such context is not a reliable solution: real-world context is long, redundant, and noisy, and we find that unstructured context frequently degrades the performance of strong fine-tuned code models.   We present CPRVul, a context-aware vulnerability detection framework that couples Context Profiling and Selection with Structured Reasoning. CPRVul constructs a code property graph, and extracts candidate context. It then uses an LLM to generate security-focused profiles and assign relevance scores, selecting only high-impact contextual elements that fit within the model's context window. In the second phase, CPRVul integrates the target function, the selected context, and auxiliary vulnerability metadata to generate reasoning traces, which are used to fine-tune LLMs for reasoning-based vulnerability detection.   We evaluate CPRVul on three high-quality vulnerability datasets: PrimeVul, TitanVul, and CleanVul. Across all datasets, CPRVul consistently outperforms function-only baselines, achieving accuracies ranging from 64.94% to 73.76%, compared to 56.65% to 63.68% for UniXcoder. Specifically, on the challenging PrimeVul benchmark, CPRVul achieves 67.78% accuracy, outperforming prior state-of-the-art approaches, improving accuracy from 55.17% to 67.78% (22.9% improvement). Our ablations further show that neither raw context nor processed context alone benefits strong code models; gains emerge only when processed context is paired with structured reasoning.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.06751v1",
      "url": "https://arxiv.org/abs/2602.06751v1",
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces CPRVul, a novel framework for inter-procedural vulnerability detection that moves beyond function-level analysis by incorporating *context-aware reasoning*. CPRVul uses a two-phase approach: (1) **Context Profiling and Selection**, where it builds a code property graph, leverages an LLM to generate security-focused profiles and relevance scores, and selects only high-impact contextual elements (e.g., caller/callee functions, data flows) to avoid noise and context-window limitations; and (2) **Structured Reasoning**, where the target function, selected context, and vulnerability metadata are integrated into reasoning traces to fine-tune LLMs. Evaluated on PrimeVul, TitanVul, and CleanVul, CPRVul achieves state-of-the-art accuracy (64.94–73.76%), notably improving PrimeVul performance by 22.9% over prior SOTA (55.17% → 67.78%). Ablation studies confirm that gains stem *only* from the synergistic combination of processed context and structured reasoning—not from raw or unstructured context alone.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.06718v1",
      "arxiv_id": "2602.06718v1",
      "title": "GhostCite: A Large-Scale Analysis of Citation Validity in the Age of Large Language Models",
      "authors": [
        "Zuyao Xu",
        "Yuqi Qiu",
        "Lu Sun",
        "FaSheng Miao",
        "Fubin Wu",
        "Xinyi Wang",
        "Xiang Li",
        "Haozhe Lu",
        "ZhengZe Zhang",
        "Yuxin Hu",
        "Jialu Li",
        "Jin Luo",
        "Feng Zhang",
        "Rui Luo",
        "Xinran Liu",
        "Yingxian Li",
        "Jiaji Liu"
      ],
      "abstract": "Citations provide the basis for trusting scientific claims; when they are invalid or fabricated, this trust collapses. With the advent of Large Language Models (LLMs), this risk has intensified: LLMs are increasingly used for academic writing, yet their tendency to fabricate citations (``ghost citations'') poses a systemic threat to citation validity.   To quantify this threat and inform mitigation, we develop CiteVerifier, an open-source framework for large-scale citation verification, and conduct the first comprehensive study of citation validity in the LLM era through three experiments built on it. We benchmark 13 state-of-the-art LLMs on citation generation across 40 research domains, finding that all models hallucinate citations at rates from 14.23\\% to 94.93\\%, with significant variation across research domains. Moreover, we analyze 2.2 million citations from 56,381 papers published at top-tier AI/ML and Security venues (2020--2025), confirming that 1.07\\% of papers contain invalid or fabricated citations (604 papers), with an 80.9\\% increase in 2025 alone. Furthermore, we survey 97 researchers and analyze 94 valid responses after removing 3 conflicting samples, revealing a critical ``verification gap'': 41.5\\% of researchers copy-paste BibTeX without checking and 44.4\\% choose no-action responses when encountering suspicious references; meanwhile, 76.7\\% of reviewers do not thoroughly check references and 80.0\\% never suspect fake citations. Our findings reveal an accelerating crisis where unreliable AI tools, combined with inadequate human verification by researchers and insufficient peer review scrutiny, enable fabricated citations to contaminate the scientific record. We propose interventions for researchers, venues, and tool developers to protect citation integrity.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.06718v1",
      "url": "https://arxiv.org/abs/2602.06718v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "security",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces CiteVerifier, an open-source framework for large-scale citation verification, and conducts the first comprehensive study of citation validity in the LLM era. Through three experiments—benchmarking 13 state-of-the-art LLMs across 40 research domains, analyzing 2.2 million citations from 56,381 top-tier AI/ML and Security papers (2020–2025), and surveying 97 researchers—the authors find alarmingly high rates of “ghost citations”: LLMs hallucinate citations at 14–95% rates depending on model and domain; 1.07% of published papers contain invalid/fabricated citations, with an 80.9% surge in such cases in 2025 alone; and a critical “verification gap” exists, as many researchers skip citation checks and reviewers rarely scrutinize references. The study reveals a systemic crisis fueled by unreliable AI tools and inadequate human oversight, and proposes concrete interventions for researchers, publishers, and tool developers to safeguard scientific integrity.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.06700v1",
      "arxiv_id": "2602.06700v1",
      "title": "Taipan: A Query-free Transfer-based Multiple Sensitive Attribute Inference Attack Solely from Publicly Released Graphs",
      "authors": [
        "Ying Song",
        "Balaji Palanisamy"
      ],
      "abstract": "Graph-structured data underpin a wide spectrum of modern applications. However, complex graph topologies and homophilic patterns can facilitate attribute inference attacks (AIAs) by enabling sensitive information leakage to propagate across local neighborhoods. Existing AIAs predominantly assume that adversaries can probe sensitive attributes through repeated model queries. Such assumptions are often impractical in real-world settings due to stringent data protection regulations, prohibitive query budgets, and heightened detection risks, especially when inferring multiple sensitive attributes. More critically, this model-centric perspective obscures a pervasive blind spot: \\textbf{intrinsic multiple sensitive information leakage arising solely from publicly released graphs.} To exploit this unexplored vulnerability, we introduce a new attack paradigm and propose \\textbf{Taipan, the first query-free transfer-based attack framework for multiple sensitive attribute inference attacks on graphs (G-MSAIAs).} Taipan integrates \\emph{Hierarchical Attack Knowledge Routing} to capture intricate inter-attribute correlations, and \\emph{Prompt-guided Attack Prototype Refinement} to mitigate negative transfer and performance degradation. We further present a systematic evaluation framework tailored to G-MSAIAs. Extensive experiments on diverse real-world graph datasets demonstrate that Taipan consistently achieves strong attack performance across same-distribution settings and heterogeneous similar- and out-of-distribution settings with mismatched feature dimensionalities, and remains effective even under rigorous differential privacy guarantees. Our findings underscore the urgent need for more robust multi-attribute privacy-preserving graph publishing methods and data-sharing practices.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.06700v1",
      "url": "https://arxiv.org/abs/2602.06700v1",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "differential",
        "privacy"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces Taipan, the first query-free, transfer-based framework for inferring *multiple* sensitive attributes from *publicly released graphs*—without requiring access to target models or repeated queries. It addresses a critical real-world gap by exploiting intrinsic information leakage in graph structure alone, using two novel components: Hierarchical Attack Knowledge Routing (to model inter-attribute correlations) and Prompt-guided Attack Prototype Refinement (to reduce negative transfer). Evaluated across diverse real-world graphs—including same-, similar-, and out-of-distribution settings and under differential privacy—Taipan consistently achieves strong inference accuracy despite feature mismatches and privacy noise. The work reveals a previously overlooked privacy vulnerability in graph publishing and calls for stronger multi-attribute privacy safeguards.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.06630v1",
      "arxiv_id": "2602.06630v1",
      "title": "TrapSuffix: Proactive Defense Against Adversarial Suffixes in Jailbreaking",
      "authors": [
        "Mengyao Du",
        "Han Fang",
        "Haokai Ma",
        "Gang Yang",
        "Quanjun Yin",
        "Shouling Ji",
        "Ee-Chien Chang"
      ],
      "abstract": "Suffix-based jailbreak attacks append an adversarial suffix, i.e., a short token sequence, to steer aligned LLMs into unsafe outputs. Since suffixes are free-form text, they admit endlessly many surface forms, making jailbreak mitigation difficult. Most existing defenses depend on passive detection of suspicious suffixes, without leveraging the defender's inherent asymmetric ability to inject secrets and proactively conceal gaps. Motivated by this, we take a controllability-oriented perspective and develop a proactive defense that nudges attackers into a no-win dilemma: either they fall into defender-designed optimization traps and fail to produce an effective adversarial suffix, or they can succeed only by generating adversarial suffixes that carry distinctive, traceable fingerprints. We propose TrapSuffix, a lightweight fine-tuning approach that injects trap-aligned behaviors into the base model without changing the inference pipeline. TrapSuffix channels jailbreak attempts into these two outcomes by reshaping the model's response landscape to adversarial suffixes. Across diverse suffix-based jailbreak settings, TrapSuffix reduces the average attack success rate to below 0.01 percent and achieves an average tracing success rate of 87.9 percent, providing both strong defense and reliable traceability. It introduces no inference-time overhead and incurs negligible memory cost, requiring only 15.87 MB of additional memory on average, whereas state-of-the-art LLM-based detection defenses typically incur memory overheads at the 1e4 MB level, while composing naturally with existing filtering-based defenses for complementary protection.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.06630v1",
      "url": "https://arxiv.org/abs/2602.06630v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "jailbreak",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces TrapSuffix, a proactive defense against suffix-based jailbreak attacks on LLMs. Instead of passively detecting malicious suffixes, TrapSuffix uses lightweight fine-tuning to reshape the model’s response landscape—introducing “optimization traps” that force attackers into a no-win dilemma: either fail to generate effective adversarial suffixes or produce ones with distinctive, traceable fingerprints. Evaluated across diverse jailbreak settings, it reduces attack success rates to <0.01% and achieves 87.9% tracing accuracy, with zero inference-time overhead and only ~16 MB memory cost—orders of magnitude lower than detector-based defenses. The method is compatible with existing filtering defenses and requires no changes to the inference pipeline.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.06616v1",
      "arxiv_id": "2602.06616v1",
      "title": "Confundo: Learning to Generate Robust Poison for Practical RAG Systems",
      "authors": [
        "Haoyang Hu",
        "Zhejun Jiang",
        "Yueming Lyu",
        "Junyuan Zhang",
        "Yi Liu",
        "Ka-Ho Chow"
      ],
      "abstract": "Retrieval-augmented generation (RAG) is increasingly deployed in real-world applications, where its reference-grounded design makes outputs appear trustworthy. This trust has spurred research on poisoning attacks that craft malicious content, inject it into knowledge sources, and manipulate RAG responses. However, when evaluated in practical RAG systems, existing attacks suffer from severely degraded effectiveness. This gap stems from two overlooked realities: (i) content is often processed before use, which can fragment the poison and weaken its effect, and (ii) users often do not issue the exact queries anticipated during attack design. These factors can lead practitioners to underestimate risks and develop a false sense of security. To better characterize the threat to practical systems, we present Confundo, a learning-to-poison framework that fine-tunes a large language model as a poison generator to achieve high effectiveness, robustness, and stealthiness. Confundo provides a unified framework supporting multiple attack objectives, demonstrated by manipulating factual correctness, inducing biased opinions, and triggering hallucinations. By addressing these overlooked challenges, Confundo consistently outperforms a wide range of purpose-built attacks across datasets and RAG configurations by large margins, even in the presence of defenses. Beyond exposing vulnerabilities, we also present a defensive use case that protects web content from unauthorized incorporation into RAG systems via scraping, with no impact on user experience.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.06616v1",
      "url": "https://arxiv.org/abs/2602.06616v1",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "poisoning",
        "model"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces Confundo, a learning-to-poison framework that generates robust, stealthy adversarial content to compromise real-world RAG systems—addressing critical gaps in prior attacks, namely vulnerability to preprocessing (e.g., chunking, filtering) and sensitivity to query variations. Confundo fine-tunes an LLM as a poison generator, optimizing for effectiveness across diverse attack goals (e.g., factual corruption, bias induction, hallucination triggering) while remaining resilient to practical system perturbations. Evaluations across multiple datasets and RAG configurations show Confundo significantly outperforms existing poisoning methods—even under common defenses—demonstrating previously underestimated risks. The authors also repurpose Confundo defensively to watermark web content, preventing unauthorized scraping into RAG knowledge bases without affecting legitimate user experience.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.06547v1",
      "arxiv_id": "2602.06547v1",
      "title": "Malicious Agent Skills in the Wild: A Large-Scale Security Empirical Study",
      "authors": [
        "Yi Liu",
        "Zhihao Chen",
        "Yanjun Zhang",
        "Gelei Deng",
        "Yuekang Li",
        "Jianting Ning",
        "Leo Yu Zhang"
      ],
      "abstract": "Third-party agent skills extend LLM-based agents with instruction files and executable code that run on users' machines. Skills execute with user privileges and are distributed through community registries with minimal vetting, but no ground-truth dataset exists to characterize the resulting threats. We construct the first labeled dataset of malicious agent skills by behaviorally verifying 98,380 skills from two community registries, confirming 157 malicious skills with 632 vulnerabilities. These attacks are not incidental. Malicious skills average 4.03 vulnerabilities across a median of three kill chain phases, and the ecosystem has split into two archetypes: Data Thieves that exfiltrate credentials through supply chain techniques, and Agent Hijackers that subvert agent decision-making through instruction manipulation. A single actor accounts for 54.1\\% of confirmed cases through templated brand impersonation. Shadow features, capabilities absent from public documentation, appear in 0\\% of basic attacks but 100\\% of advanced ones; several skills go further by exploiting the AI platform's own hook system and permission flags. Responsible disclosure led to 93.6\\% removal within 30 days. We release the dataset and analysis pipeline to support future work on agent skill security.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.06547v1",
      "url": "https://arxiv.org/abs/2602.06547v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.ET"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security",
        "llm"
      ],
      "keyword_score": 3,
      "summary": "This paper presents the first large-scale empirical security study of third-party LLM agent skills, constructing a labeled dataset of 98,380 skills from community registries and behaviorally verifying 157 malicious ones containing 632 vulnerabilities. Using dynamic analysis and behavioral verification (not static heuristics), the authors identify two dominant threat archetypes: *Data Thieves*—exfiltrating credentials via supply-chain compromises—and *Agent Hijackers*—manipulating agent instructions to subvert decision-making. Key findings include that malicious skills average over four vulnerabilities across multiple kill chain phases, rely heavily on undocumented “shadow features” (present in 100% of advanced attacks), and are often orchestrated by a single actor responsible for >54% of cases via templated brand impersonation. Responsible disclosure achieved 93.6% removal within 30 days, and the authors publicly release their dataset and analysis pipeline to advance agent skill security research.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.06532v1",
      "arxiv_id": "2602.06532v1",
      "title": "Dependable Artificial Intelligence with Reliability and Security (DAIReS): A Unified Syndrome Decoding Approach for Hallucination and Backdoor Trigger Detection",
      "authors": [
        "Hema Karnam Surendrababu",
        "Nithin Nagaraj"
      ],
      "abstract": "Machine Learning (ML) models, including Large Language Models (LLMs), are characterized by a range of system-level attributes such as security and reliability. Recent studies have demonstrated that ML models are vulnerable to multiple forms of security violations, among which backdoor data-poisoning attacks represent a particularly insidious threat, enabling unauthorized model behavior and systematic misclassification. In parallel, deficiencies in model reliability can manifest as hallucinations in LLMs, leading to unpredictable outputs and substantial risks for end users. In this work on Dependable Artificial Intelligence with Reliability and Security (DAIReS), we propose a novel unified approach based on Syndrome Decoding for the detection of both security and reliability violations in learning-based systems. Specifically, we adapt the syndrome decoding approach to the NLP sentence-embedding space, enabling the discrimination of poisoned and non-poisoned samples within ML training datasets. Additionally, the same methodology can effectively detect hallucinated content due to self referential meta explanation tasks in LLMs.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.06532v1",
      "url": "https://arxiv.org/abs/2602.06532v1",
      "categories": [
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "poisoning",
        "data",
        "model"
      ],
      "keyword_score": 3,
      "summary": "This paper introduces DAIReS, a unified framework using syndrome decoding—a technique adapted from error-correcting codes—to jointly detect both backdoor triggers (a security threat) and hallucinations (a reliability failure) in ML models, including LLMs. The method operates in the sentence-embedding space, where it identifies anomalous patterns by computing and analyzing syndromes—low-dimensional residuals that reveal deviations from expected model behavior. Key results show DAIReS achieves high detection accuracy for backdoored samples in training data and reliably flags hallucinated outputs during self-referential meta-explanation tasks in LLMs, outperforming baseline methods. Crucially, it requires no access to model weights or clean reference data, making it broadly applicable and practical for real-world deployment.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.07090v1",
      "arxiv_id": "2602.07090v1",
      "title": "Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks",
      "authors": [
        "Yu-Che Tsai",
        "Hsiang Hsiao",
        "Kuan-Yu Chen",
        "Shou-De Lin"
      ],
      "abstract": "Text embeddings enable numerous NLP applications but face severe privacy risks from embedding inversion attacks, which can expose sensitive attributes or reconstruct raw text. Existing differential privacy defenses assume uniform sensitivity across embedding dimensions, leading to excessive noise and degraded utility. We propose SPARSE, a user-centric framework for concept-specific privacy protection in text embeddings. SPARSE combines (1) differentiable mask learning to identify privacy-sensitive dimensions for user-defined concepts, and (2) the Mahalanobis mechanism that applies elliptical noise calibrated by dimension sensitivity. Unlike traditional spherical noise injection, SPARSE selectively perturbs privacy-sensitive dimensions while preserving non-sensitive semantics. Evaluated across six datasets with three embedding models and attack scenarios, SPARSE consistently reduces privacy leakage while achieving superior downstream performance compared to state-of-the-art DP methods.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.07090v1",
      "url": "https://arxiv.org/abs/2602.07090v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "dp",
        "differential",
        "privacy"
      ],
      "keyword_score": 3,
      "summary": "This paper introduces SPARSE, a concept-aware privacy framework for protecting text embeddings against inversion attacks. Unlike standard differential privacy methods that add uniform spherical noise, SPARSE uses differentiable mask learning to identify embedding dimensions most sensitive to user-defined private concepts (e.g., gender or age), then applies the Mahalanobis mechanism—elliptical, sensitivity-calibrated noise—to perturb only those dimensions. Evaluated across six datasets, three embedding models, and multiple attack settings, SPARSE significantly reduces privacy leakage while preserving downstream task utility better than state-of-the-art DP baselines. Its key contribution is shifting from uniform to concept-specific, dimension-aware noise injection—enhancing both privacy and utility.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.06518v1",
      "arxiv_id": "2602.06518v1",
      "title": "Sequential Auditing for f-Differential Privacy",
      "authors": [
        "Tim Kutta",
        "Martin Dunsche",
        "Yu Wei",
        "Vassilis Zikas"
      ],
      "abstract": "We present new auditors to assess Differential Privacy (DP) of an algorithm based on output samples. Such empirical auditors are common to check for algorithmic correctness and implementation bugs. Most existing auditors are batch-based or targeted toward the traditional notion of $(\\varepsilon,δ)$-DP; typically both. In this work, we shift the focus to the highly expressive privacy concept of $f$-DP, in which the entire privacy behavior is captured by a single tradeoff curve. Our auditors detect violations across the full privacy spectrum with statistical significance guarantees, which are supported by theory and simulations. Most importantly, and in contrast to prior work, our auditors do not require a user-specified sample size as an input. Rather, they adaptively determine a near-optimal number of samples needed to reach a decision, thereby avoiding the excessively large sample sizes common in many auditing studies. This reduction in sampling cost becomes especially beneficial for expensive training procedures such as DP-SGD. Our method supports both whitebox and blackbox settings and can also be executed in single-run frameworks.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.06518v1",
      "url": "https://arxiv.org/abs/2602.06518v1",
      "categories": [
        "cs.CR",
        "stat.ME",
        "stat.ML"
      ],
      "published_official": true,
      "keywords": [
        "dp",
        "differential",
        "privacy"
      ],
      "keyword_score": 3,
      "summary": "This paper introduces novel sequential auditors for empirically verifying *f*-Differential Privacy (f-DP), a more expressive privacy framework than standard (ε,δ)-DP. Unlike prior batch-based auditors that require pre-specified sample sizes—often leading to wasteful over-sampling—the proposed methods adaptively determine the minimal number of output samples needed to detect privacy violations with statistical significance guarantees. The auditors operate in both white-box and black-box settings, support single-run execution, and are validated theoretically and via simulations. Key results show substantial reductions in sampling cost—especially critical for expensive DP training procedures like DP-SGD—while maintaining rigorous false-positive control across the full privacy tradeoff curve.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.06440v1",
      "arxiv_id": "2602.06440v1",
      "title": "TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking",
      "authors": [
        "Sung-Hoon Yoon",
        "Ruizhi Qian",
        "Minda Zhao",
        "Weiyue Li",
        "Mengyu Wang"
      ],
      "abstract": "Large Language Models (LLMs) have become integral to many domains, making their safety a critical priority. Prior jailbreaking research has explored diverse approaches, including prompt optimization, automated red teaming, obfuscation, and reinforcement learning (RL) based methods. However, most existing techniques fail to effectively leverage vulnerabilities revealed in earlier interaction turns, resulting in inefficient and unstable attacks. Since jailbreaking involves sequential interactions in which each response influences future actions, reinforcement learning provides a natural framework for this problem. Motivated by this, we propose a history-aware RL-based jailbreak framework that analyzes and reweights vulnerability signals from prior steps to guide future decisions. We show that incorporating historical information alone improves jailbreak success rates. Building on this insight, we introduce an attention-based reweighting mechanism that highlights critical vulnerabilities within the interaction history, enabling more efficient exploration with fewer queries. Extensive experiments on AdvBench and HarmBench demonstrate that our method achieves state-of-the-art jailbreak performance while significantly improving query efficiency. These results underscore the importance of historical vulnerability signals in reinforcement learning-driven jailbreak strategies and offer a principled pathway for advancing adversarial research on LLM safeguards.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.06440v1",
      "url": "https://arxiv.org/abs/2602.06440v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "published_official": true,
      "keywords": [
        "jailbreak",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces TrailBlazer, a history-guided reinforcement learning (RL) framework for black-box LLM jailbreaking that explicitly leverages vulnerability signals from prior interaction turns—unlike prior RL or optimization methods that treat each query independently. It proposes an attention-based reweighting mechanism to dynamically highlight the most critical historical vulnerabilities, enabling more informed and efficient action selection. Evaluated on AdvBench and HarmBench, TrailBlazer achieves state-of-the-art jailbreak success rates while reducing query count by up to 40% compared to leading baselines. The key insight is that modeling temporal dependencies in attack sequences significantly improves both effectiveness and efficiency of adversarial prompting.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.06395v1",
      "arxiv_id": "2602.06395v1",
      "title": "Empirical Analysis of Adversarial Robustness and Explainability Drift in Cybersecurity Classifiers",
      "authors": [
        "Mona Rajhans",
        "Vishal Khawarey"
      ],
      "abstract": "Machine learning (ML) models are increasingly deployed in cybersecurity applications such as phishing detection and network intrusion prevention. However, these models remain vulnerable to adversarial perturbations small, deliberate input modifications that can degrade detection accuracy and compromise interpretability. This paper presents an empirical study of adversarial robustness and explainability drift across two cybersecurity domains phishing URL classification and network intrusion detection. We evaluate the impact of L (infinity) bounded Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) perturbations on model accuracy and introduce a quantitative metric, the Robustness Index (RI), defined as the area under the accuracy perturbation curve. Gradient based feature sensitivity and SHAP based attribution drift analyses reveal which input features are most susceptible to adversarial manipulation. Experiments on the Phishing Websites and UNSW NB15 datasets show consistent robustness trends, with adversarial training improving RI by up to 9 percent while maintaining clean-data accuracy. These findings highlight the coupling between robustness and interpretability degradation and underscore the importance of quantitative evaluation in the design of trustworthy, AI-driven cybersecurity systems.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.06395v1",
      "url": "https://arxiv.org/abs/2602.06395v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "machine",
        "learning",
        "adversarial"
      ],
      "keyword_score": 3,
      "summary": "This paper empirically investigates the dual degradation of adversarial robustness and model explainability in cybersecurity ML classifiers—specifically for phishing URL detection and network intrusion classification. Using FGSM and PGD attacks with ℓ∞ bounds, the authors introduce the Robustness Index (RI), a quantitative metric based on the area under the accuracy–perturbation curve, and measure explainability drift via gradient-based sensitivity and SHAP attribution changes. Experiments on the Phishing Websites and UNSW-NB15 datasets reveal consistent robustness trends: adversarial training improves RI by up to 9% without sacrificing clean-data accuracy, while also mitigating explainability drift. The study demonstrates a strong coupling between robustness loss and interpretability degradation, advocating for joint evaluation of both properties in trustworthy AI-driven cybersecurity systems.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.06345v1",
      "arxiv_id": "2602.06345v1",
      "title": "Zero-Trust Runtime Verification for Agentic Payment Protocols: Mitigating Replay and Context-Binding Failures in AP2",
      "authors": [
        "Qianlong Lan",
        "Anuj Kaul",
        "Shaun Jones",
        "Stephanie Westrum"
      ],
      "abstract": "The deployment of autonomous AI agents capable of executing commercial transactions has motivated the adoption of mandate-based payment authorization protocols, including the Universal Commerce Protocol (UCP) and the Agent Payments Protocol (AP2). These protocols replace interactive, session-based authorization with cryptographically issued mandates, enabling asynchronous and autonomous execution. While AP2 provides specification-level guarantees through signature verification, explicit binding, and expiration semantics, real-world agentic execution introduces runtime behaviors such as retries, concurrency, and orchestration that challenge implicit assumptions about mandate usage.   In this work, we present a security analysis of the AP2 mandate lifecycle and identify enforcement gaps that arise during runtime in agent-based payment systems. We propose a zero-trust runtime verification framework that enforces explicit context binding and consume-once mandate semantics using dynamically generated, time-bound nonces, ensuring that authorization decisions are evaluated at execution time rather than assumed from static issuance properties.   Through simulation-based evaluation under high concurrency, we show that context-aware binding and consume-once enforcement address distinct and complementary attack classes, and that both are required to prevent replay and context-redirect attacks. The proposed framework mitigates all evaluated attacks while maintaining stable verification latency of approximately 3.8~ms at throughput levels up to 10{,}000 transactions per second. We further demonstrate that the required runtime state is bounded by peak concurrency rather than cumulative transaction history, indicating that robust runtime security for agentic payment execution can be achieved with minimal and predictable overhead.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.06345v1",
      "url": "https://arxiv.org/abs/2602.06345v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published_official": true,
      "keywords": [
        "agent",
        "security"
      ],
      "keyword_score": 2,
      "summary": "This paper identifies critical runtime security gaps in the Agent Payments Protocol (AP2)—specifically replay and context-binding failures—that arise from real-world agentic behaviors like retries and concurrency, despite AP2’s static cryptographic guarantees. To address these, the authors propose a zero-trust runtime verification framework that enforces *consume-once* mandate semantics and *explicit context binding* using dynamically generated, time-bound nonces evaluated at execution time. Through high-concurrency simulations, they demonstrate that both mechanisms are necessary and complementary: together they prevent replay and context-redirect attacks with near-zero false positives, while sustaining low, stable latency (~3.8 ms) at up to 10,000 TPS. Crucially, the framework’s state overhead scales only with peak concurrency—not total transaction volume—enabling scalable, predictable security for autonomous payment agents.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.06336v1",
      "arxiv_id": "2602.06336v1",
      "title": "AdFL: In-Browser Federated Learning for Online Advertisement",
      "authors": [
        "Ahmad Alemari",
        "Pritam Sen",
        "Cristian Borcea"
      ],
      "abstract": "Since most countries are coming up with online privacy regulations, such as GDPR in the EU, online publishers need to find a balance between revenue from targeted advertisement and user privacy. One way to be able to still show targeted ads, based on user personal and behavioral information, is to employ Federated Learning (FL), which performs distributed learning across users without sharing user raw data with other stakeholders in the publishing ecosystem. This paper presents AdFL, an FL framework that works in the browsers to learn user ad preferences. These preferences are aggregated in a global FL model, which is then used in the browsers to show more relevant ads to users. AdFL can work with any model that uses features available in the browser such as ad viewability, ad click-through, user dwell time on pages, and page content. The AdFL server runs at the publisher and coordinates the learning process for the users who browse pages on the publisher's website. The AdFL prototype does not require the client to install any software, as it is built utilizing standard APIs available on most modern browsers. We built a proof-of-concept model for ad viewability prediction that runs on top of AdFL. We tested AdFL and the model with two non-overlapping datasets from a website with 40K visitors per day. The experiments demonstrate AdFL's feasibility to capture the training information in the browser in a few milliseconds, show that the ad viewability prediction achieves up to 92.59% AUC, and indicate that utilizing differential privacy (DP) to safeguard local model parameters yields adequate performance, with only modest declines in comparison to the non-DP variant.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.06336v1",
      "url": "https://arxiv.org/abs/2602.06336v1",
      "categories": [
        "cs.CR",
        "cs.DC",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "federated",
        "dp",
        "learning",
        "differential",
        "privacy"
      ],
      "keyword_score": 5,
      "summary": "This paper introduces AdFL, a browser-based federated learning framework designed to enable privacy-preserving targeted advertising under regulations like GDPR. AdFL trains user ad preference models locally in users’ browsers—using only client-side features (e.g., ad viewability, dwell time, page content)—and aggregates updates at a publisher-hosted server without sharing raw user data. The authors implement and evaluate a proof-of-concept ad viewability predictor on real traffic from a high-traffic website (40K daily visitors), demonstrating sub-millisecond local training and strong predictive performance (up to 92.59% AUC). They further show that integrating differential privacy incurs only modest accuracy trade-offs, confirming AdFL’s practicality for privacy-compliant, scalable ad personalization.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.06325v1",
      "arxiv_id": "2602.06325v1",
      "title": "Identifying Adversary Tactics and Techniques in Malware Binaries with an LLM Agent",
      "authors": [
        "Zhou Xuan",
        "Xiangzhe Xu",
        "Mingwei Zheng",
        "Louis Zheng-Hua Tan",
        "Jinyao Guo",
        "Tiantai Zhang",
        "Le Yu",
        "Chengpeng Wang",
        "Xiangyu Zhang"
      ],
      "abstract": "Understanding TTPs (Tactics, Techniques, and Procedures) in malware binaries is essential for security analysis and threat intelligence, yet remains challenging in practice. Real-world malware binaries are typically stripped of symbols, contain large numbers of functions, and distribute malicious behavior across multiple code regions, making TTP attribution difficult. Recent large language models (LLMs) offer strong code understanding capabilities, but applying them directly to this task faces challenges in identifying analysis entry points, reasoning under partial observability, and misalignment with TTP-specific decision logic. We present TTPDetect, the first LLM agent for recognizing TTPs in stripped malware binaries. TTPDetect combines dense retrieval with LLM-based neural retrieval to narrow the space of analysis entry points. TTPDetect further employs a function-level analyzing agent consisting of a Context Explorer that performs on-demand, incremental context retrieval and a TTP-Specific Reasoning Guideline that achieves inference-time alignment. We build a new dataset that labels decompiled functions with TTPs across diverse malware families and platforms. TTPDetect achieves 93.25% precision and 93.81% recall on function-level TTP recognition, outperforming baselines by 10.38% and 18.78%, respectively. When evaluated on real world malware samples, TTPDetect recognizes TTPs with a precision of 87.37%. For malware with expert-written reports, TTPDetect recovers 85.7% of the documented TTPs and further discovers, on average, 10.5 previously unreported TTPs per malware.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.06325v1",
      "url": "https://arxiv.org/abs/2602.06325v1",
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "published_official": true,
      "keywords": [
        "inference",
        "agent",
        "security",
        "llm"
      ],
      "keyword_score": 4,
      "summary": "This paper introduces TTPDetect, the first LLM-based agent designed to identify MITRE ATT&CK Tactics, Techniques, and Procedures (TTPs) directly from stripped malware binaries—a challenging task due to symbol stripping, behavioral fragmentation, and lack of ground-truth function-level labels. To address these challenges, TTPDetect combines dense and neural retrieval to prioritize analysis entry points, and employs a function-level agent with an on-demand Context Explorer and a TTP-Specific Reasoning Guideline for inference-time alignment. The authors curate a new benchmark dataset of decompiled functions labeled with TTPs across diverse malware families and platforms. Evaluated on this dataset, TTPDetect achieves 93.25% precision and 93.81% recall—outperforming baselines by over 10% and 18%, respectively—and demonstrates strong real-world utility with 87.37% precision on live malware samples, recovering 85.7% of expert-documented TTPs while discovering ~10.5 novel TTPs per sample on average.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.06854v1",
      "arxiv_id": "2602.06854v1",
      "title": "SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks",
      "authors": [
        "Mingqian Feng",
        "Xiaodong Liu",
        "Weiwei Yang",
        "Jialin Song",
        "Xuekai Zhu",
        "Chenliang Xu",
        "Jianfeng Gao"
      ],
      "abstract": "Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average $80.1\\%$ ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.06854v1",
      "url": "https://arxiv.org/abs/2602.06854v1",
      "categories": [
        "cs.CL"
      ],
      "published_official": true,
      "keywords": [
        "jailbreak",
        "llm"
      ],
      "keyword_score": 2,
      "summary": "SEMA is a novel, self-supervised framework for generating effective multi-turn jailbreak attacks against safety-aligned LLMs—addressing the realistic threat where attackers iteratively refine prompts across turns. It introduces two key innovations: (1) *prefilling self-tuning*, which bootstraps training using self-generated, non-refusal multi-turn adversarial prompts with minimal prefixes to stabilize exploration; and (2) *intent-drift-aware reinforcement learning*, which optimizes attack success while preserving harmful intent via a composite reward combining intent alignment, compliance risk, and prompt detail. Operating in an open-loop regime (no victim feedback required), SEMA unifies single- and multi-turn settings and significantly reduces exploration complexity. It achieves state-of-the-art attack success rates—e.g., 80.1% ASR@1 on AdvBench across diverse victim models—outperforming all prior single-turn, scripted, template-based, and SFT/DPO baselines by up to 33.9%. The method is compact, reproducible, transferable across models, and enables scalable, automated redteaming for robust LLM safety evaluation.",
      "summary_status": "success"
    },
    {
      "id": "arxiv_2602.06268v1",
      "arxiv_id": "2602.06268v1",
      "title": "MPIB: A Benchmark for Medical Prompt Injection Attacks and Clinical Safety in LLMs",
      "authors": [
        "Junhyeok Lee",
        "Han Jang",
        "Kyu Sung Choi"
      ],
      "abstract": "Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems are increasingly integrated into clinical workflows; however, prompt injection attacks can steer these systems toward clinically unsafe or misleading outputs. We introduce the Medical Prompt Injection Benchmark (MPIB), a dataset-and-benchmark suite for evaluating clinical safety under both direct prompt injection and indirect, RAG-mediated injection across clinically grounded tasks. MPIB emphasizes outcome-level risk via the Clinical Harm Event Rate (CHER), which measures high-severity clinical harm events under a clinically grounded taxonomy, and reports CHER alongside Attack Success Rate (ASR) to disentangle instruction compliance from downstream patient risk. The benchmark comprises 9,697 curated instances constructed through multi-stage quality gates and clinical safety linting. Evaluating MPIB across a diverse set of baseline LLMs and defense configurations, we find that ASR and CHER can diverge substantially, and that robustness depends critically on whether adversarial instructions appear in the user query or in retrieved context. We release MPIB with evaluation code, adversarial baselines, and comprehensive documentation to support reproducible and systematic research on clinical prompt injection. Code and data are available at GitHub (code) and Hugging Face (data).",
      "published": "2026-02-06",
      "source": "arXiv",
      "pdf_link": "https://arxiv.org/pdf/2602.06268v1",
      "url": "https://arxiv.org/abs/2602.06268v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published_official": true,
      "keywords": [
        "prompt",
        "injection"
      ],
      "keyword_score": 2,
      "summary": "This paper introduces MPIB, a new benchmark specifically designed to evaluate clinical safety risks from prompt injection attacks against LLMs and RAG systems in medical settings. It proposes the Clinical Harm Event Rate (CHER)—a novel, outcome-focused metric grounded in clinical taxonomy—to measure high-severity patient harm, complementing traditional Attack Success Rate (ASR) to distinguish instruction compliance from real-world risk. MPIB comprises 9,697 rigorously curated instances covering both direct (user-query) and indirect (RAG-retrieved context) injection scenarios, validated through multi-stage clinical safety linting. Evaluation across diverse LLMs and defenses reveals that ASR and CHER often diverge significantly, and robustness critically depends on attack location (user input vs. retrieved context). The benchmark is publicly released with code, data, and documentation to enable reproducible research on medical AI safety.",
      "summary_status": "success"
    }
  ],
  "last_updated": "2026-02-12T09:35:19.694327",
  "total_count": 103
}